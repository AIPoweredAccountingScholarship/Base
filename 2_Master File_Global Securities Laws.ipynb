{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code was developed in collaboration with Claude4-Sonnet\n",
    "#Claude was used for code generation, documentation, and error handling\n",
    "#Note: the code will run best when executing each section separately\n",
    "#Required inputs: CSV files with securities law data, data with \"GVKEY\" and \"FYEAR\" as identifiers\n",
    "\n",
    "# 1. Claude identify global securities laws\n",
    "import os\n",
    "import anthropic\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def get_securities_laws(conversation_history=None):\n",
    "    # Initialize the Anthropic client\n",
    "    client = anthropic.Anthropic(\n",
    "        api_key=\"enter API here\"\n",
    "    )\n",
    "    \n",
    "    # Initial prompt - modified to ensure consistent date format\n",
    "    initial_content = \"\"\"Your task is to identify and compile a comprehensive database of at least 100 securities \n",
    "    laws around the world - NOT in the U.S. Securities regulation is the field of law that covers transactions and other dealings with securities. \n",
    "    Securities laws aim at ensuring that investors receive accurate and necessary information regarding the type and value\n",
    "    of the interest under consideration for purchase.\n",
    "\n",
    "IMPORTANT: Only identify new non-U.S.securities regulations. Exclude amendments, updates, or revisions to existing rules. \n",
    "Focus on major new laws only.\n",
    "\n",
    "IMPORTANT: Do not include laws with titles containing the following words: \"Amendment\", \"Update\", or \"Revision\"\n",
    "\n",
    "IMPORTANT: Before adding any regulation to your list, check if you have already included it. \n",
    "Do not include any duplicate entries - each regulation should appear only once.\n",
    "The goal is to create a dataset that captures the following key details for each law. \n",
    "\n",
    "Please follow these guidelines:\n",
    "\n",
    "Data Fields to Collect:\n",
    "• Date: The announcement or implementation date of the law (use YYYY-MM-DD format).\n",
    "• Regulation Title or Name: The official name or designation of the regulatory change. Include the country or jurisdiction\n",
    "  this law applies to after the law name.\n",
    "• Regulatory Body/Authority: The government entity responsible for the law.\n",
    "• Description: A brief overview of the law, including key provisions and the rationale behind it.\n",
    "• Impact: The potential or observed effects on industries, markets, or stakeholders.\n",
    "•Litigation Risk: Is this law related to the risk of litigation against managers? By risk of litigation we mean the probability that a manager will be sued or face legal action because of this law. Answer this question with Yes or No. If yes, label the entry \"Litigation Risk\".\n",
    "•Corporate Governance: Is this law related to corporate governance of firms? Corporate governance refers to the internal monitoring system charged with overseeing managers and commonly focuses on matters such as board independence or insider trading policy. Answer this question with Yes or No.If yes, label the entry \"Corporate Governance\".\n",
    "•Proprietary Costs: Is this law related to proprietary costs of firms? By proprietary costs, we mean costs that result from the disclosure of information to competitors which could harm a firm’s competitive position. Answer this question with Yes or No.If yes, label the entry \"Proprietary Costs\".\n",
    "•Information Asymmetry: Is this law related to information asymmetry between owners and managers? By information asymmetry we mean that one party has more or better information than the other party. Answer this question with Yes or No. If yes, label the entry \"Information Asymmetry\".\n",
    "•Unsophisticated Investors: Is the law related to protecting unsophisticated investors? By unsophisticated investors, we mean investors that are either new to investing or are not well informed. Answer this question with Yes or No. If yes, label the entry \"Unsophisticated Investors\".\n",
    "•Equity Issuance in Public vs. Private Markets: Is this law related to the costs and benefits of issuing equity in public versus private markets? Answer this question with Yes or No. If yes, label the entry \"Equity Issuance in Public vs. Private Markets\".\n",
    "•Reputation Risk: Is this law related to the reputation of firm managers? By of firm manager, we mean the career prospects and prestige of an individual manager. Answer this question with Yes or No. If yes, label the entry \"Reputation Risk\".\n",
    "\n",
    "• References: References must link directly to the specific regulation or announcement, not to general websites. \n",
    "If you cannot find a specific document or article about the regulation, DO NOT INCLUDE THE LAW.\n",
    "\n",
    "Requirements:\n",
    "• Scope: Cover as many laws as possible that were announced or implemented in the last 25 years.\n",
    "• Consistency: Ensure uniform formatting for all entries in the dataset.\n",
    "• Dates must be in YYYY-MM-DD format (e.g., 2002-07-30).\n",
    "\n",
    "Output:\n",
    "Provide data in a tabular format with rows for each law and columns for the data fields listed above. \n",
    "Use credible, authoritative sources such as government websites, legal databases, academic journals, or credible news sources.\n",
    "Do not include duplicate laws.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        if conversation_history:\n",
    "            messages = conversation_history\n",
    "        else:\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": initial_content\n",
    "            }]\n",
    "\n",
    "        response = client.messages.create(\n",
    "            max_tokens=8000,\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            temperature=0.5,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.content[0].text, messages + [\n",
    "            {\"role\": \"assistant\", \"content\": response.content[0].text}\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error making API call: {e}\")\n",
    "        return None, messages\n",
    "\n",
    "def add_follow_up_prompt(conversation_history, follow_up_prompt):\n",
    "    \"\"\"Add a follow-up prompt to the conversation history\"\"\"\n",
    "    return conversation_history + [{\"role\": \"user\", \"content\": follow_up_prompt}]\n",
    "\n",
    "def standardize_date(date_str):\n",
    "    \"\"\"Attempt to standardize date format to YYYY-MM-DD\"\"\"\n",
    "    try:\n",
    "        # Convert to datetime and then back to string in desired format\n",
    "        return pd.to_datetime(date_str).strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        # If conversion fails, return original string\n",
    "        return date_str\n",
    "\n",
    "def parse_table_fallback(response_text: str) -> pd.DataFrame:\n",
    "    \"\"\"Fallback parser for when Claude returns table format instead of numbered format.\"\"\"\n",
    "    print(\"Debugging table parsing...\")\n",
    "    \n",
    "    lines = response_text.split('\\n')\n",
    "    table_lines = [line.strip() for line in lines if line.strip().startswith('|') and len(line.strip()) > 5]\n",
    "    \n",
    "    print(f\"Found {len(table_lines)} table lines\")\n",
    "    \n",
    "    if len(table_lines) < 2:\n",
    "        print(\"Not enough table lines found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Remove separator lines (containing ---)\n",
    "    data_lines = [line for line in table_lines if '---' not in line]\n",
    "    print(f\"Found {len(data_lines)} data lines (after removing separators)\")\n",
    "    \n",
    "    if len(data_lines) < 2:\n",
    "        print(\"Not enough data lines after removing separators\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Parse header line\n",
    "    header_line = data_lines[0]\n",
    "    raw_headers = header_line.split('|')\n",
    "    headers = [col.strip() for col in raw_headers if col.strip()]\n",
    "    \n",
    "    print(f\"Original headers ({len(headers)}): {headers}\")\n",
    "    \n",
    "    # Parse data rows\n",
    "    data_rows = []\n",
    "    for i, line in enumerate(data_lines[1:], 1):\n",
    "        raw_columns = line.split('|')\n",
    "        columns = [col.strip() for col in raw_columns if col.strip()]\n",
    "        \n",
    "        if len(columns) == len(headers):\n",
    "            data_rows.append(columns)\n",
    "            print(f\"Row {i}: ✓ Added ({len(columns)} columns)\")\n",
    "        else:\n",
    "            print(f\"Row {i}: ✗ Skipped - {len(columns)} columns vs {len(headers)} headers\")\n",
    "    \n",
    "    print(f\"Successfully parsed {len(data_rows)} data rows\")\n",
    "    \n",
    "    if not data_rows:\n",
    "        print(\"No valid data rows found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create DataFrame with original headers first\n",
    "    df = pd.DataFrame(data_rows, columns=headers)\n",
    "    print(f\"Created DataFrame with columns: {list(df.columns)}\")\n",
    "\n",
    "    # Make the mapping more robust for title column\n",
    "    for col in df.columns:\n",
    "        if 'title' in col.lower() or 'name' in col.lower():\n",
    "            df = df.rename(columns={col: 'Regulation Title'})\n",
    "            print(f\"Mapped column '{col}' to 'Regulation Title'\")\n",
    "            break\n",
    "            \n",
    "    # Now standardize the column names to match your expected format\n",
    "    # Create a mapping from the table headers to your standard column names\n",
    "    standard_columns = {\n",
    "        'Date': 'Date',\n",
    "        'Regulation Title/Name': 'Regulation Title', \n",
    "        'Regulatory Body': 'Regulatory Body',\n",
    "        'Regulatory Body/Authority': 'Regulatory Body',  # Handle variation\n",
    "        'Description': 'Description',\n",
    "        'Impact': 'Impact',\n",
    "        'Litigation Risk': 'Litigation Risk',\n",
    "        'Corporate Governance': 'Corporate Governance', \n",
    "        'Proprietary Costs': 'Proprietary Costs',\n",
    "        'Information Asymmetry': 'Information Asymmetry',\n",
    "        'Unsophisticated Investors': 'Unsophisticated Investors',\n",
    "        'Equity Issuance Public vs Private': 'Equity Issuance',\n",
    "        'Equity Issuance in Public vs. Private Markets': 'Equity Issuance',  # Handle variation\n",
    "        'Reputation Risk': 'Reputation Risk',\n",
    "        'References': 'References'\n",
    "    }\n",
    "    \n",
    "    # Rename columns using the mapping\n",
    "    df_renamed = df.rename(columns=standard_columns)\n",
    "    \n",
    "    # Ensure all required columns exist\n",
    "    required_columns = [\n",
    "        'Date', 'Regulation Title', 'Regulatory Body', 'Description', 'Impact',\n",
    "        'Litigation Risk', 'Corporate Governance', 'Proprietary Costs', \n",
    "        'Information Asymmetry', 'Unsophisticated Investors', 'Equity Issuance',\n",
    "        'Reputation Risk', 'References'\n",
    "    ]\n",
    "    \n",
    "    # Add missing columns with None values\n",
    "    for col in required_columns:\n",
    "        if col not in df_renamed.columns:\n",
    "            print(f\"Adding missing column: {col}\")\n",
    "            df_renamed[col] = None\n",
    "    \n",
    "    # Select only the required columns in the correct order\n",
    "    final_df = df_renamed[required_columns].copy()\n",
    "    \n",
    "    # Standardize date format\n",
    "    if 'Date' in final_df.columns:\n",
    "        print(\"Standardizing dates...\")\n",
    "        final_df['Date'] = final_df['Date'].apply(lambda x: standardize_date(x) if pd.notna(x) and str(x).strip() else x)\n",
    "    \n",
    "    # Clean up any completely empty rows\n",
    "    final_df = final_df.dropna(how='all')\n",
    "    \n",
    "    print(f\"Final DataFrame: {len(final_df)} rows x {len(final_df.columns)} columns\")\n",
    "    print(f\"Final columns: {list(final_df.columns)}\")\n",
    "    \n",
    "    return final_df\n",
    "    \n",
    "def parse_response_to_dataframe(response_text: str) -> pd.DataFrame:\n",
    "    \"\"\"Parse the response text into a pandas DataFrame.\"\"\"\n",
    "    print(\"\\nParsing response...\")\n",
    "    \n",
    "    # First try the original numbered format parsing\n",
    "    data = []\n",
    "    current_entry = None\n",
    "    entry_number = None\n",
    "    \n",
    "    lines = [line.strip() for line in response_text.split('\\n') if line.strip()]\n",
    "    \n",
    "    for line in lines:\n",
    "        number_match = re.match(r'^\\*?\\*?(\\d+)\\.\\*?\\*?', line)\n",
    "        if number_match:\n",
    "            if current_entry and len(current_entry) > 0:\n",
    "                if 'Regulation Title' not in current_entry and entry_number:\n",
    "                    current_entry['Regulation Title'] = f\"Law {entry_number}\"\n",
    "                data.append(current_entry)\n",
    "            current_entry = {}\n",
    "            entry_number = number_match.group(1)\n",
    "            continue\n",
    "            \n",
    "        if ':' in line and current_entry is not None:\n",
    "            key, value = [x.strip() for x in line.split(':', 1)]\n",
    "            \n",
    "            key_mapping = {\n",
    "                'Date': 'Date',\n",
    "                'Title': 'Regulation Title',\n",
    "                'Authority': 'Regulatory Body',\n",
    "                'Description': 'Description',\n",
    "                'Impact': 'Impact',\n",
    "                'Litigation Risk': 'Litigation Risk',\n",
    "                'Corporate Governance': 'Corporate Governance',\n",
    "                'Proprietary Costs': 'Proprietary Costs',\n",
    "                'Information Asymmetry': 'Information Asymmetry',\n",
    "                'Unsophisticated Investors': 'Unsophisticated Investors',\n",
    "                'Equity Issuance': 'Equity Issuance',\n",
    "                'Reputation Risk': 'Reputation Risk',\n",
    "                'References': 'References'\n",
    "            }\n",
    "            \n",
    "            if key in key_mapping:\n",
    "                column_name = key_mapping[key]\n",
    "                if column_name == 'Date':\n",
    "                    current_entry[column_name] = standardize_date(value)\n",
    "                else:\n",
    "                    current_entry[column_name] = value.strip()\n",
    "\n",
    "    if current_entry and len(current_entry) > 0:\n",
    "        if 'Regulation Title' not in current_entry and entry_number:\n",
    "            current_entry['Regulation Title'] = f\"Law {entry_number}\"\n",
    "        data.append(current_entry)\n",
    "    \n",
    "    print(f\"\\nFound {len(data)} entries in numbered format\")\n",
    "    \n",
    "    # If numbered format parsing found data, use it\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        required_columns = ['Date', 'Regulation Title', 'Regulatory Body', 'Description', 'Impact',\n",
    "                          'Litigation Risk', 'Corporate Governance', 'Proprietary Costs',\n",
    "                          'Information Asymmetry', 'Unsophisticated Investors', 'Equity Issuance',\n",
    "                          'Reputation Risk', 'References']\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                print(f\"Adding missing column: {col}\")\n",
    "                df[col] = None\n",
    "        \n",
    "        df['Regulation Title'] = df['Regulation Title'].fillna('Unknown')\n",
    "        df['Regulation Title'] = df['Regulation Title'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "        \n",
    "        df['dedup_key'] = df.apply(lambda row: f\"{row['Date']}_{row['Regulation Title']}\", axis=1)\n",
    "        df = df.drop_duplicates(subset=['dedup_key'], keep='first')\n",
    "        df = df.drop('dedup_key', axis=1)\n",
    "        \n",
    "        df = df[required_columns]\n",
    "        print(f\"Created DataFrame with {len(df)} rows\")\n",
    "        return df.copy()\n",
    "    \n",
    "    # If numbered format failed, try table format as fallback\n",
    "    print(\"Numbered format parsing failed, trying table format...\")\n",
    "    df = parse_table_fallback(response_text)\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(f\"Table format parsing succeeded with {len(df)} rows\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Both parsing methods failed - no valid data to create DataFrame\")\n",
    "        return pd.DataFrame()\n",
    "                \n",
    "def compile_all_responses() -> pd.DataFrame:\n",
    "    \"\"\"Compile multiple API responses into a single DataFrame.\"\"\"\n",
    "    all_responses = []\n",
    "    conversation_history = None\n",
    "\n",
    "    # Get initial response\n",
    "    initial_response, conversation_history = get_securities_laws()\n",
    "    if initial_response:\n",
    "        print(\"\\nInitial response:\")\n",
    "        print(initial_response)\n",
    "        all_responses.append(initial_response)\n",
    "\n",
    "        follow_up_prompts = [\n",
    "            \"\"\"Starting with number {last_num}, list 20 more non-U.S. securities laws using this exact format for each:\n",
    "Date: YYYY-MM-DD\n",
    "Title: [title]\n",
    "Authority: [body]\n",
    "Description: [brief]\n",
    "Impact: [impact]\n",
    "Litigation Risk: Yes/No\n",
    "Corporate Governance: Yes/No\n",
    "Proprietary Costs: Yes/No\n",
    "Information Asymmetry: Yes/No\n",
    "Unsophisticated Investors: Yes/No\n",
    "Equity Issuance: Yes/No\n",
    "Reputation Risk: Yes/No\n",
    "References: [link]\"\"\",\n",
    "\n",
    "            \"Continue from number {last_num}. Provide 20 more laws using the exact same format.\",\n",
    "            \n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \n",
    "            \"Recall that you have to identify at least 100 securities laws. Recall securities regulation is the field of law that covers transactions and other dealings with securities. Securities laws aim at ensuring that investors receive accurate and necessary information regarding the type and value of the interest under consideration for purchase.\"\n",
    "        ]\n",
    "        last_num = len(parse_response_to_dataframe(initial_response)) + 1\n",
    "        \n",
    "        for i, prompt_template in enumerate(follow_up_prompts, 1):\n",
    "            prompt = prompt_template.format(last_num=last_num)\n",
    "            conversation_history = add_follow_up_prompt(conversation_history, prompt)\n",
    "            response, conversation_history = get_securities_laws(conversation_history)\n",
    "            \n",
    "            if response:\n",
    "                print(f\"\\nFollow-up response {i}:\")\n",
    "                print(response)\n",
    "                all_responses.append(response)\n",
    "                df = parse_response_to_dataframe(response)\n",
    "                last_num += len(df)\n",
    "            \n",
    "    # Parse all responses into DataFrames and concatenate\n",
    "    dfs = []\n",
    "    for response in all_responses:\n",
    "        df = parse_response_to_dataframe(response)\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"No valid data frames were created!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Remove duplicates using multiple fields to better identify unique laws\n",
    "    final_df['Title_clean'] = final_df['Regulation Title'].fillna('').str.lower().str.strip()\n",
    "    final_df['Description_clean'] = final_df['Description'].fillna('').str.lower().str.strip()\n",
    "    \n",
    "    # Create composite key for deduplication\n",
    "    final_df['dedup_key'] = final_df.apply(\n",
    "        lambda row: f\"{row['Date']}_{row['Title_clean']}_{row['Description_clean'][:50]}\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Remove duplicates and cleanup\n",
    "    final_df = final_df.drop_duplicates(subset=['dedup_key'], keep='first')\n",
    "    final_df = final_df.drop(['Title_clean', 'Description_clean', 'dedup_key'], axis=1)\n",
    "\n",
    "    # Sort by date\n",
    "    try:\n",
    "        final_df['DateSort'] = pd.to_datetime(final_df['Date'], errors='coerce')\n",
    "        final_df = final_df.dropna(subset=['DateSort'])\n",
    "        final_df = final_df.sort_values('DateSort', ascending=False)\n",
    "        final_df = final_df.drop('DateSort', axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not sort by date due to: {e}\")\n",
    "        print(\"Problematic dates:\")\n",
    "        print(final_df['Date'].value_counts())\n",
    "\n",
    "    # Return the final DataFrame\n",
    "    return final_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Compile all responses into a DataFrame\n",
    "    df = compile_all_responses()\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"\\nError: No data was collected!\")\n",
    "    else:\n",
    "        # Display basic statistics\n",
    "        print(f\"\\nTotal number of unique laws: {len(df)}\")\n",
    "        print(\"\\nMost recent laws:\")\n",
    "        print(df.head().to_string())\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_path = 'enter file path here'\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nDatabase saved to: {output_path}\")\n",
    "        \n",
    "# 2. Add column for Year \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"enter file path here\")\n",
    "\n",
    "# Clean parentheses and dashes from text columns\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    df[column] = df[column].str.replace('(', '').str.replace(')', '').str.replace('-', '')\n",
    "    \n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df\n",
    "\n",
    "#Excluding years prior to 2002 2020 and 2021 since we don't have forecast data. We also exclude years 2018, 2019 from law file because we need 2 years after and we have data\n",
    "#up to 2019\n",
    "filtered_df = df[~df['Year'].isin([1986, 1987, 1988, 1989,1990, 1991, 1992, 1993, 1994, 1995, 1996,\n",
    "                                   1997, 1998, 1999, 2000, 2001,2018, 2019, 2020, 2021, 2022, 2023, 2024])]\n",
    "\n",
    "filtered_df_with_titles = filtered_df.dropna(subset=[\"Regulatory Body\"])\n",
    "\n",
    "filtered_df_with_titles.to_csv(\"enter file path here\")\n",
    "\n",
    "# 3. Create Panel Datasets for Each Law and Each Channel\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def add_underscores_before_capitals(text):\n",
    "    \"\"\"Add underscores before capital letters in a string\"\"\"\n",
    "    return re.sub(r'(?<=[a-z0-9])(?=[A-Z])', '_', text)\n",
    "\n",
    "def create_channel_panels(laws_file: str, panel_file: str, output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates separate panel datasets for each law and each channel marked as \"Yes\".\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read the input files\n",
    "    laws_df = pd.read_csv(laws_file)\n",
    "    panel_df = pd.read_csv(panel_file)\n",
    "    \n",
    "    # Convert Year columns to int\n",
    "    laws_df['Year'] = pd.to_numeric(laws_df['Year'])\n",
    "    panel_df['FYEAR'] = pd.to_numeric(panel_df['FYEAR'])\n",
    "    \n",
    "    # Define channels to check for \"Yes\"\n",
    "    channels = [\n",
    "        'Litigation Risk',\n",
    "        'Corporate Governance',\n",
    "        'Proprietary Costs',\n",
    "        'Information Asymmetry',\n",
    "        'Unsophisticated Investors',\n",
    "        'Equity Issuance',\n",
    "        'Reputation Risk'\n",
    "    ]\n",
    "    \n",
    "    # List of columns to bring from laws dataset\n",
    "    law_columns = [\n",
    "        'Date', 'Regulation Title', 'Regulatory Body', 'Description', \n",
    "        'Impact', 'Litigation Risk', 'Corporate Governance', \n",
    "        'Proprietary Costs', 'Information Asymmetry', \n",
    "        'Unsophisticated Investors', 'Equity Issuance', \n",
    "        'Reputation Risk', 'References', 'Year'\n",
    "    ]\n",
    "    \n",
    "    # Process each law\n",
    "    for _, law in laws_df.iterrows():  # Fixed: removed asterisks\n",
    "        try:\n",
    "            # Check each channel\n",
    "            for channel in channels:\n",
    "                # Only create panel if channel is \"Yes\"\n",
    "                if str(law[channel]).strip().lower() == \"yes\":\n",
    "                    # Create a copy of the panel data\n",
    "                    law_panel = panel_df.copy()\n",
    "                    \n",
    "                    # Add law information to each row\n",
    "                    for col in law_columns:\n",
    "                        law_panel[col] = law[col]\n",
    "                    \n",
    "                    # Create treatment indicator\n",
    "                    law_panel['post_law'] = (law_panel['FYEAR'] >= law['Year']).astype(int)\n",
    "                    law_panel['treated'] = 1\n",
    "                    law_panel['treatment_effect'] = law_panel['post_law'] * law_panel['treated']\n",
    "                    \n",
    "                    # Create filename with both law and channel\n",
    "                    safe_title = law['Regulation Title'].replace('/', '_').replace('\\\\', '_')\n",
    "                    safe_title = ''.join(c for c in safe_title if c.isalnum() or c in ('_', '-'))\n",
    "                    \n",
    "                    # Add underscores before capital letters\n",
    "                    safe_title = add_underscores_before_capitals(safe_title)\n",
    "                    \n",
    "                    safe_channel = channel.replace(' ', '_')\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    output_file = os.path.join(output_dir, f\"panel_{safe_title}_{safe_channel}.csv\")\n",
    "                    law_panel.to_csv(output_file, index=False)\n",
    "                    \n",
    "                    print(f\"Created panel dataset for: {law['Regulation Title']} - {channel}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing law {law['Regulation Title']}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":  # Fixed: removed asterisks\n",
    "    # Configuration\n",
    "    laws_file = \"enter file path here\"\n",
    "    panel_file = \"enter file path here\"\n",
    "    output_dir = \"enter folder path here\"\n",
    "    \n",
    "    # Create panel datasets\n",
    "    create_channel_panels(laws_file, panel_file, output_dir)\n",
    "    \n",
    "    print(\"\\nPanel creation complete!\")\n",
    "    \n",
    "# 4. Add time trend variable, run regression analyses and save regression tables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant\n",
    "from linearmodels.iv import AbsorbingLS\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from fpdf import FPDF\n",
    "import traceback\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "\n",
    "def add_underscores_before_capitals(text):\n",
    "    \"\"\"Add underscores before capital letters in a string\"\"\"\n",
    "    return re.sub(r'(?<=[a-z0-9])(?=[A-Z])', '_', text)\n",
    "\n",
    "class RegressionAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize regression analyzer\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _get_significance_stars(self, pvalue: float) -> str:\n",
    "        \"\"\"Get significance stars based on p-value.\"\"\"\n",
    "        if pvalue < 0.01:\n",
    "            return \"***\"\n",
    "        elif pvalue < 0.05:\n",
    "            return \"**\"\n",
    "        elif pvalue < 0.1:\n",
    "            return \"*\"\n",
    "        return \"\"\n",
    "\n",
    "    def add_time_trends(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add time trend variable to the dataset\"\"\"\n",
    "        df_with_trends = df.copy()\n",
    "        \n",
    "        # Convert FYEAR to numeric if not already\n",
    "        df_with_trends['FYEAR'] = pd.to_numeric(df_with_trends['FYEAR'])\n",
    "        \n",
    "        # Sort by firm and year to ensure proper ordering\n",
    "        df_with_trends = df_with_trends.sort_values(['GVKEY', 'FYEAR'])\n",
    "        \n",
    "        # Time trend: FYEAR - first_year_in_panel (since each panel has different event windows)\n",
    "        min_year_in_panel = df_with_trends['FYEAR'].min()\n",
    "        df_with_trends['time_trend'] = df_with_trends['FYEAR'] - min_year_in_panel\n",
    "        \n",
    "        print(f\"Added time trend. Range: {df_with_trends['time_trend'].min()} to {df_with_trends['time_trend'].max()}\")\n",
    "        \n",
    "        return df_with_trends\n",
    "\n",
    "    def filter_event_window(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Filter data to ±2 years around regulation year\"\"\"\n",
    "        try:\n",
    "            regulation_year = int(df['Year'].iloc[0])\n",
    "            return df[\n",
    "                (df['FYEAR'] >= regulation_year - 2) &\n",
    "                (df['FYEAR'] <= regulation_year + 2)\n",
    "            ]\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing column in DataFrame: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error during filtering: {e}\")\n",
    "            raise\n",
    "\n",
    "    def prepare_fixed_effects_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare fixed effects columns for AbsorbingLS\"\"\"\n",
    "        df_prep = df.copy()\n",
    "        \n",
    "        # Ensure categorical variables for fixed effects\n",
    "        df_prep['firm_id'] = df_prep['GVKEY'].astype('category')\n",
    "        \n",
    "        print(f\"Number of firms for fixed effects: {len(df_prep['firm_id'].unique())}\")\n",
    "        \n",
    "        return df_prep\n",
    "\n",
    "    def run_regressions(self, df: pd.DataFrame) -> dict:\n",
    "        \"\"\"Run multiple regression specifications with and without fixed effects\"\"\"\n",
    "        results_dict = {}\n",
    "        \n",
    "        # Prepare data for fixed effects\n",
    "        df_prep = self.prepare_fixed_effects_data(df.copy())\n",
    "        \n",
    "        specifications = {\n",
    "            '(1)': {\n",
    "                'dep_var': 'freqMF',\n",
    "                'controls': [],\n",
    "                'method': 'OLS',\n",
    "                'absorb': None\n",
    "            },\n",
    "            '(2)': {\n",
    "                'dep_var': 'freqMF',\n",
    "                'controls': ['linstown', 'lsize', 'lbtm', 'lroa', 'lsaret12', 'levol', 'lloss', 'lcalrisk','time_trend'],\n",
    "                'method': 'OLS',\n",
    "                'absorb': None\n",
    "            },\n",
    "            '(3)': {\n",
    "                'dep_var': 'freqMF',\n",
    "                'controls': ['linstown', 'lsize', 'lbtm', 'lroa', 'lsaret12', 'levol', 'lloss', 'lcalrisk','time_trend'],\n",
    "                'method': 'AbsorbingLS',\n",
    "                'absorb': ['firm_id']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for spec_name, spec in specifications.items():\n",
    "            print(f\"\\nRunning regression for specification {spec_name}\")\n",
    "            try:\n",
    "                dep_var = spec['dep_var']\n",
    "                controls = spec.get('controls', [])\n",
    "                variables = controls + ['treatment_effect']\n",
    "\n",
    "                # Clean data\n",
    "                print(\"Getting required columns...\")\n",
    "                required_columns = variables + [dep_var, 'GVKEY']\n",
    "                \n",
    "                # Add fixed effects columns if needed\n",
    "                if spec.get('absorb'):\n",
    "                    for absorb_var in spec['absorb']:\n",
    "                        if absorb_var not in required_columns:\n",
    "                            required_columns.append(absorb_var)\n",
    "                \n",
    "                # Check for missing columns\n",
    "                if not all(col in df_prep.columns for col in required_columns):\n",
    "                    missing_cols = [col for col in required_columns if col not in df_prep.columns]\n",
    "                    print(f\"Missing columns: {missing_cols}\")\n",
    "                    raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "            \n",
    "                reg_data = df_prep[required_columns].copy()\n",
    "                reg_data = reg_data.replace([np.inf, -np.inf], np.nan)\n",
    "                reg_data = reg_data.dropna()\n",
    "            \n",
    "                print(f\"Observations: {len(reg_data)}\")\n",
    "                print(f\"Number of unique firms: {len(reg_data['GVKEY'].unique())}\")\n",
    "\n",
    "                # Prepare dependent and independent variables\n",
    "                y = reg_data[dep_var]\n",
    "                X = reg_data[variables]\n",
    "\n",
    "                if spec['method'] == 'OLS':\n",
    "                    # Standard OLS for specifications without fixed effects\n",
    "                    X_with_const = add_constant(X)\n",
    "                    model = OLS(y, X_with_const)\n",
    "                    # Fit with clustered standard errors at firm level\n",
    "                    results = model.fit(cov_type='cluster', cov_kwds={'groups': reg_data['GVKEY']})\n",
    "\n",
    "                    # Store results\n",
    "                    results_dict[spec_name] = {\n",
    "                        'coefficients': results.params.to_dict(),\n",
    "                        'pvalues': results.pvalues.to_dict(),\n",
    "                        't_stats': (results.params / results.bse).to_dict(),\n",
    "                        'r_squared': results.rsquared,\n",
    "                        'n_obs': int(results.nobs),\n",
    "                        'n_firms': len(reg_data['GVKEY'].unique()),\n",
    "                        'controls': controls,\n",
    "                        'fixed_effects': {\n",
    "                            'firm': False\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                elif spec['method'] == 'AbsorbingLS':\n",
    "                    # Use AbsorbingLS for high-dimensional fixed effects\n",
    "                    absorb_vars = spec['absorb']\n",
    "                    print(f\"Absorbing fixed effects: {absorb_vars}\")\n",
    "                    \n",
    "                    # Ensure absorption variables are always passed as a DF\n",
    "                    absorb_data = reg_data[absorb_vars]\n",
    "                    \n",
    "                    # Create the AbsorbingLS model\n",
    "                    model = AbsorbingLS(\n",
    "                        dependent=y,\n",
    "                        exog=X,\n",
    "                        absorb=absorb_data\n",
    "                    )\n",
    "                    \n",
    "                    # Fit with clustered standard errors at firm level\n",
    "                    results = model.fit(cov_type='clustered', clusters=reg_data['GVKEY'])\n",
    "                    \n",
    "                    results_dict[spec_name] = {\n",
    "                        'coefficients': results.params.to_dict(),\n",
    "                        'pvalues': results.pvalues.to_dict(),\n",
    "                        't_stats': results.tstats.to_dict(),\n",
    "                        'r_squared': results.rsquared,\n",
    "                        'n_obs': int(results.nobs),\n",
    "                        'n_firms': len(reg_data['GVKEY'].unique()),\n",
    "                        'controls': controls,\n",
    "                        'fixed_effects': {\n",
    "                            'firm': 'firm_id' in absorb_vars\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                print(f\"Successfully completed regression for specification {spec_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in specification {spec_name}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        if not results_dict:\n",
    "            raise ValueError(\"No successful regressions completed\")\n",
    "        \n",
    "        return results_dict\n",
    "\n",
    "    def save_regression_table_as_pdf(self, results: dict, regulation_title: str, output_path: str):\n",
    "        \"\"\"Save regression table as PDF matching the target format for 3 columns\"\"\"\n",
    "        try:\n",
    "            pdf = FPDF(format='A4', orientation='L')  # Landscape for better fit\n",
    "            pdf.set_margins(15, 15, 15)\n",
    "            pdf.add_page()\n",
    "            \n",
    "            # Title - all in bold Times New Roman\n",
    "            try:\n",
    "                # First try Windows standard folder for Times New Roman\n",
    "                pdf.add_font('Times New Roman', '', r'C:\\Windows\\Fonts\\times.ttf', uni=True)\n",
    "                pdf.add_font('Times New Roman', 'B', r'C:\\Windows\\Fonts\\timesbd.ttf', uni=True)\n",
    "                pdf.set_font('Times New Roman', 'B', 11)\n",
    "            except:\n",
    "                try:\n",
    "                    # Try alternative paths for Times New Roman\n",
    "                    pdf.add_font('Times New Roman', '', 'times.ttf', uni=True)\n",
    "                    pdf.add_font('Times New Roman', 'B', 'timesbd.ttf', uni=True)\n",
    "                    pdf.set_font('Times New Roman', 'B', 11)\n",
    "                except:\n",
    "                    print(\"Times New Roman font not found, using Arial Bold\")\n",
    "                    pdf.set_font('Arial', 'B', 11)\n",
    "\n",
    "            # Both title and table number in bold\n",
    "            pdf.cell(0, 8, \"Table 3\", ln=True, align='C')\n",
    "            pdf.cell(0, 8, f\"The Impact of {regulation_title} on Management Forecast Frequency\", ln=True, align='C')\n",
    "            pdf.ln(3)\n",
    "\n",
    "            # Switch back to regular font for table content\n",
    "            try:\n",
    "                pdf.set_font('Times New Roman', '', 10)\n",
    "            except:\n",
    "                pdf.set_font('Arial', '', 10)\n",
    "            \n",
    "            # Calculate column widths for 3 columns (Variable name + 3 specifications)\n",
    "            # Landscape A4: 297mm width - 30mm margins = 267mm available\n",
    "            first_col_width = 70  # Wider for variable names since we have fewer columns\n",
    "            col_width = (pdf.w - 30 - first_col_width) / 3  # Divide remaining space by 3\n",
    "\n",
    "            # Table header\n",
    "            pdf.cell(first_col_width, 7, \"\", 1)\n",
    "            for i in range(1, 4):  # 3 specifications: (1), (2), (3)\n",
    "                pdf.cell(col_width, 7, f\"({i})\", 1, align='C')\n",
    "            pdf.ln()\n",
    "\n",
    "            # Treatment Effect\n",
    "            pdf.cell(first_col_width, 7, \"Treatment Effect\", 1)\n",
    "            for i in range(1, 4):  # Only loop through (1), (2), (3)\n",
    "                spec = f'({i})'\n",
    "                if spec in results:\n",
    "                    coef = results[spec]['coefficients']['treatment_effect']\n",
    "                    tstat = abs(results[spec]['t_stats']['treatment_effect'])\n",
    "                    stars = self._get_significance_stars(results[spec]['pvalues']['treatment_effect'])\n",
    "                    pdf.cell(col_width, 7, f\"{coef:.4f}{stars} ({tstat:.2f})\", 1, align='C')\n",
    "                else:\n",
    "                    pdf.cell(col_width, 7, \"\", 1, align='C')\n",
    "            pdf.ln()\n",
    "\n",
    "            # Control variables for specifications (2) and (3)\n",
    "            control_labels = {\n",
    "                'linstown': 'Institutional ownership',\n",
    "                'lsize': 'Firm size',\n",
    "                'lbtm': 'Book-to-market',\n",
    "                'lroa': 'ROA',\n",
    "                'lsaret12': 'Stock return',\n",
    "                'levol': 'Earnings volatility',\n",
    "                'lloss': 'Loss',\n",
    "                'lcalrisk': 'Class action litigation risk',\n",
    "                'time_trend': 'Time Trend'\n",
    "            }\n",
    "\n",
    "            for var, label in control_labels.items():\n",
    "                pdf.cell(first_col_width, 7, label, 1)\n",
    "                for i in range(1, 4):  # Only loop through (1), (2), (3)\n",
    "                    spec = f'({i})'\n",
    "                    if spec in results and var in results[spec]['coefficients']:\n",
    "                        coef = results[spec]['coefficients'][var]\n",
    "                        tstat = abs(results[spec]['t_stats'][var])\n",
    "                        stars = self._get_significance_stars(results[spec]['pvalues'][var])\n",
    "                        pdf.cell(col_width, 7, f\"{coef:.4f}{stars} ({tstat:.2f})\", 1, align='C')\n",
    "                    else:\n",
    "                        pdf.cell(col_width, 7, \"\", 1, align='C')\n",
    "                pdf.ln()\n",
    "\n",
    "            # Fixed effects rows\n",
    "            pdf.cell(first_col_width, 7, \"Firm fixed effects\", 1)\n",
    "            for i in range(1, 4):  # Only loop through (1), (2), (3)\n",
    "                spec = f'({i})'\n",
    "                if spec in results:\n",
    "                    text = \"Yes\" if results[spec]['fixed_effects']['firm'] else \"No\"\n",
    "                    pdf.cell(col_width, 7, text, 1, align='C')\n",
    "                else:\n",
    "                    pdf.cell(col_width, 7, \"No\", 1, align='C')\n",
    "            pdf.ln()\n",
    "\n",
    "            # N and R²\n",
    "            for stat in ['N', 'R²']:\n",
    "                pdf.cell(first_col_width, 7, stat, 1)\n",
    "                for i in range(1, 4):  # Only loop through (1), (2), (3)\n",
    "                    spec = f'({i})'\n",
    "                    if spec in results:\n",
    "                        value = results[spec]['n_obs'] if stat == 'N' else results[spec]['r_squared']\n",
    "                        text = f\"{value:,}\" if stat == 'N' else f\"{value:.4f}\"\n",
    "                        pdf.cell(col_width, 7, text, 1, align='C')\n",
    "                    else:\n",
    "                        pdf.cell(col_width, 7, \"\", 1, align='C')\n",
    "                pdf.ln()\n",
    "\n",
    "            # Notes\n",
    "            pdf.ln(10)\n",
    "            pdf.set_font('Times', size=10)\n",
    "            notes = \"Notes: t-statistics in parentheses. *, **, and *** represent significance at the 10%, 5%, and 1% level, respectively.\"\n",
    "            pdf.multi_cell(0, 5, notes)\n",
    "\n",
    "            pdf.output(output_path)\n",
    "            print(f\"PDF saved at {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving PDF: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def analyze_panel(self, panel_file: str, output_dir: str):\n",
    "        \"\"\"Analyze a single panel dataset\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nStarting analysis of {os.path.basename(panel_file)}...\")\n",
    "            print(\"Reading data...\")\n",
    "            df = pd.read_csv(panel_file)\n",
    "            \n",
    "            print(\"Filtering event window...\")\n",
    "            df_filtered = self.filter_event_window(df)\n",
    "            \n",
    "            print(\"Adding time trends...\")\n",
    "            df_with_trends = self.add_time_trends(df_filtered)\n",
    "            \n",
    "            print(\"Running regressions...\")\n",
    "            results = self.run_regressions(df_with_trends)\n",
    "            \n",
    "            print(\"Saving results...\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Save filtered data with time trends and results\n",
    "            print(\"Saving filtered data with time trends...\")\n",
    "            df_with_trends.to_csv(os.path.join(output_dir, 'filtered_data_with_trends.csv'), index=False)\n",
    "            with open(os.path.join(output_dir, 'regression_results.json'), 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            \n",
    "            # Save regression table\n",
    "            table_path = os.path.join(output_dir, 'regression_table.pdf')\n",
    "            self.save_regression_table_as_pdf(\n",
    "                results,\n",
    "                df['Regulation Title'].iloc[0],\n",
    "                table_path\n",
    "            )\n",
    "            print(f\"Saved regression table to {table_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing panel: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "def analyze_all_panels(input_dir: str, output_dir: str):\n",
    "    \"\"\"Analyze all panel datasets in a directory\"\"\"\n",
    "    analyzer = RegressionAnalyzer()\n",
    "    panel_files = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "    total_files = len(panel_files)\n",
    "    successful_runs = 0\n",
    "    failed_runs = 0\n",
    "    print(f\"\\nFound {total_files} panel files to analyze\")\n",
    "    \n",
    "    for i, panel_file in enumerate(panel_files, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing panel {i} of {total_files}: {os.path.basename(panel_file)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Get the base filename without extension\n",
    "            base_filename = os.path.basename(panel_file).replace('.csv', '')\n",
    "            \n",
    "            # Apply the same naming convention (add underscores before capitals)\n",
    "            formatted_filename = add_underscores_before_capitals(base_filename)\n",
    "            \n",
    "            analyzer.analyze_panel(panel_file, os.path.join(output_dir, formatted_filename))\n",
    "            successful_runs += 1\n",
    "            print(f\"Successfully processed panel {i}\")\n",
    "        except Exception as e:\n",
    "            failed_runs += 1\n",
    "            print(f\"Failed to process panel {i}: {str(e)}\")\n",
    "            \n",
    "        # Print progress summary\n",
    "        print(f\"\\nProgress Summary:\")\n",
    "        print(f\"Processed: {i}/{total_files} ({(i/total_files)*100:.1f}%)\")\n",
    "        print(f\"Successful: {successful_runs}\")\n",
    "        print(f\"Failed: {failed_runs}\")\n",
    "\n",
    "\n",
    "def process_significance(base_dir, delete_nonsig=False):\n",
    "    \"\"\"\n",
    "    Process panels based on significance and either delete or move non-significant results.\n",
    "    t-stat >= 1.96 is considered significant.\n",
    "    \"\"\"\n",
    "    # Create directory for non-significant results if not deleting\n",
    "    if not delete_nonsig:\n",
    "        nonsig_dir = os.path.join(os.path.dirname(base_dir), 'nonsignificant_results')\n",
    "        os.makedirs(nonsig_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all panel directories\n",
    "    panel_dirs = glob.glob(os.path.join(base_dir, 'panel_*'))\n",
    "    print(f\"Found {len(panel_dirs)} panel directories\")\n",
    "    \n",
    "    # Track results\n",
    "    significant_count = 0\n",
    "    not_significant_count = 0\n",
    "    \n",
    "    # Process each panel\n",
    "    for panel_dir in panel_dirs:\n",
    "        panel_name = os.path.basename(panel_dir)\n",
    "        json_file = os.path.join(panel_dir, 'regression_results.json')\n",
    "        \n",
    "        try:\n",
    "            # Read regression results\n",
    "            with open(json_file, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Check specification (3) since that's the firm FE specification\n",
    "            if '(3)' in results:\n",
    "                t_stat = abs(results['(3)']['t_stats']['treatment_effect'])\n",
    "                is_significant = t_stat >= 1.96\n",
    "                \n",
    "                if is_significant:\n",
    "                    print(f\"{panel_name}: t-stat = {t_stat:.2f} (significant - keeping)\")\n",
    "                    significant_count += 1\n",
    "                else:\n",
    "                    print(f\"{panel_name}: t-stat = {t_stat:.2f} (not significant - {'deleting' if delete_nonsig else 'moving'})\")\n",
    "                    if delete_nonsig:\n",
    "                        shutil.rmtree(panel_dir)\n",
    "                    else:\n",
    "                        shutil.move(panel_dir, os.path.join(nonsig_dir, panel_name))\n",
    "                    not_significant_count += 1\n",
    "            else:\n",
    "                print(f\"{panel_name}: No specification (3) found - {'deleting' if delete_nonsig else 'moving'}\")\n",
    "                if delete_nonsig:\n",
    "                    shutil.rmtree(panel_dir)\n",
    "                else:\n",
    "                    shutil.move(panel_dir, os.path.join(nonsig_dir, panel_name))\n",
    "                not_significant_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {panel_name}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    total_processed = significant_count + not_significant_count\n",
    "    if total_processed > 0:\n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"Total panels processed: {total_processed}\")\n",
    "        print(f\"Significant results: {significant_count} ({(significant_count/total_processed)*100:.1f}%)\")\n",
    "        print(f\"Not significant results: {not_significant_count} ({(not_significant_count/total_processed)*100:.1f}%)\")\n",
    "        if not delete_nonsig:\n",
    "            print(f\"\\nNon-significant results moved to: {nonsig_dir}\")\n",
    "        else:\n",
    "            print(\"\\nNon-significant results deleted\")\n",
    "    else:\n",
    "        print(\"No panels processed successfully\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    INPUT_DIR = \"enter folder path here\"\n",
    "    OUTPUT_DIR = \"enter folder path here\"\n",
    "    \n",
    "    # Run analysis on all panels\n",
    "    analyze_all_panels(INPUT_DIR, OUTPUT_DIR)\n",
    "    \n",
    "# 5. Check and keep significant results \n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def process_significance(base_dir, delete_nonsig=False):\n",
    "    \"\"\"\n",
    "    Process panels based on significance and either delete or move non-significant results.\n",
    "    t-stat >= 1.96 is considered significant.\n",
    "    \"\"\"\n",
    "    # Create directory for non-significant results if not deleting\n",
    "    if not delete_nonsig:\n",
    "        nonsig_dir = os.path.join(os.path.dirname(base_dir), 'nonsignificant_results')\n",
    "        os.makedirs(nonsig_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all panel directories\n",
    "    panel_dirs = glob.glob(os.path.join(base_dir, 'panel_*'))\n",
    "    print(f\"Found {len(panel_dirs)} panel directories\")\n",
    "    \n",
    "    # Track results\n",
    "    significant_count = 0\n",
    "    not_significant_count = 0\n",
    "    \n",
    "    # Process each panel\n",
    "    for panel_dir in panel_dirs:\n",
    "        panel_name = os.path.basename(panel_dir)\n",
    "        json_file = os.path.join(panel_dir, 'regression_results.json')\n",
    "        \n",
    "        try:\n",
    "            # Read regression results\n",
    "            with open(json_file, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Check specification (3)\n",
    "            if '(3)' in results:\n",
    "                t_stat = abs(results['(3)']['t_stats']['treatment_effect'])\n",
    "                is_significant = t_stat >= 1.96\n",
    "                \n",
    "                if is_significant:\n",
    "                    print(f\"{panel_name}: t-stat = {t_stat:.2f} (significant - keeping)\")\n",
    "                    significant_count += 1\n",
    "                else:\n",
    "                    print(f\"{panel_name}: t-stat = {t_stat:.2f} (not significant - {'deleting' if delete_nonsig else 'moving'})\")\n",
    "                    if delete_nonsig:\n",
    "                        shutil.rmtree(panel_dir)\n",
    "                    else:\n",
    "                        shutil.move(panel_dir, os.path.join(nonsig_dir, panel_name))\n",
    "                    not_significant_count += 1\n",
    "            else:\n",
    "                print(f\"{panel_name}: No specification (3) found - {'deleting' if delete_nonsig else 'moving'}\")\n",
    "                if delete_nonsig:\n",
    "                    shutil.rmtree(panel_dir)\n",
    "                else:\n",
    "                    shutil.move(panel_dir, os.path.join(nonsig_dir, panel_name))\n",
    "                not_significant_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {panel_name}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    total_processed = significant_count + not_significant_count\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Total panels processed: {total_processed}\")\n",
    "    print(f\"Significant results: {significant_count} ({(significant_count/total_processed)*100:.1f}%)\")\n",
    "    print(f\"Not significant results: {not_significant_count} ({(not_significant_count/total_processed)*100:.1f}%)\")\n",
    "    if not delete_nonsig:\n",
    "        print(f\"\\nNon-significant results moved to: {nonsig_dir}\")\n",
    "    else:\n",
    "        print(\"\\nNon-significant results deleted\")\n",
    "\n",
    "# Usage\n",
    "base_dir = r\"enter folder path here\"\n",
    "\n",
    "# Choose whether to delete (True) or move (False) non-significant results\n",
    "delete_nonsig = False  # Change to True to delete instead of move\n",
    "process_significance(base_dir, delete_nonsig)\n",
    "\n",
    "# 6. Ask Claude to write a background, theoretical framework, and hypothesis development section\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from anthropic import Anthropic\n",
    "import glob\n",
    "\n",
    "class ComprehensiveAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    def get_laws_with_regression_results(self, csv_file: str, regression_dir: str) -> list:\n",
    "        \"\"\"Read laws from CSV and filter to only those with regression analysis results\"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Get all panel directories in regression_analyses folder\n",
    "        panel_dirs = glob.glob(os.path.join(regression_dir, 'panel_*'))\n",
    "        existing_panels = set()\n",
    "        \n",
    "        for panel_dir in panel_dirs:\n",
    "            # Extract the panel identifier from directory name\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            existing_panels.add(panel_name)\n",
    "        \n",
    "        print(f\"Found {len(existing_panels)} regression analysis folders\")\n",
    "        print(f\"Sample panel names: {list(existing_panels)[:5]}\")\n",
    "        \n",
    "        # Extract unique law names from existing panel folders\n",
    "        law_names_in_panels = set()\n",
    "        for panel_name in existing_panels:\n",
    "            # Remove 'panel_' prefix and mechanism suffix\n",
    "            parts = panel_name.replace('panel_', '').split('_')\n",
    "            # Take all parts except the last 2 (which are mechanism)\n",
    "            law_part = '_'.join(parts[:-2])\n",
    "\n",
    "            # Apply the EXACT same cleaning as CSV names - remove spaces and special chars entirely\n",
    "            cleaned_law_part = law_part.replace('-', '').replace('(', '').replace(')', '')\n",
    "            law_names_in_panels.add(cleaned_law_part)\n",
    "    \n",
    "        print(f\"Unique law names found: {len(law_names_in_panels)}\")\n",
    "        print(f\"Sample law names: {list(law_names_in_panels)[:5]}\")\n",
    "        \n",
    "        print(\"\\n=== DEBUGGING NAMES ===\")\n",
    "        print(\"Sample law names from panels:\")\n",
    "        for name in list(law_names_in_panels)[:10]:\n",
    "            print(f\"  '{name}'\")\n",
    "\n",
    "        print(\"\\nSample law names from CSV (after cleaning):\")\n",
    "        for _, row in df.head(10).iterrows():\n",
    "            clean_title = row['Regulation Title'].replace(' ', '_').replace('/', '').replace('-', '').replace('(', '').replace(')', '')\n",
    "            # Remove multiple consecutive underscores\n",
    "            clean_title = re.sub(r'_+', '_', clean_title)\n",
    "            print(f\"  '{clean_title}' (Original: '{row['Regulation Title']}')\")\n",
    "        \n",
    "        laws = []\n",
    "        \n",
    "        # Define economic mechanisms\n",
    "        mechanisms = [\n",
    "            'Litigation Risk', \n",
    "            'Corporate Governance', \n",
    "            'Proprietary Costs', \n",
    "            'Information Asymmetry',\n",
    "            'Unsophisticated Investors', \n",
    "            'Equity Issuance', \n",
    "            'Reputation Risk'\n",
    "        ]\n",
    "        \n",
    "        laws_found = 0\n",
    "        laws_with_regression = 0\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            laws_found += 1\n",
    "            \n",
    "            # Create expected panel name based on the law\n",
    "            # This should match the naming convention used when creating panels\n",
    "            clean_title = row['Regulation Title'].replace(' ', '_').replace('/', '').replace('-', '').replace('(', '').replace(')', '')\n",
    "            # Remove multiple consecutive underscores\n",
    "            clean_title = re.sub(r'_+', '_', clean_title)\n",
    "            \n",
    "            # Check if this law has a corresponding regression analysis folder\n",
    "            if clean_title in law_names_in_panels:\n",
    "                laws_with_regression += 1\n",
    "                \n",
    "                # Get active mechanisms (where value is 'Yes')\n",
    "                active_mechanisms = [mech for mech in mechanisms if row[mech] == 'Yes']\n",
    "                \n",
    "                law = {\n",
    "                    'title': row['Regulation Title'],\n",
    "                    'year': row['Year'],\n",
    "                    'body': row['Regulatory Body'],\n",
    "                    'description': row['Description'],\n",
    "                    'impact': row['Impact'],\n",
    "                    'mechanisms': active_mechanisms,\n",
    "                    'panel_name': clean_title \n",
    "                }\n",
    "                laws.append(law)\n",
    "                print(f\"✓ Including law: {row['Regulation Title']} (Panel: {clean_title})\")\n",
    "            else:\n",
    "                print(f\"✗ Skipping law: {row['Regulation Title']} (No panel: {clean_title})\")\n",
    "        \n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"Total laws in CSV: {laws_found}\")\n",
    "        print(f\"Laws with regression results: {laws_with_regression}\")\n",
    "        print(f\"Laws to generate hypotheses for: {len(laws)}\")\n",
    "        \n",
    "        return laws\n",
    "    \n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        # Remove headers (##, ###, etc.)\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove bold/italic formatting - more precise pattern\n",
    "        # This handles **text**, ***text***, ****text**** but preserves content\n",
    "        content = re.sub(r'\\*{2,4}([^*]+?)\\*{2,4}', r'\\1', content)\n",
    "        content = re.sub(r'\\*([^*]+?)\\*', r'\\1', content)  # Handle single asterisks\n",
    "        \n",
    "        # Clean up any remaining asterisks\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        \n",
    "        # Clean up any extra whitespace that might be left\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def create_short_filename(self, panel_name: str, mechanism: str, main_dir: str) -> str:\n",
    "        \"\"\"Create a shorter filename if the full path would be too long\"\"\"\n",
    "        clean_mechanism = mechanism.replace(' ', '_')\n",
    "        suffix = \"_background_hypothesis.txt\"\n",
    "        \n",
    "        # Calculate maximum allowed length for panel name\n",
    "        max_panel_length = 250 - len(main_dir) - len(clean_mechanism) - len(suffix) - 10  # 10 char buffer\n",
    "    \n",
    "        if len(panel_name) > max_panel_length:\n",
    "            truncated_panel = panel_name[:max_panel_length].rstrip('_')\n",
    "            filename = f\"{truncated_panel}_{clean_mechanism}{suffix}\"\n",
    "        else:\n",
    "            filename = f\"{panel_name}_{clean_mechanism}{suffix}\"\n",
    "    \n",
    "        return filename\n",
    "\n",
    "    def get_background_hypothesis(self, law: dict, mechanism: str) -> str:\n",
    "        \"\"\"Get background, theoretical framework, and hypothesis development for a law and specific mechanism\"\"\"\n",
    "        prompt = f\"\"\"You are an accounting academic writing a research paper examining {law['title']} and its impact on \n",
    "        voluntary disclosure in the U.S. through the {mechanism} channel.\n",
    "Please write the background, theoretical framework, and hypothesis development section following these guidelines:\n",
    "\n",
    "Law Details:\n",
    "Title: {law['title']} ({law['year']})\n",
    "Regulatory Body: {law['body']}\n",
    "Description: {law['description']}\n",
    "Impact: {law['impact']}\n",
    "Economic Mechanism: {mechanism}\n",
    "\n",
    "Please structure your response as follows:\n",
    "\n",
    "1. Background (3 paragraphs, ~400 words total):\n",
    "    - Label this subsection \"Background\"\n",
    "    - Describe the relevant securities law in  {law['title']}\n",
    "    -Include the date that the change is effective ({law['year']}), which firms are affected, and why the change was instituted.  \n",
    "    -Discuss the effective date ({law['year']}) and implementation details\n",
    "    -Please also discuss whether there were other contemporaneous securities law adoptions. \n",
    "    -Support each claim with citations to foundational papers \n",
    "\n",
    "2. Theoretical Framework\n",
    "    - Begin with a brief introduction connecting the law to the relevant theoretical perspective {mechanism}\n",
    "    - Explain core concepts of {mechanism}\n",
    "    - Connect to voluntary disclosure decisions in U.S. firms\n",
    "    - Link to the specific {mechanism} being studied\n",
    "    - Support with 2-3 seminal citations\n",
    "\n",
    "3. Hypothesis Development (3 paragraphs, ~800 words total):\n",
    "    - Label this subsection \"Hypothesis Development\"\n",
    "    - Present economic mechanisms linking {law['title']} to voluntary disclosure decisions in the U.S. through the {mechanism} channel\n",
    "    - Draw on established theoretical frameworks specifically related to {mechanism}\n",
    "    - Propose a theoretically supported hypothesis about the relationship between the  \n",
    "    securities law from file {law['title']} and voluntary disclosure in the U.S. for the specific {mechanism} channel\n",
    "    - Build logical arguments step by step think through whether prior literature suggests competing theoretical \n",
    "    predictions or if the literature suggests only one direction for the relationship. \n",
    "    - Present the formal hypothesis statement on its own line, clearly labeled \"H1:\"\n",
    "    - Support each claim with citations to foundational papers \n",
    "\n",
    "Writing Guidelines:\n",
    "- Use active voice (e.g., \"We examine\" instead of \"This paper examines\")\n",
    "- Maintain formal academic tone suitable for a top journal\n",
    "- Include 2-3 citations per paragraph \n",
    "- Use present tense for established findings\n",
    "- Make clear distinctions between correlation and causation\n",
    "- Cite papers from top accounting and finance journals such as:\n",
    "    The Accounting Review, Journal of Accounting Research, \n",
    "    Journal of Accounting and Economics, Contemporary Accounting Research, Accounting, Organizations, and Society,\n",
    "    and Review of Accounting Studies\n",
    "    \n",
    "IMPORTANT: Include in-text citations but do not include a separate References section at the end.\"\"\" \n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting background and hypothesis: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def create_background_hypothesis_files(self, csv_file: str, regression_dir: str, output_dir: str):\n",
    "        \"\"\"Generate and save background and hypothesis sections for laws with regression results\"\"\"\n",
    "        # Create main directory\n",
    "        main_dir = os.path.join(output_dir, 'background and hypothesis development')\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        try:\n",
    "            os.makedirs(main_dir, exist_ok=True)\n",
    "            print(f\"Created/verified directory: {main_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating directory {main_dir}: {str(e)}\")\n",
    "            return\n",
    "    \n",
    "        # Get laws that have regression results\n",
    "        laws = self.get_laws_with_regression_results(csv_file, regression_dir)\n",
    "        \n",
    "        # Process each law and mechanism\n",
    "        for law in laws:\n",
    "            print(f\"\\nProcessing law: {law['title']}\")\n",
    "            \n",
    "            # Generate separate background and hypothesis for each mechanism\n",
    "            for mechanism in law['mechanisms']:\n",
    "                print(f\"Processing mechanism: {mechanism}\")\n",
    "                \n",
    "                # Create filename, handling long paths\n",
    "                filename = self.create_short_filename(law['panel_name'], mechanism, main_dir)\n",
    "                file_path = os.path.join(main_dir, filename)\n",
    "                \n",
    "                # Debug: print the path length\n",
    "                print(f\"  File path length: {len(file_path)} characters\")\n",
    "                if len(file_path) > 250:\n",
    "                    print(f\"  WARNING: Path might be too long!\")\n",
    "            \n",
    "                # Check if file already exists\n",
    "                if os.path.exists(file_path):\n",
    "                    print(f\"  Skipping {law['title']} - {mechanism}: File already exists\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Get background and hypothesis content\n",
    "                    content = self.get_background_hypothesis(law, mechanism)\n",
    "                    \n",
    "                    # Remove markdown formatting\n",
    "                    content = self.clean_markdown_formatting(content)\n",
    "                    \n",
    "                    # Write the file\n",
    "                    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(content)\n",
    "                    print(f\"  ✓ Saved: {filename}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error saving file for {law['title']} - {mechanism}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"\n",
    "    CSV_FILE = \"enter file path here\"\n",
    "    REGRESSION_DIR = r\"enter folder path here\"\n",
    "    OUTPUT_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        analyzer = ComprehensiveAnalyzer(API_KEY)\n",
    "        analyzer.create_background_hypothesis_files(CSV_FILE, REGRESSION_DIR, OUTPUT_DIR)\n",
    "        print(\"\\nBackground and hypothesis development sections complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "# 7. Send regression results to Claude for interpretation\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from typing import Dict, List\n",
    "from anthropic import Anthropic\n",
    "\n",
    "class RegressionInterpreter:\n",
    "    def __init__(self, input_dir: str, output_dir: str, api_key: str):\n",
    "        \"\"\"Initialize interpreter with input and output directories\"\"\"\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        \n",
    "    def _get_significance_stars(self, pvalue: float) -> str:\n",
    "        \"\"\"Get significance stars based on p-value.\"\"\"\n",
    "        if pvalue < 0.01:\n",
    "            return \"***\"\n",
    "        elif pvalue < 0.05:\n",
    "            return \"**\"\n",
    "        elif pvalue < 0.1:\n",
    "            return \"*\"\n",
    "        return \"\"\n",
    "    \n",
    "    def _get_significance_level(self, pvalue: float) -> str:\n",
    "        \"\"\"Convert p-value to significance level description\"\"\"\n",
    "        if pvalue < 0.01:\n",
    "            return \"at the 1% level\"\n",
    "        elif pvalue < 0.05:\n",
    "            return \"at the 5% level\"\n",
    "        elif pvalue < 0.1:\n",
    "            return \"at the 10% level\"\n",
    "        return \"not statistically significant\"\n",
    "\n",
    "    def read_regression_results(self, regulation_name: str) -> Dict:\n",
    "        \"\"\"Read regression results JSON file for a specific regulation\"\"\"\n",
    "        results_path = os.path.join(self.output_dir, regulation_name, 'regression_results.json')\n",
    "        \n",
    "        try:\n",
    "            with open(results_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No results file found for {regulation_name}\")\n",
    "            return {}\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error reading results file for {regulation_name}\")\n",
    "            return {}\n",
    "\n",
    "    def _create_exact_hypothesis_mapping(self):\n",
    "        \"\"\"Create exact 1:1 mapping from panel names to hypothesis files\"\"\"\n",
    "        panel_names = [\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Reputation_Risk\",\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Unsophisticated_Investors\",\n",
    "            \"panel_Capital_Market_Law_Lebanon_Corporate_Governance\",\n",
    "            \"panel_Capital_Market_Law_Lebanon_Equity_Issuance\",\n",
    "            \"panel_Capital_Market_Law_Lebanon_Information_Asymmetry\",\n",
    "            \"panel_Capital_Market_Law_Lebanon_Litigation_Risk\",\n",
    "            \"panel_Capital_Market_Law_Lebanon_Proprietary_Costs\",\n",
    "            \"panel_Capital_Market_Law_Lebanon_Reputation_Risk\",\n",
    "            \"panel_Capital_Market_Law_Lebanon_Unsophisticated_Investors\",\n",
    "            \"panel_Capital_Markets_Act_Uganda_Corporate_Governance\",\n",
    "            \"panel_Capital_Markets_Act_Uganda_Equity_Issuance\",\n",
    "            \"panel_Capital_Markets_Act_Uganda_Information_Asymmetry\",\n",
    "            \"panel_Capital_Markets_Act_Uganda_Litigation_Risk\",\n",
    "            \"panel_Capital_Markets_Act_Uganda_Proprietary_Costs\",\n",
    "            \"panel_Capital_Markets_Act_Uganda_Reputation_Risk\",\n",
    "            \"panel_Capital_Markets_Act_Uganda_Unsophisticated_Investors\",\n",
    "            \"panel_Capital_Markets_Law_Mexico_Corporate_Governance\",\n",
    "            \"panel_Capital_Markets_Law_Mexico_Equity_Issuance\",\n",
    "            \"panel_Capital_Markets_Law_Mexico_Information_Asymmetry\",\n",
    "            \"panel_Capital_Markets_Law_Mexico_Litigation_Risk\",\n",
    "            \"panel_Capital_Markets_Law_Mexico_Proprietary_Costs\",\n",
    "            \"panel_Capital_Markets_Law_Mexico_Reputation_Risk\",\n",
    "            \"panel_Capital_Markets_Law_Mexico_Unsophisticated_Investors\",\n",
    "            \"panel_European_Market_Infrastructure_Regulation_EMIR_European_Union_Corporate_Governance\",\n",
    "            \"panel_European_Market_Infrastructure_Regulation_EMIR_European_Union_Information_Asymmetry\",\n",
    "            \"panel_European_Market_Infrastructure_Regulation_EMIR_European_Union_Litigation_Risk\",\n",
    "            \"panel_European_Market_Infrastructure_Regulation_EMIR_European_Union_Proprietary_Costs\",\n",
    "            \"panel_European_Market_Infrastructure_Regulation_EMIR_European_Union_Reputation_Risk\",\n",
    "            \"panel_Financial_Instruments_and_Exchange_Act_Japan_Corporate_Governance\",\n",
    "            \"panel_Financial_Instruments_and_Exchange_Act_Japan_Equity_Issuance\",\n",
    "            \"panel_Financial_Instruments_and_Exchange_Act_Japan_Information_Asymmetry\",\n",
    "            \"panel_Financial_Instruments_and_Exchange_Act_Japan_Litigation_Risk\",\n",
    "            \"panel_Financial_Instruments_and_Exchange_Act_Japan_Reputation_Risk\",\n",
    "            \"panel_Financial_Instruments_and_Exchange_Act_Japan_Unsophisticated_Investors\",\n",
    "            \"panel_Financial_Market_Supervision_Act_Switzerland_Corporate_Governance\",\n",
    "            \"panel_Financial_Market_Supervision_Act_Switzerland_Equity_Issuance\",\n",
    "            \"panel_Financial_Market_Supervision_Act_Switzerland_Information_Asymmetry\",\n",
    "            \"panel_Financial_Market_Supervision_Act_Switzerland_Litigation_Risk\",\n",
    "            \"panel_Financial_Market_Supervision_Act_Switzerland_Reputation_Risk\",\n",
    "            \"panel_Financial_Market_Supervision_Act_Switzerland_Unsophisticated_Investors\",\n",
    "            \"panel_Financial_Services_Act_2012_United_Kingdom_Corporate_Governance\",\n",
    "            \"panel_Financial_Services_Act_2012_United_Kingdom_Information_Asymmetry\",\n",
    "            \"panel_Financial_Services_Act_2012_United_Kingdom_Litigation_Risk\",\n",
    "            \"panel_Financial_Services_Act_2012_United_Kingdom_Reputation_Risk\",\n",
    "            \"panel_Financial_Services_Act_2012_United_Kingdom_Unsophisticated_Investors\",\n",
    "            \"panel_Financial_Services_Law_Brazil_Corporate_Governance\",\n",
    "            \"panel_Financial_Services_Law_Brazil_Equity_Issuance\",\n",
    "            \"panel_Financial_Services_Law_Brazil_Information_Asymmetry\",\n",
    "            \"panel_Financial_Services_Law_Brazil_Litigation_Risk\",\n",
    "            \"panel_Financial_Services_Law_Brazil_Proprietary_Costs\",\n",
    "            \"panel_Financial_Services_Law_Brazil_Reputation_Risk\",\n",
    "            \"panel_Financial_Services_Law_Brazil_Unsophisticated_Investors\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_Italy_Corporate_Governance\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_Italy_Equity_Issuance\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_Italy_Information_Asymmetry\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_Italy_Litigation_Risk\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_Italy_Proprietary_Costs\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_Italy_Reputation_Risk\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_Italy_Unsophisticated_Investors\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_MiFID_European_Union_Corporate_Governance\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_MiFID_European_Union_Equity_Issuance\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_MiFID_European_Union_Information_Asymmetry\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_MiFID_European_Union_Litigation_Risk\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_MiFID_European_Union_Reputation_Risk\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_MiFID_European_Union_Unsophisticated_Investors\",\n",
    "            \"panel_National_Instrument_31103_Registration_Requirements_Canada_Corporate_Governance\",\n",
    "            \"panel_National_Instrument_31103_Registration_Requirements_Canada_Information_Asymmetry\",\n",
    "            \"panel_National_Instrument_31103_Registration_Requirements_Canada_Litigation_Risk\",\n",
    "            \"panel_National_Instrument_31103_Registration_Requirements_Canada_Reputation_Risk\",\n",
    "            \"panel_National_Instrument_31103_Registration_Requirements_Canada_Unsophisticated_Investors\",\n",
    "            \"panel_Securities_and_Exchange_Act_Ghana_Corporate_Governance\",\n",
    "            \"panel_Securities_and_Exchange_Act_Ghana_Equity_Issuance\",\n",
    "            \"panel_Securities_and_Exchange_Act_Ghana_Information_Asymmetry\",\n",
    "            \"panel_Securities_and_Exchange_Act_Ghana_Litigation_Risk\",\n",
    "            \"panel_Securities_and_Exchange_Act_Ghana_Proprietary_Costs\",\n",
    "            \"panel_Securities_and_Exchange_Act_Ghana_Reputation_Risk\",\n",
    "            \"panel_Securities_and_Exchange_Act_Ghana_Unsophisticated_Investors\",\n",
    "            \"panel_Securities_and_Exchange_Ordinance_Bangladesh_Corporate_Governance\",\n",
    "            \"panel_Securities_and_Exchange_Ordinance_Bangladesh_Equity_Issuance\",\n",
    "            \"panel_Securities_and_Exchange_Ordinance_Bangladesh_Information_Asymmetry\",\n",
    "            \"panel_Securities_and_Exchange_Ordinance_Bangladesh_Litigation_Risk\",\n",
    "            \"panel_Securities_and_Exchange_Ordinance_Bangladesh_Proprietary_Costs\",\n",
    "            \"panel_Securities_and_Exchange_Ordinance_Bangladesh_Reputation_Risk\",\n",
    "            \"panel_Securities_and_Exchange_Ordinance_Bangladesh_Unsophisticated_Investors\",\n",
    "            \"panel_Securities_Exchange_Act_Zambia_Corporate_Governance\",\n",
    "            \"panel_Securities_Exchange_Act_Zambia_Equity_Issuance\",\n",
    "            \"panel_Securities_Exchange_Act_Zambia_Information_Asymmetry\",\n",
    "            \"panel_Securities_Exchange_Act_Zambia_Litigation_Risk\",\n",
    "            \"panel_Securities_Exchange_Act_Zambia_Proprietary_Costs\",\n",
    "            \"panel_Securities_Exchange_Act_Zambia_Reputation_Risk\",\n",
    "            \"panel_Securities_Exchange_Act_Zambia_Unsophisticated_Investors\",\n",
    "            \"panel_Securities_Industry_Act_Trinidad_and_Tobago_Corporate_Governance\",\n",
    "            \"panel_Securities_Industry_Act_Trinidad_and_Tobago_Equity_Issuance\",\n",
    "            \"panel_Securities_Industry_Act_Trinidad_and_Tobago_Information_Asymmetry\",\n",
    "            \"panel_Securities_Industry_Act_Trinidad_and_Tobago_Litigation_Risk\",\n",
    "            \"panel_Securities_Industry_Act_Trinidad_and_Tobago_Proprietary_Costs\",\n",
    "            \"panel_Securities_Industry_Act_Trinidad_and_Tobago_Reputation_Risk\",\n",
    "            \"panel_Securities_Industry_Act_Trinidad_and_Tobago_Unsophisticated_Investors\",\n",
    "            \"panel_Securities_Law_Cambodia_Corporate_Governance\",\n",
    "            \"panel_Securities_Law_Cambodia_Equity_Issuance\",\n",
    "            \"panel_Securities_Law_Cambodia_Information_Asymmetry\",\n",
    "            \"panel_Securities_Law_Cambodia_Litigation_Risk\",\n",
    "            \"panel_Securities_Law_Cambodia_Proprietary_Costs\",\n",
    "            \"panel_Securities_Law_Cambodia_Reputation_Risk\",\n",
    "            \"panel_Securities_Law_Cambodia_Unsophisticated_Investors\",\n",
    "            \"panel_Securities_Law_China_Corporate_Governance\",\n",
    "            \"panel_Securities_Law_China_Equity_Issuance\",\n",
    "            \"panel_Securities_Law_China_Information_Asymmetry\",\n",
    "            \"panel_Securities_Law_China_Litigation_Risk\",\n",
    "            \"panel_Securities_Law_China_Proprietary_Costs\",\n",
    "            \"panel_Securities_Law_China_Reputation_Risk\",\n",
    "            \"panel_Securities_Law_China_Unsophisticated_Investors\",\n",
    "            \"panel_Securities_Market_Law_Laos_Corporate_Governance\",\n",
    "            \"panel_Securities_Market_Law_Laos_Equity_Issuance\",\n",
    "            \"panel_Securities_Market_Law_Laos_Information_Asymmetry\",\n",
    "            \"panel_Securities_Market_Law_Laos_Litigation_Risk\",\n",
    "            \"panel_Securities_Market_Law_Laos_Proprietary_Costs\",\n",
    "            \"panel_Securities_Market_Law_Laos_Reputation_Risk\",\n",
    "            \"panel_Securities_Market_Law_Laos_Unsophisticated_Investors\",\n",
    "            \"panel_Securities_Market_Law_Myanmar_Corporate_Governance\",\n",
    "            \"panel_Securities_Market_Law_Myanmar_Equity_Issuance\",\n",
    "            \"panel_Securities_Market_Law_Myanmar_Information_Asymmetry\",\n",
    "            \"panel_Securities_Market_Law_Myanmar_Litigation_Risk\",\n",
    "            \"panel_Securities_Market_Law_Myanmar_Proprietary_Costs\",\n",
    "            \"panel_Securities_Market_Law_Myanmar_Reputation_Risk\",\n",
    "            \"panel_Securities_Market_Law_Myanmar_Unsophisticated_Investors\",\n",
    "            \"panel_Securities_Market_Law_Pakistan_Corporate_Governance\",\n",
    "            \"panel_Securities_Market_Law_Pakistan_Equity_Issuance\",\n",
    "            \"panel_Securities_Market_Law_Pakistan_Information_Asymmetry\",\n",
    "            \"panel_Securities_Market_Law_Pakistan_Litigation_Risk\",\n",
    "            \"panel_Securities_Market_Law_Pakistan_Proprietary_Costs\",\n",
    "            \"panel_Securities_Market_Law_Pakistan_Reputation_Risk\",\n",
    "            \"panel_Securities_Market_Law_Pakistan_Unsophisticated_Investors\",\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Corporate_Governance\",\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Equity_Issuance\",\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Information_Asymmetry\",\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Litigation_Risk\",\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Proprietary_Costs\"\n",
    "        ]\n",
    "        \n",
    "        # Create the mapping dictionary\n",
    "        mapping = {}\n",
    "        for panel_name in panel_names:\n",
    "            # Remove \"panel_\" prefix to get the regulation name\n",
    "            regulation_name = panel_name[6:]  # Remove \"panel_\"\n",
    "            # Create hypothesis filename by adding suffix\n",
    "            hypothesis_filename = f\"{regulation_name}_background_hypothesis.txt\"\n",
    "            mapping[panel_name] = hypothesis_filename\n",
    "        \n",
    "        return mapping\n",
    "    \n",
    "    def _create_corrected_hypothesis_mapping(self):\n",
    "        \"\"\"Create corrected mapping that handles abbreviated hypothesis file names\"\"\"\n",
    "    \n",
    "        # Manual mapping for files with abbreviations\n",
    "        abbreviation_corrections = {\n",
    "            # AIFMD European Union -> Various abbreviations\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Corporate_Governance\": \n",
    "                \"Alternative_Investment_Fund_Managers_Directive_AIFMD_Eur_Corporate_Governance_background_hypothesis.txt\",\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Equity_Issuance\": \n",
    "                \"Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Equity_Issuance_background_hypothesis.txt\",\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Information_Asymmetry\": \n",
    "                \"Alternative_Investment_Fund_Managers_Directive_AIFMD_Eu_Information_Asymmetry_background_hypothesis.txt\",\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Litigation_Risk\": \n",
    "                \"Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Litigation_Risk_background_hypothesis.txt\",\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Proprietary_Costs\": \n",
    "                \"Alternative_Investment_Fund_Managers_Directive_AIFMD_Europe_Proprietary_Costs_background_hypothesis.txt\",\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Reputation_Risk\": \n",
    "                \"Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Reputation_Risk_background_hypothesis.txt\",\n",
    "            \"panel_Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union_Unsophisticated_Investors\": \n",
    "                \"Alternative_Investment_Fund_Managers_Directive_AIFMD_Eu_Unsophisticated_Investors_background_hypothesis.txt\",\n",
    "            \n",
    "            # EMIR European Union -> Various abbreviations\n",
    "            \"panel_European_Market_Infrastructure_Regulation_EMIR_European_Union_Corporate_Governance\": \n",
    "                \"European_Market_Infrastructure_Regulation_EMIR_European_Corporate_Governance_background_hypothesis.txt\",\n",
    "            \"panel_European_Market_Infrastructure_Regulation_EMIR_European_Union_Information_Asymmetry\": \n",
    "                \"European_Market_Infrastructure_Regulation_EMIR_European_Information_Asymmetry_background_hypothesis.txt\",\n",
    "            \"panel_European_Market_Infrastructure_Regulation_EMIR_European_Union_Litigation_Risk\": \n",
    "                \"European_Market_Infrastructure_Regulation_EMIR_European_Union_Litigation_Risk_background_hypothesis.txt\",\n",
    "            \"panel_European_Market_Infrastructure_Regulation_EMIR_European_Union_Proprietary_Costs\": \n",
    "                \"European_Market_Infrastructure_Regulation_EMIR_European_Uni_Proprietary_Costs_background_hypothesis.txt\",\n",
    "            \"panel_European_Market_Infrastructure_Regulation_EMIR_European_Union_Reputation_Risk\": \n",
    "                \"European_Market_Infrastructure_Regulation_EMIR_European_Union_Reputation_Risk_background_hypothesis.txt\",\n",
    "            \n",
    "            # MiFID European Union -> Various abbreviations\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_MiFID_European_Union_Corporate_Governance\": \n",
    "                \"Markets_in_Financial_Instruments_Directive_MiFID_Europea_Corporate_Governance_background_hypothesis.txt\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_MiFID_European_Union_Equity_Issuance\": \n",
    "                \"Markets_in_Financial_Instruments_Directive_MiFID_European_Uni_Equity_Issuance_background_hypothesis.txt\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_MiFID_European_Union_Information_Asymmetry\": \n",
    "                \"Markets_in_Financial_Instruments_Directive_MiFID_Europe_Information_Asymmetry_background_hypothesis.txt\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_MiFID_European_Union_Litigation_Risk\": \n",
    "                \"Markets_in_Financial_Instruments_Directive_MiFID_European_Uni_Litigation_Risk_background_hypothesis.txt\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_MiFID_European_Union_Reputation_Risk\": \n",
    "                \"Markets_in_Financial_Instruments_Directive_MiFID_European_Uni_Reputation_Risk_background_hypothesis.txt\",\n",
    "            \"panel_Markets_in_Financial_Instruments_Directive_MiFID_European_Union_Unsophisticated_Investors\": \n",
    "                \"Markets_in_Financial_Instruments_Directive_MiFID_Eu_Unsophisticated_Investors_background_hypothesis.txt\",\n",
    "            \n",
    "            # Canada abbreviations\n",
    "            \"panel_National_Instrument_31103_Registration_Requirements_Canada_Corporate_Governance\": \n",
    "                \"National_Instrument_31103_Registration_Requirements_Cana_Corporate_Governance_background_hypothesis.txt\",\n",
    "            \"panel_National_Instrument_31103_Registration_Requirements_Canada_Information_Asymmetry\": \n",
    "                \"National_Instrument_31103_Registration_Requirements_Can_Information_Asymmetry_background_hypothesis.txt\",\n",
    "            \"panel_National_Instrument_31103_Registration_Requirements_Canada_Litigation_Risk\": \n",
    "                \"National_Instrument_31103_Registration_Requirements_Canada_Litigation_Risk_background_hypothesis.txt\",\n",
    "            \"panel_National_Instrument_31103_Registration_Requirements_Canada_Reputation_Risk\": \n",
    "                \"National_Instrument_31103_Registration_Requirements_Canada_Reputation_Risk_background_hypothesis.txt\",\n",
    "            \"panel_National_Instrument_31103_Registration_Requirements_Canada_Unsophisticated_Investors\": \n",
    "                \"National_Instrument_31103_Registration_Requirements_Unsophisticated_Investors_background_hypothesis.txt\"\n",
    "        }\n",
    "    \n",
    "        # Start with the standard mapping\n",
    "        standard_mapping = self._create_exact_hypothesis_mapping()\n",
    "    \n",
    "        # Override with corrected abbreviations\n",
    "        standard_mapping.update(abbreviation_corrections)\n",
    "    \n",
    "        return standard_mapping\n",
    "\n",
    "    def read_hypothesis(self, regulation_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Read hypothesis file using corrected mapping that handles abbreviations\n",
    "        \"\"\"\n",
    "        # Use the corrected mapping that handles abbreviations\n",
    "        mapping = self._create_corrected_hypothesis_mapping()\n",
    "    \n",
    "        # Check if we have a mapping (either standard or corrected)\n",
    "        if regulation_name in mapping:\n",
    "            hypothesis_filename = mapping[regulation_name]\n",
    "        \n",
    "            # Set up hypothesis directory  \n",
    "            hypothesis_dir = os.path.join(os.path.dirname(self.output_dir), \n",
    "                                    'background and hypothesis development')\n",
    "        \n",
    "            hypothesis_file = os.path.join(hypothesis_dir, hypothesis_filename)\n",
    "        \n",
    "            print(f\"Looking for corrected match: {hypothesis_filename}\")\n",
    "        \n",
    "            try:\n",
    "                with open(hypothesis_file, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Extract hypothesis development section\n",
    "                if \"Hypothesis Development\" in content:\n",
    "                    hypothesis_section = content.split(\"Hypothesis Development\")[1]\n",
    "                \n",
    "                    if \"H1:\" in hypothesis_section:\n",
    "                        hypothesis_development = hypothesis_section.split(\"H1:\")[0].strip()\n",
    "                        h1_statement = \"H1:\" + hypothesis_section.split(\"H1:\")[1].strip()\n",
    "                        return f\"Hypothesis Development:\\n\\n{hypothesis_development}\\n\\n{h1_statement}\"\n",
    "                    else:\n",
    "                        return f\"Hypothesis Development:\\n\\n{hypothesis_section.strip()}\"\n",
    "                else:\n",
    "                    print(f\"No Hypothesis Development section found in {hypothesis_filename}\")\n",
    "                    return content  # Return full content if no specific section found\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(f\"Hypothesis file not found: {hypothesis_file}\")\n",
    "                return \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading hypothesis file {hypothesis_filename}: {str(e)}\")\n",
    "                return \"\"\n",
    "        else:\n",
    "            print(f\"No mapping found for regulation: {regulation_name}\")\n",
    "            return \"\"\n",
    "\n",
    "    def format_results_text(self, regulation_title: str, regulation_year: int, results: Dict) -> str:\n",
    "        \"\"\"Format regression results into text for the academic prompt\"\"\"\n",
    "        results_text = f\"Regression Analysis for {regulation_title} (Year: {regulation_year})\\n\\n\"\n",
    "        \n",
    "        for spec_name, res in results.items():\n",
    "            results_text += f\"\\nSpecification {spec_name}:\\n\"\n",
    "            results_text += f\"Treatment Effect: {res['coefficients']['treatment_effect']:.4f}\\n\"\n",
    "            results_text += f\"T-statistic: {res['t_stats']['treatment_effect']:.2f}\\n\"\n",
    "            results_text += f\"P-value: {res['pvalues']['treatment_effect']:.4f}\\n\"\n",
    "            results_text += f\"R-squared: {res['r_squared']:.4f}\\n\"\n",
    "            results_text += f\"Number of observations: {int(res['n_obs'])}\\n\"\n",
    "            results_text += f\"Number of firms: {res['n_firms']}\\n\"\n",
    "            \n",
    "            if res['controls']:\n",
    "                results_text += \"\\nControl Variables:\\n\"\n",
    "                for control in res['controls']:\n",
    "                    coef = res['coefficients'][control]\n",
    "                    tstat = res['t_stats'][control]\n",
    "                    pvalue = res['pvalues'][control]\n",
    "                    stars = self._get_significance_stars(pvalue)\n",
    "                    results_text += f\"{control}: {coef:.4f}{stars} (t={tstat:.2f}, p={pvalue:.4f})\\n\"\n",
    "            \n",
    "            results_text += \"\\nFixed Effects:\\n\"\n",
    "            for fe, included in res['fixed_effects'].items():\n",
    "                results_text += f\"{fe}: {'Yes' if included else 'No'}\\n\"\n",
    "            \n",
    "            results_text += \"-\" * 50 + \"\\n\"\n",
    "        \n",
    "        return results_text\n",
    "\n",
    "    def generate_claude_interpretation(self, regulation_title: str, regulation_year: int, results_text: str, hypothesis_text: str) -> str:\n",
    "        \"\"\"Generate interpretation using Claude API\"\"\"\n",
    "        prompt = f\"\"\"You are an accounting academic with a PhD in accounting. \n",
    "        You should use active voice (e.g. \"We find\" instead of \"It is found\"). \n",
    "        Use present tense for all established findings. \n",
    "        Distinguish between correlation and causation. \n",
    "        Write the results description for this analysis as if you were writing an academic paper for an accounting journal, \n",
    "        you are studying the association between a change in mandatory disclosure and voluntary disclosure in U.S. firms. \n",
    "        \n",
    "        Here is the hypothesis that was developed:\n",
    "        {hypothesis_text}\n",
    "        \n",
    "        Please provide a detailed academic analysis of these regression results:\n",
    "\n",
    "{results_text}\n",
    "\n",
    "Please structure your analysis as follows (3 paragraphs, ~600 words total):\n",
    "1. Label this section Regression Analysis\n",
    "2. Main finding (treatment effect interpretation)\n",
    "3. Statistical significance and economic magnitude\n",
    "4. Model specification comparison\n",
    "5. Control variable effects\n",
    "   Describe whether the relationship is consistent with prior literature\n",
    "6. Explain whether the results support the hypothesis stated in the Hypothesis section above\n",
    "\n",
    "Write in an academic style suitable for a top accounting journal.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting Claude interpretation: {str(e)}\")\n",
    "            return f\"Error in Claude analysis: {str(e)}\"\n",
    "        \n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        # Remove headers (##, ###, etc.)\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove bold/italic formatting - this handles **text**, ***text***, ****text****\n",
    "        content = re.sub(r'\\*{1,4}([^*]*?)\\*{1,4}', r'\\1', content)\n",
    "        \n",
    "        # Clean up any remaining asterisks\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        \n",
    "        # Clean up any extra whitespace that might be left\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        \n",
    "        return content    \n",
    "    \n",
    "\n",
    "    def interpret_regulation_impact(self, regulation_name: str, force_rerun: bool = False) -> str:\n",
    "        \"\"\"Generate interpretation for a single regulation's results\"\"\"\n",
    "\n",
    "        # FIRST: Check if regression results exist BEFORE creating any folders\n",
    "        regulation_dir = os.path.join(self.output_dir, regulation_name)\n",
    "        results_path = os.path.join(regulation_dir, 'regression_results.json')\n",
    "    \n",
    "        if not os.path.exists(results_path):\n",
    "            print(f\"No regression_results.json found for {regulation_name}\")\n",
    "            return \"\"\n",
    "\n",
    "        # Create subfolder only AFTER confirming results exist\n",
    "        os.makedirs(regulation_dir, exist_ok=True)\n",
    "    \n",
    "        # Check if interpretation already exists\n",
    "        claude_path = os.path.join(regulation_dir, 'claude_interpretation.txt')\n",
    "        if os.path.exists(claude_path) and not force_rerun:\n",
    "            print(f\"Interpretation already exists for {regulation_name}, skipping...\")\n",
    "            with open(claude_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "\n",
    "        # Check if regression results exist\n",
    "        results = self.read_regression_results(regulation_name)\n",
    "        if not results:\n",
    "            print(f\"No regression results found for {regulation_name}\")\n",
    "            return \"\"\n",
    "\n",
    "        # Check if hypothesis exists\n",
    "        hypothesis_text = self.read_hypothesis(regulation_name)\n",
    "        if not hypothesis_text:\n",
    "            print(f\"No hypothesis found for {regulation_name}\")\n",
    "            return \"\"\n",
    "\n",
    "        print(f\"Both regression results and hypothesis found for {regulation_name} - proceeding with interpretation\")\n",
    "\n",
    "        # Read the original panel file to get regulation title\n",
    "        panel_file = os.path.join(self.input_dir, f\"{regulation_name}.csv\")\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = pd.read_csv(panel_file)\n",
    "            regulation_title = df['Regulation Title'].iloc[0]\n",
    "            regulation_year = df['Year'].iloc[0]\n",
    "        except:\n",
    "            regulation_title = regulation_name\n",
    "            regulation_year = \"N/A\"\n",
    "\n",
    "        # Format results text\n",
    "        results_text = self.format_results_text(regulation_title, regulation_year, results)\n",
    "\n",
    "        # Generate interpretation using Claude\n",
    "        interpretation = self.generate_claude_interpretation(\n",
    "            regulation_title, \n",
    "            regulation_year, \n",
    "            results_text,\n",
    "            hypothesis_text\n",
    "        )\n",
    "\n",
    "        # Clean markdown formatting before saving\n",
    "        clean_interpretation = self.clean_markdown_formatting(interpretation)\n",
    "\n",
    "        # Save interpretation to file\n",
    "        try:\n",
    "            with open(claude_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(clean_interpretation)\n",
    "            print(f\"Saved interpretation to {claude_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving interpretation to file: {str(e)}\")\n",
    "\n",
    "        return clean_interpretation\n",
    "\n",
    "    def analyze_all_regulations(self, force_rerun: bool = False) -> None:\n",
    "        \"\"\"Analyze results for all regulations in the directory\"\"\"\n",
    "    \n",
    "        # Get all panel files first\n",
    "        panel_files = glob.glob(os.path.join(self.input_dir, \"panel_*_*.csv\"))\n",
    "        print(f\"Found {len(panel_files)} panel CSV files\")\n",
    "    \n",
    "        # Filter to only those that have regression results\n",
    "        regulations_with_results = []\n",
    "        regulations_without_results = []\n",
    "    \n",
    "        for panel_file in panel_files:\n",
    "            regulation_name = os.path.splitext(os.path.basename(panel_file))[0]\n",
    "            results_path = os.path.join(self.output_dir, regulation_name, 'regression_results.json')\n",
    "        \n",
    "            if os.path.exists(results_path):\n",
    "                regulations_with_results.append(regulation_name)\n",
    "            else:\n",
    "                regulations_without_results.append(regulation_name)\n",
    "    \n",
    "        print(f\"Found {len(regulations_with_results)} regulations WITH regression results\")\n",
    "        print(f\"Found {len(regulations_without_results)} regulations WITHOUT regression results\")\n",
    "    \n",
    "        if regulations_without_results:\n",
    "            print(\"\\nRegulations WITHOUT regression results (will be skipped):\")\n",
    "            for reg in regulations_without_results[:5]:  # Show first 5\n",
    "                print(f\"  - {reg}\")\n",
    "            if len(regulations_without_results) > 5:\n",
    "                print(f\"  ... and {len(regulations_without_results) - 5} more\")\n",
    "    \n",
    "        print(f\"\\nProcessing {len(regulations_with_results)} regulations with results...\")\n",
    "        print(f\"Force rerun: {force_rerun}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        skipped = 0\n",
    "    \n",
    "        for regulation_name in regulations_with_results:\n",
    "            # Quick check if interpretation already exists\n",
    "            regulation_dir = os.path.join(self.output_dir, regulation_name)\n",
    "            claude_path = os.path.join(regulation_dir, 'claude_interpretation.txt')\n",
    "        \n",
    "            if os.path.exists(claude_path) and not force_rerun:\n",
    "                print(f\"SKIPPED: {regulation_name} - interpretation already exists\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "        \n",
    "            try:\n",
    "                print(f\"\\n[{successful + failed + 1}/{len(regulations_with_results)}] PROCESSING: {regulation_name}\")\n",
    "                print(\"=\"*80)\n",
    "                result = self.interpret_regulation_impact(regulation_name, force_rerun)\n",
    "            \n",
    "                if result:\n",
    "                    print(f\"✅ SUCCESS: Generated interpretation\")\n",
    "                    successful += 1\n",
    "                else:\n",
    "                    print(f\"❌ FAILED: No interpretation generated\")\n",
    "                    failed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ ERROR: {str(e)}\")\n",
    "                failed += 1\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FINAL SUMMARY:\")\n",
    "        print(f\"✅ Successful: {successful}\")\n",
    "        print(f\"❌ Failed: {failed}\")\n",
    "        print(f\"⏭️  Skipped (already done): {skipped}\")\n",
    "        print(f\"📁 Regulations without results: {len(regulations_without_results)}\")\n",
    "        print(f\"📊 Total regulations with results: {len(regulations_with_results)}\")\n",
    "        print(f\"📋 Total panel files found: {len(panel_files)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"  # Replace with your Claude API key\n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    INPUT_DIR = os.path.join(BASE_DIR, \"folder path here\")\n",
    "    OUTPUT_DIR = os.path.join(BASE_DIR, \"folder path here\")\n",
    "    \n",
    "    interpreter = RegressionInterpreter(INPUT_DIR, OUTPUT_DIR, API_KEY)\n",
    "    interpreter.analyze_all_regulations(force_rerun=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()   \n",
    "\n",
    "# 8. Create Correlation tables\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_CENTER, TA_LEFT \n",
    "from scipy import stats\n",
    "from reportlab.lib.pagesizes import letter, landscape\n",
    "\n",
    "def add_underscores_before_capitals(text):\n",
    "    \"\"\"Add underscores before capital letters in a string\"\"\"\n",
    "    return re.sub(r'(?<=[a-z0-9])(?=[A-Z])', '_', text)\n",
    "\n",
    "def create_correlation_table(data_path, output_dir):\n",
    "    \"\"\"\n",
    "    Creates a clean correlation table PDF in the style of academic papers.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the panel data CSV\n",
    "        output_dir (str): Path to save output files\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Select numerical variables for correlation\n",
    "    numeric_vars = ['treatment_effect','freqMF','linstown', 'lsize', 'lbtm', 'lroa', 'lsaret12', 'levol', 'lloss', \n",
    "                    'lcalrisk']\n",
    "                   \n",
    "    # Create shorter variable names for the table\n",
    "    var_mapping = {\n",
    "        'treatment_effect': 'Treatment Effect',\n",
    "        'freqMF': 'FreqMF',\n",
    "        'linstown': 'Institutional ownership',\n",
    "        'lsize': 'Firm size',\n",
    "        'lbtm': 'Book-to-market',\n",
    "        'lroa': 'ROA',\n",
    "        'lsaret12': 'Stock return',\n",
    "        'levol': 'Earnings volatility',\n",
    "        'lloss': 'Loss',\n",
    "        'lcalrisk': 'Class action litigation risk'\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df[numeric_vars].corr()\n",
    "    \n",
    "    # Calculate p-values for significance testing\n",
    "    def calculate_pvalue(x, y):\n",
    "        return stats.pearsonr(x.dropna(), y.dropna())[1]\n",
    "    \n",
    "    p_values = pd.DataFrame(index=numeric_vars, columns=numeric_vars)\n",
    "    for i in numeric_vars:\n",
    "        for j in numeric_vars:\n",
    "            p_values.loc[i,j] = calculate_pvalue(df[i], df[j])\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get panel name from path\n",
    "    panel_name = os.path.basename(os.path.dirname(data_path))\n",
    "    \n",
    "    # Create PDF\n",
    "    clean_name = panel_name.replace('panel_', '')\n",
    "    pdf_path = os.path.join(output_dir, f'{clean_name}_correlation_table.pdf')\n",
    "    doc = SimpleDocTemplate(pdf_path, pagesize=landscape(letter), rightMargin=30, leftMargin=30, topMargin=50, bottomMargin=50)\n",
    "    \n",
    "    # Prepare table data\n",
    "    table_data = [['']]  # First cell empty\n",
    "    \n",
    "    # Add column headers\n",
    "    for var in numeric_vars:\n",
    "        table_data[0].append(var_mapping[var])\n",
    "    \n",
    "    # Add rows\n",
    "    for i, var1 in enumerate(numeric_vars, 1):\n",
    "        row = [var_mapping[var1]]  # Row header\n",
    "        for var2 in numeric_vars:\n",
    "            if var1 == var2:\n",
    "                row.append('1.00')\n",
    "            else:\n",
    "                value = corr_matrix.loc[var1, var2]\n",
    "                # Format to 2 decimal places\n",
    "                formatted_value = f'{value:.2f}'\n",
    "                row.append(formatted_value)\n",
    "        table_data.append(row)\n",
    "    \n",
    "    # Create table style\n",
    "    style = [\n",
    "        ('FONTNAME', (0,0), (-1,-1), 'Times-Roman'),\n",
    "        ('FONTSIZE', (0,0), (-1,-1), 8),\n",
    "        ('ALIGN', (0,0), (-1,-1), 'CENTER'),\n",
    "        ('TOPPADDING', (0,0), (-1,-1), 3),\n",
    "        ('BOTTOMPADDING', (0,0), (-1,-1), 3),\n",
    "        ('GRID', (0,0), (-1,-1), 0.25, colors.black),  # Lighter grid lines\n",
    "        ('BOX', (0,0), (-1,-1), 0.25, colors.black),\n",
    "        # Make column headers and row headers bold\n",
    "        ('FONTNAME', (0,0), (-1,0), 'Times-Bold'),\n",
    "        ('FONTNAME', (0,0), (0,-1), 'Times-Bold'),\n",
    "    ]\n",
    "    \n",
    "    # Add bold style for significant correlations\n",
    "    for i in range(1, len(table_data)):\n",
    "        for j in range(1, len(table_data[0])):\n",
    "            if i != j:  # Skip diagonal\n",
    "                var1 = numeric_vars[i-1]\n",
    "                var2 = numeric_vars[j-1]\n",
    "                if p_values.loc[var1,var2] < 0.05:  # 5% significance level\n",
    "                    style.append(('FONTNAME', (j,i), (j,i), 'Times-Bold'))\n",
    "    \n",
    "    # Create table\n",
    "    table = Table(table_data)\n",
    "    table.setStyle(TableStyle(style))\n",
    "    \n",
    "    # Create title\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=12,\n",
    "        alignment=TA_CENTER,\n",
    "        spaceBefore=12,\n",
    "        spaceAfter=20,\n",
    "        fontName='Times-Bold'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Create Panel title in smaller text if needed\n",
    "    panel_title = \"\"\n",
    "    if panel_name:\n",
    "        clean_panel_name = panel_name.replace('panel_', '').replace('_', ' ')\n",
    "        # Format the law name with proper spacing\n",
    "        if 'NominatingCommitteeRequirements' in clean_panel_name:\n",
    "            law_name = 'Nominating Committee Requirements'\n",
    "        elif 'ResourceExtractionDisclosureRules' in clean_panel_name:\n",
    "            law_name = 'Resource Extraction Disclosure Rules'\n",
    "        elif 'PayRatioDisclosureRule' in clean_panel_name:\n",
    "            law_name = 'Pay Ratio Disclosure Rule'\n",
    "        else:\n",
    "            law_name = clean_panel_name\n",
    "    \n",
    "        panel_title = f\"<br/>{law_name}\"\n",
    "    \n",
    "    title = Paragraph(f\"Table 2<br/>Pearson Correlations{panel_title}\", title_style)\n",
    "    \n",
    "    # Add footnote\n",
    "    footnote_style = ParagraphStyle(\n",
    "        'Footnote',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=8,\n",
    "        alignment=TA_LEFT,\n",
    "        fontName='Times-Roman',\n",
    "        spaceBefore=6,\n",
    "        leading=10  # Controls line spacing\n",
    "    )\n",
    "    footnote = Paragraph(\"This table shows the Pearson correlations for the sample. \"\n",
    "                        \"Correlations that are significant at the 0.05 level or better are highlighted in bold. \", footnote_style)\n",
    "    \n",
    "    # Build PDF\n",
    "    doc.build([title, table, Spacer(1, 12), footnote])\n",
    "    \n",
    "    print(f\"Created correlation table PDF for {panel_name}\")\n",
    "    return pdf_path\n",
    "\n",
    "def batch_process_panels(base_dir,output_base_dir):\n",
    "    \"\"\"\n",
    "    Process all panel folders and create correlation tables, skipping existing ones.\n",
    "    \"\"\"\n",
    "    print(f\"Starting to process panels in: {base_dir}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = os.path.join(r\"enter folder path here\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "    \n",
    "    # Count total panels\n",
    "    panel_folders = [f for f in os.listdir(base_dir) if f.startswith('panel_')]\n",
    "    total_panels = len(panel_folders)\n",
    "    processed = 0\n",
    "    skipped = 0\n",
    "    errors = 0\n",
    "    \n",
    "    print(f\"\\nFound {total_panels} panel folders to process\")\n",
    "    \n",
    "    # Process each panel folder\n",
    "    for i, panel_folder in enumerate(panel_folders, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing panel {i} of {total_panels}: {panel_folder}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        panel_path = os.path.join(base_dir, panel_folder)\n",
    "        \n",
    "        # Check if correlation table already exists\n",
    "        clean_name = panel_folder.replace('panel_', '')  # Keep as-is since already formatted\n",
    "        existing_table = os.path.join(output_dir, f'{clean_name}_correlation_table.pdf')\n",
    "        \n",
    "        if os.path.exists(existing_table):\n",
    "            print(f\"Skipping {panel_folder}: Correlation table already exists\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        # Look for the data file\n",
    "        data_file = 'filtered_data_with_trends.csv'\n",
    "        data_path = os.path.join(panel_path, data_file)\n",
    "            \n",
    "        if os.path.exists(data_path):\n",
    "            try:\n",
    "                table_path = create_correlation_table(data_path, output_dir)\n",
    "                print(f\"Created correlation table for {panel_folder}\")\n",
    "                print(f\"Table saved to: {table_path}\")\n",
    "                processed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {panel_folder}: {str(e)}\")\n",
    "                errors += 1\n",
    "        else:\n",
    "            print(f\"No data file found in {panel_folder}\")\n",
    "            errors += 1\n",
    "        \n",
    "        # Print progress summary\n",
    "        print(f\"\\nProgress Summary:\")\n",
    "        print(f\"Processed: {i}/{total_panels} ({(i/total_panels)*100:.1f}%)\")\n",
    "        print(f\"Successfully created: {processed}\")\n",
    "        print(f\"Skipped (already exist): {skipped}\")\n",
    "        print(f\"Errors: {errors}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # Base directory containing panel folders\n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    OUTPUT_BASE_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    # Process all panels\n",
    "    batch_process_panels(BASE_DIR, OUTPUT_BASE_DIR)\n",
    "    \n",
    "# 9. Send sample and descriptive statistics results to Claude for interpretation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from typing import Dict, List\n",
    "import json\n",
    "from anthropic import Anthropic\n",
    "import traceback\n",
    "from fpdf import FPDF\n",
    "import re\n",
    "\n",
    "class DescriptiveStatsAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "\n",
    "    def convert_numpy_types(self, obj):\n",
    "        \"\"\"Convert numpy/pandas types to native Python types for JSON serialization\"\"\"\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self.convert_numpy_types(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.convert_numpy_types(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    \n",
    "    def calculate_descriptive_stats(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Calculate descriptive statistics for the dataset\"\"\"\n",
    "        # List of numeric columns to analyze (excluding GVKEY, FYEAR, etc.)\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col not in ['GVKEY', 'FYEAR', 'sic3', 'Year']]\n",
    "        \n",
    "        # Sort columns to match example order if possible\n",
    "        preferred_order = [\n",
    "            'linstown', 'lsize', 'lbtm', 'lroa', 'lsaret12', 'levol', 'lloss', 'lcalrisk'\n",
    "        ]\n",
    "        sorted_cols = sorted(numeric_cols, key=lambda x: \n",
    "                           preferred_order.index(x) if x in preferred_order else float('inf'))\n",
    "        \n",
    "        stats = {}\n",
    "        for col in sorted_cols:  # Use sorted_cols instead of numeric_cols\n",
    "            col_stats = {\n",
    "                'n': len(df[col].dropna()),\n",
    "                'mean': df[col].mean(),\n",
    "                'median': df[col].median(),\n",
    "                'std': df[col].std(),\n",
    "                'p25': df[col].quantile(0.25),\n",
    "                'p75': df[col].quantile(0.75),\n",
    "                'min': df[col].min(),\n",
    "                'max': df[col].max()\n",
    "            }\n",
    "            stats[col] = col_stats\n",
    "        \n",
    "        # Add additional summary statistics\n",
    "        summary_stats = {\n",
    "            'total_observations': len(df),\n",
    "            'unique_firms': len(df['GVKEY'].unique()),\n",
    "            'year_range': f\"{df['FYEAR'].min()} to {df['FYEAR'].max()}\",\n",
    "            'industries': len(df['sic3'].unique())\n",
    "        }\n",
    "\n",
    "        # Convert numpy types to Python types\n",
    "        summary_stats = self.convert_numpy_types(summary_stats)\n",
    "        stats['summary'] = summary_stats\n",
    "\n",
    "        stats = self.convert_numpy_types(stats)\n",
    "        return stats\n",
    "    \n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        # Remove headers (##, ###, etc.)\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove bold/italic formatting - this handles **text**, ***text***, ****text****\n",
    "        content = re.sub(r'\\*{1,4}([^*]*?)\\*{1,4}', r'\\1', content)\n",
    "        \n",
    "        # Clean up any remaining asterisks\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        \n",
    "        # Clean up any extra whitespace that might be left\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def get_claude_interpretation(self, stats: Dict, regulation_title: str) -> str:\n",
    "        \"\"\"Get Claude's interpretation of descriptive statistics\"\"\"\n",
    "        # Format statistics for Claude\n",
    "        stats_text = f\"Descriptive Statistics for {regulation_title}\\n\\n\"\n",
    "        \n",
    "        # Add summary information\n",
    "        summary = stats['summary']\n",
    "        stats_text += \"Sample Characteristics:\\n\"\n",
    "        stats_text += f\"Total observations: {summary['total_observations']:,}\\n\"\n",
    "        stats_text += f\"Number of unique firms: {summary['unique_firms']:,}\\n\"\n",
    "        stats_text += f\"Sample period: {summary['year_range']}\\n\"\n",
    "        stats_text += f\"Number of industries: {summary['industries']}\\n\\n\"\n",
    "        \n",
    "        # Add variable statistics\n",
    "        stats_text += \"Variable Statistics:\\n\"\n",
    "        for var, var_stats in {k: v for k, v in stats.items() if k != 'summary'}.items():\n",
    "            stats_text += f\"\\n{var}:\\n\"\n",
    "            stats_text += f\"N: {var_stats['n']:,}\\n\"\n",
    "            stats_text += f\"Mean: {var_stats['mean']:.3f}\\n\"\n",
    "            stats_text += f\"Median: {var_stats['median']:.3f}\\n\"\n",
    "            stats_text += f\"Std Dev: {var_stats['std']:.3f}\\n\"\n",
    "            stats_text += f\"25th percentile: {var_stats['p25']:.3f}\\n\"\n",
    "            stats_text += f\"75th percentile: {var_stats['p75']:.3f}\\n\"\n",
    "            stats_text += f\"Min: {var_stats['min']:.3f}\\n\"\n",
    "            stats_text += f\"Max: {var_stats['max']:.3f}\\n\"\n",
    "        \n",
    "        # Create prompt for Claude\n",
    "        prompt = f\"\"\"You are an accounting academic with a PhD in accounting. \n",
    "        You should use active voice (e.g. \"We find\" instead of \"It is found\"). \n",
    "        Use present tense for all established findings. Write the descriptive statistics section for this analysis as if \n",
    "        you were writing an academic paper for an accounting journal. The descriptive statistics are for U.S. firms. \n",
    "        Here are the descriptive statistics:\n",
    "\n",
    "{stats_text}\n",
    "\n",
    "Please structure your analysis as follows (400 words):\n",
    "1. Label this section \"Sample Description and Descriptive Statistics\"\n",
    "2. Describe the sample characteristics (number of firms, time period)\n",
    "3. Describe the key variables' distributions\n",
    "4. Highlight any notable patterns or potential outliers\n",
    "5. Compare statistics to relevant benchmarks from prior literature where applicable\n",
    "\n",
    "IMPORTANT: DO NOT include the number of industries in the sample. For example, DO NOT write that the sample represents\n",
    "a specific number of industries. \n",
    "\n",
    "Write in an academic style suitable for a top accounting journal.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting Claude interpretation: {str(e)}\")\n",
    "            return f\"Error in Claude analysis: {str(e)}\"\n",
    "    \n",
    "    def create_descriptive_stats_table(self, stats: Dict, output_path: str, regulation_title: str):\n",
    "        \"\"\"Create a PDF table of descriptive statistics in academic paper format\"\"\"\n",
    "        try:\n",
    "            pdf = FPDF(format='A4', orientation='L')\n",
    "            pdf.add_page()\n",
    "        \n",
    "            # Set Times New Roman font\n",
    "            try:\n",
    "                pdf.add_font('Times', '', 'times.ttf', uni=True)\n",
    "                pdf.add_font('Times', 'B', 'timesbd.ttf', uni=True)\n",
    "                pdf.set_font('Times', size=11)\n",
    "            except:\n",
    "                pdf.set_font('Times', size=11)\n",
    "        \n",
    "            pdf.set_margins(20, 20, 20)\n",
    "        \n",
    "            # Title\n",
    "            pdf.set_font('Times', 'B', 14)\n",
    "            pdf.cell(0, 10, 'Table 1', align='C', ln=True)\n",
    "            pdf.set_font('Times', '', 12)\n",
    "            pdf.cell(0, 10, 'Descriptive Statistics', align='C', ln=True)\n",
    "            pdf.ln(5)\n",
    "        \n",
    "            # Calculate column widths\n",
    "            var_width = 70\n",
    "            num_width = 30\n",
    "        \n",
    "            # Table headers\n",
    "            pdf.set_font('Times', 'B')\n",
    "            headers = ['Variables', 'N', 'Mean', 'Std. Dev.', 'P25', 'Median', 'P75']\n",
    "            pdf.cell(var_width, 8, headers[0], border=1)\n",
    "            for header in headers[1:]:\n",
    "                pdf.cell(num_width, 8, header, border=1, align='C')\n",
    "            pdf.ln()\n",
    "        \n",
    "            # Variable name mappings with ordered display\n",
    "            var_display_names = {\n",
    "                'freqMF': 'FreqMF',\n",
    "                'treatment_effect': 'Treatment Effect',\n",
    "                'linstown': 'Institutional ownership',\n",
    "                'lsize': 'Firm size',\n",
    "                'lbtm': 'Book-to-market',\n",
    "                'lroa': 'ROA',\n",
    "                'lsaret12': 'Stock return',\n",
    "                'levol': 'Earnings volatility',\n",
    "                'lloss': 'Loss',\n",
    "                'lcalrisk': 'Class action litigation risk',\n",
    "                'time_trend': 'Time Trend'\n",
    "            }\n",
    "        \n",
    "            # Excluded variables\n",
    "            excluded_vars = {'sic4', 'permno', 'post-law', 'treated'}\n",
    "        \n",
    "            # Sort variables to ensure FreqMF is first\n",
    "            variables = {k: v for k, v in stats.items() \n",
    "                        if k != 'summary' and k not in excluded_vars}\n",
    "        \n",
    "            # Define display order\n",
    "            display_order = ['freqMF', 'treatment_effect'] + [\n",
    "                k for k in var_display_names.keys() \n",
    "                if k not in ['freqMF', 'treatment_effect']\n",
    "            ]\n",
    "        \n",
    "            pdf.set_font('Times', '')\n",
    "            for var_name in display_order:\n",
    "                if var_name in variables:\n",
    "                    var_stats = variables[var_name]\n",
    "                    display_name = var_display_names.get(var_name, var_name)\n",
    "                    pdf.cell(var_width, 8, display_name, border=1)\n",
    "                \n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['n']:,}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['mean']:.4f}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['std']:.4f}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['p25']:.4f}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['median']:.4f}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['p75']:.4f}\", border=1, align='C')\n",
    "                    pdf.ln()\n",
    "        \n",
    "            # Footnote\n",
    "            pdf.ln(10)\n",
    "            pdf.set_font('Times', '', 10)\n",
    "            footnote = \"This table shows the descriptive statistics. All continuous variables are winsorized at the 1st and 99th percentiles.\"\n",
    "            pdf.multi_cell(0, 5, footnote)\n",
    "        \n",
    "            pdf.output(output_path)\n",
    "            print(f\"Successfully saved descriptive statistics table to {output_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating descriptive statistics table: {str(e)}\")\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        \n",
    "            try:\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Times\", size=12)\n",
    "                pdf.cell(0, 10, \"Error occurred while creating descriptive statistics table\")\n",
    "                pdf.ln()\n",
    "                pdf.cell(0, 10, f\"Error: {str(e)}\")\n",
    "                pdf.output(output_path)\n",
    "            except Exception as e2:\n",
    "                print(f\"Emergency PDF save also failed: {str(e2)}\")\n",
    "    \n",
    "    def analyze_panel(self, panel_dir: str, output_dir: str) -> None:\n",
    "        \"\"\"Analyze descriptive statistics for a single panel dataset\"\"\"\n",
    "        try:\n",
    "            # Get panel name from directory name\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            print(f\"\\nAnalyzing {panel_name}...\")\n",
    "            \n",
    "            # Read filtered data\n",
    "            data_file = os.path.join(panel_dir, 'filtered_data_with_trends.csv')\n",
    "            if not os.path.exists(data_file):\n",
    "                print(f\"No filtered_data_with_trends.csv found in {panel_dir}\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Reading data from {data_file}\")\n",
    "            df = pd.read_csv(data_file)\n",
    "            \n",
    "            # Create output directory\n",
    "            panel_output_dir = os.path.join(output_dir, panel_name)\n",
    "            os.makedirs(panel_output_dir, exist_ok=True)\n",
    "            print(f\"Created output directory: {panel_output_dir}\")\n",
    "            \n",
    "            # Calculate descriptive statistics\n",
    "            print(\"Calculating descriptive statistics...\")\n",
    "            stats = self.calculate_descriptive_stats(df)\n",
    "            \n",
    "            # Save descriptive statistics to JSON\n",
    "            stats_path = os.path.join(panel_output_dir, 'descriptive_stats.json')\n",
    "            with open(stats_path, 'w') as f:\n",
    "                json.dump(stats, f, indent=4, default=str)\n",
    "            print(f\"Saved descriptive statistics to {stats_path}\")\n",
    "            \n",
    "            # Create and save descriptive statistics table\n",
    "            table_path = os.path.join(panel_output_dir, 'descriptive_stats_table.pdf')\n",
    "            print(f\"Attempting to create PDF table at {table_path}\")\n",
    "            self.create_descriptive_stats_table(stats, table_path, panel_name)\n",
    "            \n",
    "            # Get Claude's interpretation\n",
    "            print(\"Getting Claude's interpretation...\")\n",
    "            interpretation = self.get_claude_interpretation(stats, panel_name)\n",
    "            interpretation = self.clean_markdown_formatting(interpretation)\n",
    "            \n",
    "            # Save Claude's interpretation\n",
    "            interpretation_path = os.path.join(panel_output_dir, 'descriptive_stats_analysis.txt')\n",
    "            with open(interpretation_path, 'w') as f:\n",
    "                f.write(interpretation)\n",
    "            print(f\"Saved Claude's analysis to {interpretation_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {panel_dir}: {str(e)}\")\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "\n",
    "def analyze_all_panels(base_dir: str, output_dir: str, api_key: str):\n",
    "    \"\"\"Analyze all panel datasets in subfolders\"\"\"\n",
    "    analyzer = DescriptiveStatsAnalyzer(api_key)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Find all subfolders that start with 'panel_'\n",
    "    panel_dirs = [d for d in glob.glob(os.path.join(base_dir, 'panel_*_*')) if os.path.isdir(d)]\n",
    "    print(f\"Found {len(panel_dirs)} panel directories to analyze\")\n",
    "\n",
    "    for i, panel_dir in enumerate(panel_dirs, 1):\n",
    "        print(f\"\\nProcessing panel {i} of {len(panel_dirs)}: {panel_dir}\")\n",
    "        analyzer.analyze_panel(panel_dir, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"\n",
    "    \n",
    "    # Updated paths for Windows using raw strings to handle backslashes\n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    OUTPUT_DIR = r\"enter folder path here\"\"\n",
    "    \n",
    "    # Run analysis on all panels\n",
    "    analyze_all_panels(BASE_DIR, OUTPUT_DIR, API_KEY)\n",
    "    \n",
    "# 10. Ask Claude to write introduction\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "import glob\n",
    "import re\n",
    "\n",
    "class ComprehensiveAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    def get_laws_with_regression_results(self, csv_file: str, regression_dir: str) -> list:\n",
    "        \"\"\"Read laws from CSV and filter to only those with regression analysis results\"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        # Get all panel directories in regression_analyses folder\n",
    "        panel_dirs = glob.glob(os.path.join(regression_dir, 'panel_*'))\n",
    "        existing_panels = set()\n",
    "\n",
    "        for panel_dir in panel_dirs:\n",
    "            # Extract the panel identifier from directory name\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            existing_panels.add(panel_name)\n",
    "\n",
    "        print(f\"Found {len(existing_panels)} regression analysis folders\")\n",
    "\n",
    "        # Extract unique law names from existing panel folders\n",
    "        law_names_in_panels = set()\n",
    "        for panel_name in existing_panels:\n",
    "            # Remove 'panel_' prefix and mechanism suffix\n",
    "            parts = panel_name.replace('panel_', '').split('_')\n",
    "            # Take all parts except the last 2 (which are mechanism)\n",
    "            law_part = '_'.join(parts[:-2])\n",
    "            law_names_in_panels.add(law_part)\n",
    "\n",
    "        print(f\"Unique law names found: {len(law_names_in_panels)}\")\n",
    "\n",
    "        # Create comprehensive mapping from CSV names to panel names\n",
    "        csv_to_panel_mapping = {\n",
    "            'AssetBacked_Securities_Reform': 'Asset_Backed_Securities_Reform',\n",
    "            'Interactive_Data_for_Financial_Reporting': 'Interactive_Datafor_Financial_Reporting', \n",
    "            'Internet_Availability_of_Proxy_Materials': 'Internet_Availabilityof_Proxy_Materials',\n",
    "            'Jumpstart_Our_Business_Startups_JOBS_Act': 'Jumpstart_Our_Business_Startups_JOBSAct',\n",
    "            'Political_Contributions_by_Investment_Advisers': 'Political_Contributionsby_Investment_Advisers',\n",
    "            'Proxy_Voting_by_Investment_Advisers': 'Proxy_Votingby_Investment_Advisers',\n",
    "            'Regulation_AB_AssetBacked_Securities': 'Regulation_ABAsset_Backed_Securities',\n",
    "            'Regulation_BTR_Blackout_Trading_Restriction': 'Regulation_BTRBlackout_Trading_Restriction',\n",
    "            'Regulation_R_Bank_Securities_Activities': 'Regulation_RBank_Securities_Activities',\n",
    "            'Regulation_SBSR_SecurityBased_Swap_Reporting': 'Regulation_SBSRSecurity_Based_Swap_Reporting',\n",
    "            'Regulation_SFPS_Securities_Financing_Transaction_Reporting': 'Regulation_SFPSSecurities_Financing_Transaction_Reporting',\n",
    "            'Standards_for_Publicly_Traded_Companies_Audit_Committees': 'Standardsfor_Publicly_Traded_Companies_Audit_Committees'\n",
    "        }\n",
    "\n",
    "        laws = []\n",
    "\n",
    "        # Define economic mechanisms\n",
    "        mechanisms = [\n",
    "            'Litigation Risk', \n",
    "            'Corporate Governance', \n",
    "            'Proprietary Costs', \n",
    "            'Information Asymmetry',\n",
    "            'Unsophisticated Investors', \n",
    "            'Equity Issuance', \n",
    "            'Reputation Risk'\n",
    "        ]\n",
    "\n",
    "        laws_found = 0\n",
    "        laws_with_regression = 0\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            laws_found += 1\n",
    "        \n",
    "            # Create expected panel name based on the law\n",
    "            clean_title = row['Regulation Title'].replace(' ', '_').replace('/', '').replace('-', '').replace('(', '').replace(')', '')\n",
    "            \n",
    "            # Check if we have a direct mapping, otherwise use the cleaned title\n",
    "            panel_name_to_check = csv_to_panel_mapping.get(clean_title, clean_title)\n",
    "            \n",
    "            # If still no match, try fuzzy matching\n",
    "            if panel_name_to_check not in law_names_in_panels:\n",
    "                # Get words from the cleaned title\n",
    "                clean_words = set(clean_title.lower().split('_'))\n",
    "                best_match = None\n",
    "                best_score = 0\n",
    "                \n",
    "                for panel_name in law_names_in_panels:\n",
    "                    panel_words = set(panel_name.lower().split('_'))\n",
    "                    # Calculate overlap score\n",
    "                    overlap = len(clean_words.intersection(panel_words))\n",
    "                    total_words = len(clean_words.union(panel_words))\n",
    "                    score = overlap / total_words if total_words > 0 else 0\n",
    "                    \n",
    "                    # If >70% of words match, consider it a match\n",
    "                    if score > 0.7 and score > best_score:\n",
    "                        best_score = score\n",
    "                        best_match = panel_name\n",
    "                \n",
    "                if best_match:\n",
    "                    panel_name_to_check = best_match\n",
    "                    print(f\"DEBUG: Fuzzy match '{clean_title}' -> '{best_match}' (score: {best_score:.2f})\")\n",
    "            \n",
    "            # Check if this law has a corresponding regression analysis folder\n",
    "            if panel_name_to_check in law_names_in_panels:\n",
    "                laws_with_regression += 1\n",
    "            \n",
    "                # Get active mechanisms (where value is 'Yes')\n",
    "                active_mechanisms = [mech for mech in mechanisms if row[mech] == 'Yes']\n",
    "            \n",
    "                law = {\n",
    "                    'title': row['Regulation Title'],\n",
    "                    'year': row['Year'],\n",
    "                    'body': row['Regulatory Body'],\n",
    "                    'description': row['Description'],\n",
    "                    'impact': row['Impact'],\n",
    "                    'mechanisms': active_mechanisms,\n",
    "                    'panel_name': panel_name_to_check \n",
    "                }\n",
    "                laws.append(law)\n",
    "                print(f\"✓ Including law: {row['Regulation Title']} (Panel: {panel_name_to_check})\")\n",
    "            else:\n",
    "                print(f\"✗ Skipping law: {row['Regulation Title']} (CSV: {clean_title}, Looking for: {panel_name_to_check})\")\n",
    "\n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"Total laws in CSV: {laws_found}\")\n",
    "        print(f\"Laws with regression results: {laws_with_regression}\")\n",
    "        print(f\"Laws to generate introductions for: {len(laws)}\")\n",
    "\n",
    "        return laws\n",
    "\n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        # Remove headers (##, ###, etc.)\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove bold/italic formatting - this handles **text**, ***text***, ****text****\n",
    "        content = re.sub(r'\\*{1,4}([^*]*?)\\*{1,4}', r'\\1', content)\n",
    "        \n",
    "        # Clean up any remaining asterisks\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        \n",
    "        # Clean up any extra whitespace that might be left\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def read_regression_results(self, base_dir: str) -> dict:\n",
    "        \"\"\"Read regression results from all panel subfolders\"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        # Look for panel directories that include mechanism\n",
    "        panel_dirs = [d for d in glob.glob(os.path.join(base_dir, 'panel_*_*')) if os.path.isdir(d)]\n",
    "        \n",
    "        for panel_dir in panel_dirs:\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            results_file = os.path.join(panel_dir, 'regression_results.json')\n",
    "            \n",
    "            if os.path.exists(results_file):\n",
    "                print(f\"Reading results from {panel_name}\")\n",
    "                with open(results_file, 'r') as f:\n",
    "                    results = json.load(f)\n",
    "                all_results[panel_name] = results\n",
    "            else:\n",
    "                print(f\"No results file found in {panel_name}\")\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "    def format_regression_results(self, results: dict) -> str:\n",
    "        \"\"\"Format regression results for a specific panel\"\"\"\n",
    "        if not results:\n",
    "            return \"No regression results available.\"\n",
    "            \n",
    "        formatted_text = \"\\nRegression Results:\\n\\n\"\n",
    "        \n",
    "        for spec_name, spec_results in results.items():\n",
    "            formatted_text += f\"\\nSpecification {spec_name}:\\n\"\n",
    "            try:\n",
    "                formatted_text += f\"Treatment Effect: {spec_results['coefficients']['treatment_effect']:.4f}\\n\"\n",
    "                formatted_text += f\"T-statistic: {abs(spec_results['t_stats']['treatment_effect']):.2f}\\n\"\n",
    "                formatted_text += f\"P-value: {spec_results['pvalues']['treatment_effect']:.4f}\\n\"\n",
    "                formatted_text += f\"R-squared: {spec_results['r_squared']:.4f}\\n\"\n",
    "                \n",
    "                if spec_results['controls']:\n",
    "                    formatted_text += \"\\nControl Variables:\\n\"\n",
    "                    for control in spec_results['controls']:\n",
    "                        coef = spec_results['coefficients'][control]\n",
    "                        tstat = spec_results['t_stats'][control]\n",
    "                        pvalue = spec_results['pvalues'][control]\n",
    "                        formatted_text += f\"{control}: coef={coef:.4f}, t={tstat:.2f}, p={pvalue:.4f}\\n\"\n",
    "                \n",
    "                formatted_text += \"\\n\" + \"-\"*50 + \"\\n\"\n",
    "            except KeyError as e:\n",
    "                print(f\"Missing key in regression results: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return formatted_text\n",
    "\n",
    "    def get_comprehensive_introduction(self, law: dict, mechanism: str, regression_results: dict) -> str:\n",
    "        \"\"\"Get comprehensive introduction for a law and specific mechanism\"\"\"\n",
    "        regression_text = self.format_regression_results(regression_results)\n",
    "        print(f\"\\nFormatted regression results for {law['title']} - {mechanism}:\")\n",
    "        print(regression_text)\n",
    "        \n",
    "        prompt = f\"\"\"As an accounting academic, please write a comprehensive introduction section examining {law['title']} \n",
    "        and its impact on voluntary disclosure in the U.S. through the {mechanism} channel.\n",
    "\n",
    "Law Details:\n",
    "Title: {law['title']} ({law['year']})\n",
    "Regulatory Body: {law['body']}\n",
    "Description: {law['description']}\n",
    "Impact: {law['impact']}\n",
    "Economic Mechanism: {mechanism}\n",
    "\n",
    "Empirical Results:\n",
    "{regression_text}\n",
    "\n",
    "Please structure the introduction as follows:\n",
    "\n",
    "1. Motivation (2 paragraphs, ~200 words):\n",
    "   - Begin with the importance of {law['title']}\n",
    "   - Open with a broad statement about {law['title']}\n",
    "   - Focus specifically on how it relates to {mechanism}\n",
    "   - Explain its relevance to voluntary disclosure in the U.S. through this mechanism\n",
    "   - Identify the specific gap or puzzle in the literature\n",
    "   - Identify specific research questions\n",
    "\n",
    "2. Hypothesis Development (3 paragraphs, ~300 words):\n",
    "   - Present the economic mechanism linking the regulation to voluntary disclosure in the U.S. \n",
    "   - Explain how {mechanism} affects voluntary disclosure\n",
    "   - Discuss theoretical underpinnings\n",
    "   - Build on established theoretical frameworks\n",
    "   - Develop clear, testable predictions\n",
    "   - Build logical arguments step by step\n",
    "   - Support each claim with citations to foundational papers\n",
    "   - Support arguments with citations\n",
    "\n",
    "3. Results Summary (3 paragraphs, ~300 words):\n",
    "   - Lead with strongest statistical findings\n",
    "   - Present the treatment effect coefficient of {regression_text}\n",
    "   - Summarize the key findings of the analysis, \n",
    "     discussing the significance of the variable in terms of predictive power: {regression_text}\n",
    "   - Discuss significance of variables and their predictive power\n",
    "   - Present results in order of importance\n",
    "   - Include economic significance\n",
    "   - Use precise statistical language\n",
    "   - Connect findings back to the {mechanism} channel\n",
    "\n",
    "4. Contribution (2 paragraphs, ~200 words):\n",
    "   - Position relative to 3-4 most closely related papers\n",
    "   - Highlight novel findings about {mechanism}\n",
    "   - Discuss broader implications for theory and practice\n",
    "   - Emphasize contributions to understanding this specific economic channel\n",
    "\n",
    "Guidelines:\n",
    "- Do not include headers in the write up\n",
    "- Do not include extra text or explanations\n",
    "    -Example of what not to include: \"Here's a comprehensive introduction section following your guidelines\" or \n",
    "    \"Here's a comprehensive introduction section examining Resource Extraction Disclosure Rules and its impact on voluntary disclosure through the Corporate Governance channel\"\n",
    "- Use active voice (e.g., \"We find\" instead of \"It is found\")\n",
    "- Maintain formal academic tone\n",
    "- Include 2-3 citations per paragraph \n",
    "- Use present tense for established findings\n",
    "- Use past tense for your specific results\n",
    "- Make clear distinctions between correlation and causation\n",
    "- Avoid speculation beyond the data\n",
    "- Cite papers from top accounting and finance journals such as:\n",
    "    The Accounting Review, Journal of Accounting Research, \n",
    "    Journal of Accounting and Economics, Contemporary Accounting Research, Accounting, Organizations, and Society,\n",
    "    and Review of Accounting Studies\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting introduction: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def analyze_and_save_introductions(self, base_dir: str, csv_file: str, output_dir: str):\n",
    "        \"\"\"Generate and save comprehensive introductions\"\"\"\n",
    "        # Create introduction directory\n",
    "        intro_dir = os.path.join(output_dir, 'introduction')\n",
    "        os.makedirs(intro_dir, exist_ok=True)\n",
    "    \n",
    "        # Get filtered laws (only those with regression results) and regression results\n",
    "        laws = self.get_laws_with_regression_results(csv_file, base_dir)\n",
    "        regression_results = self.read_regression_results(base_dir)\n",
    "        \n",
    "        total_introductions = 0\n",
    "        total_mechanisms = 0\n",
    "    \n",
    "        # Generate introduction for each law and each mechanism\n",
    "        for law in laws:\n",
    "            print(f\"\\nProcessing law: {law['title']}\")\n",
    "            print(f\"Mechanisms for this law: {law['mechanisms']}\")\n",
    "            \n",
    "            if not law['mechanisms']:\n",
    "                print(f\"WARNING: No active mechanisms found for {law['title']}\")\n",
    "                continue\n",
    "                \n",
    "            # Generate separate introduction for each mechanism\n",
    "            for mechanism in law['mechanisms']:\n",
    "                total_mechanisms += 1\n",
    "                print(f\"Writing introduction for mechanism: {mechanism}\")\n",
    "                \n",
    "                # Create panel name to match actual folder structure\n",
    "                clean_mechanism = mechanism.replace(' ', '_')\n",
    "            \n",
    "                # Use the panel name from the law object\n",
    "                clean_title = law['panel_name']\n",
    "            \n",
    "                # Look for matching panel in regression results\n",
    "                law_results = {}\n",
    "                for panel_name, results in regression_results.items():\n",
    "                    if clean_title in panel_name and clean_mechanism in panel_name:\n",
    "                        law_results = results\n",
    "                        print(f\"Found matching regression results: {panel_name}\")\n",
    "                        break\n",
    "            \n",
    "                if not law_results:\n",
    "                    print(f\"Warning: No regression results found for {clean_title}_{clean_mechanism}\")\n",
    "                \n",
    "                try:\n",
    "                    # Generate introduction\n",
    "                    intro = self.get_comprehensive_introduction(law, mechanism, law_results)\n",
    "                    intro = self.clean_markdown_formatting(intro)\n",
    "            \n",
    "                    # Create filename using panel naming convention\n",
    "                    filename = f\"{law['panel_name']}_{clean_mechanism}_introduction.txt\"\n",
    "            \n",
    "                    # Save introduction\n",
    "                    with open(os.path.join(intro_dir, filename), 'w', encoding='utf-8') as f:\n",
    "                        f.write(intro)\n",
    "            \n",
    "                    total_introductions += 1\n",
    "                    print(f\"✓ Saved introduction for {law['title']} - {mechanism}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"✗ ERROR saving introduction for {law['title']} - {mechanism}: {str(e)}\")\n",
    "    \n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        print(f\"FINAL SUMMARY:\")\n",
    "        print(f\"Laws processed: {len(laws)}\")\n",
    "        print(f\"Total mechanisms found: {total_mechanisms}\")\n",
    "        print(f\"Introductions successfully saved: {total_introductions}\")\n",
    "        print(f\"=\"*50)\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"\n",
    "    BASE_DIR = r\"enter folder path here\"\"\n",
    "    CSV_FILE = \"enter file path here\"\n",
    "    OUTPUT_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        analyzer = ComprehensiveAnalyzer(API_KEY)\n",
    "        \n",
    "        # Run the analysis with fixed naming logic\n",
    "        analyzer.analyze_and_save_introductions(BASE_DIR, CSV_FILE, OUTPUT_DIR)\n",
    "        print(\"Analysis complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "# 11. Ask Claude to write the model specification section of a paper \n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "import glob\n",
    "import re\n",
    "\n",
    "class ComprehensiveAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        \n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        # Remove headers (##, ###, etc.)\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove bold/italic formatting - this handles **text**, ***text***, ****text****\n",
    "        content = re.sub(r'\\*{1,4}([^*]*?)\\*{1,4}', r'\\1', content)\n",
    "        \n",
    "        # Clean up any remaining asterisks\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        \n",
    "        # Clean up any extra whitespace that might be left\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def read_regression_results(self, base_dir: str) -> dict:\n",
    "        \"\"\"Read regression results from all panel subfolders\"\"\"\n",
    "        all_results = {}\n",
    "    \n",
    "\n",
    "        panel_dirs = [d for d in glob.glob(os.path.join(base_dir, 'panel_*_*')) if os.path.isdir(d)]\n",
    "        \n",
    "        \n",
    "        for panel_dir in panel_dirs:\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            results_file = os.path.join(panel_dir, 'regression_results.json')\n",
    "            \n",
    "            if os.path.exists(results_file):\n",
    "                print(f\"Reading results from {panel_name}\")\n",
    "                with open(results_file, 'r') as f:\n",
    "                    results = json.load(f)\n",
    "                all_results[panel_name] = results\n",
    "            else:\n",
    "                print(f\"No results file found in {panel_name}\")\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "    def format_regression_results(self, results: dict) -> str:\n",
    "        \"\"\"Format regression results for a specific panel\"\"\"\n",
    "        formatted_text = \"\\nRegression Results:\\n\\n\"\n",
    "        \n",
    "        for spec_name, spec_results in results.items():\n",
    "            formatted_text += f\"\\nSpecification {spec_name}:\\n\"\n",
    "            formatted_text += f\"Treatment Effect: {spec_results['coefficients']['treatment_effect']:.4f}\\n\"\n",
    "            formatted_text += f\"T-statistic: {abs(spec_results['t_stats']['treatment_effect']):.2f}\\n\"\n",
    "            formatted_text += f\"P-value: {spec_results['pvalues']['treatment_effect']:.4f}\\n\"\n",
    "            formatted_text += f\"R-squared: {spec_results['r_squared']:.4f}\\n\"\n",
    "            \n",
    "            if spec_results['controls']:\n",
    "                formatted_text += \"\\nControl Variables:\\n\"\n",
    "                for control in spec_results['controls']:\n",
    "                    coef = spec_results['coefficients'][control]\n",
    "                    tstat = spec_results['t_stats'][control]\n",
    "                    pvalue = spec_results['pvalues'][control]\n",
    "                    formatted_text += f\"{control.replace('_', ' ')}: coef={coef:.4f}, t={tstat:.2f}, p={pvalue:.4f}\\n\"\n",
    "            \n",
    "            formatted_text += \"\\n\" + \"-\"*50 + \"\\n\"\n",
    "        \n",
    "        return formatted_text\n",
    "\n",
    "    def get_laws_analysis(self, csv_file: str) -> list:\n",
    "        \"\"\"Read and analyze laws from CSV file\"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "        laws = []\n",
    "        \n",
    "        # Define economic mechanisms\n",
    "        mechanisms = [\n",
    "            'Litigation Risk', \n",
    "            'Corporate Governance', \n",
    "            'Proprietary Costs', \n",
    "            'Information Asymmetry',\n",
    "            'Unsophisticated Investors', \n",
    "            'Equity Issuance', \n",
    "            'Reputation Risk'\n",
    "        ]\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Get active mechanisms (where value is 'Yes')\n",
    "            active_mechanisms = [mech for mech in mechanisms if row[mech] == 'Yes']\n",
    "            \n",
    "            law = {\n",
    "                'title': row['Regulation Title'],\n",
    "                'year': row['Year'],\n",
    "                'body': row['Regulatory Body'],\n",
    "                'description': row['Description'],\n",
    "                'impact': row['Impact'],\n",
    "                'mechanisms': active_mechanisms\n",
    "            }\n",
    "            laws.append(law)\n",
    "        \n",
    "        return laws\n",
    "\n",
    "    def get_model_specification(self, law: dict, mechanism: str, regression_results: dict) -> str:\n",
    "        \"\"\"Get model specification section for a law and specific mechanism with regression results\"\"\"\n",
    "        regression_text = self.format_regression_results(regression_results) if regression_results else \"No regression results available.\"\n",
    "        \n",
    "        # Get number of observations from regression results\n",
    "        n_obs = None\n",
    "        if regression_results and '(3)' in regression_results:\n",
    "            n_obs = regression_results['(3)'].get('n_obs', 'Not available')\n",
    "        \n",
    "        # Get list of control variables from regression results\n",
    "        controls = []\n",
    "        if regression_results:\n",
    "            for spec in regression_results.values():\n",
    "                if spec.get('controls'):\n",
    "                    controls.extend(spec['controls'])\n",
    "            controls = list(set(controls))  # Remove duplicates\n",
    "        \n",
    "        prompt = f\"\"\"You are an accounting academic writing a research paper examining {law['title']} and its impact \n",
    "        on voluntary disclosure in the U.S.  through the {mechanism} channel. \n",
    "        Please write the research design section for an academic journal in accounting.\n",
    "\n",
    "Law Details:\n",
    "Title: {law['title']} ({law['year']})\n",
    "Description: {law['description']}\n",
    "Impact: {law['impact']}\n",
    "Economic Mechanism: {mechanism}\n",
    "\n",
    "Regression Information:\n",
    "{regression_text}\n",
    "\n",
    "IMPORTANT: This study examines all firms in the Compustat universe, not just firms directly subject to {law['title']}. \n",
    "This is a pre/post research design.\n",
    "\n",
    "Please follow these detailed guidelines:\n",
    "\n",
    "1. Sample selection and post-law indicator:\n",
    "    - Explain that the sample includes all firms in the Compustat universe, in the U.S., during the sample period \n",
    "    - Describe the regulatory authority that is responsible for the law {law['body']} \n",
    "    - Clarify that while {law['title']} may directly target specific firms/industries, \n",
    "      the analysis examines all firms in the Compustat universe\n",
    "    - Explain that the treatment variable affects all firms\n",
    "    \n",
    "2. Model Explanation (2-3 paragraphs, ~300 words total):\n",
    "    - Explain the regression model used to examine the relationship between {law['title']} \n",
    "      and voluntary disclosure in the U.S. through the {mechanism} channel\n",
    "          -The model is: FreqMF = β₀ + β₁Treatment Effect + γControls + ε\n",
    "    - Only discuss the control variables that appear in the regression results {regression_text}\n",
    "      These variables are based on prior literature and are: Institutional Ownership, Firm Size, Book-to-Market,\n",
    "      ROA, Stock Return, Earnings volatility, Loss, Class action litigation risk\n",
    "    - Support model choices with citations to foundational papers\n",
    "    - Explain potential endogeneity concerns and how the research design addresses them\n",
    "    - Use clear, academic language\n",
    "    - Avoid using underscores in variable names\n",
    "\n",
    "3. Mathematical Model:\n",
    "    - Present the complete regression equation in proper mathematical notation {regression_text}\n",
    "        - Label the equation as follows: FreqMF = β₀ + β₁Treatment Effect + γControls + ε\n",
    "            - Label the dependent variable \"FreqMF\"\n",
    "            - Label the variable of interest as \"Treatment Effect\"\n",
    "            - Label the control variables in the regression equation as \"Controls\"\n",
    "    - Do no include the subscripts i and t in the regression \n",
    "    - Format the equation professionally\n",
    "\n",
    "4. Variable Definitions (2-3 paragraphs, ~300 words total):\n",
    "    - Define the dependent variable (FreqMF - management forecast frequency)\n",
    "    - Define the \"Treatment Effect\" variable as an indicator variable for the post-{law['title']} period (affecting all firms)\n",
    "    - Define each control variable used in the model as they appear in {regression_text}\n",
    "      These variables are based on prior literature and are: Institutional Ownership, Firm Size, Book-to-Market,\n",
    "      ROA, Stock Return, Earnings volatility, Loss, Class action litigation risk\n",
    "        -Cite the appropriate paper for these variables from the Journal of Accounting Research\n",
    "    - Do no include the subscripts i and t in the variable definition\n",
    "    - For each control variable, provide detailed explanations about their expected relationships with voluntary disclosure\n",
    "    - Explain how variables relate to the {mechanism} channel\n",
    "    \n",
    "\n",
    "5. Sample Construction (2-3 paragraphs, ~300 words total):\n",
    "    - Describe the event window around {law['year']}\n",
    "        -Always clarify that the post-regulation period includes the regulation year by writing \"from {law['year']} onwards\"\n",
    "        -The time window for this analysis is 2 years before and 2 years after the regulation is implemented. Therefore,\n",
    "         The total number of years of the sample period is 5 years.\n",
    "    - Describe the source of the data from Compustat, I/B/E/S, Audit Analytics, and CRSP\n",
    "    - Describe the sample construction process based on the number of observations: {n_obs if n_obs else 'Not available'}\n",
    "    - Note any sample restrictions\n",
    "\n",
    "Writing Guidelines:\n",
    "-Recall that the data is for U.S. firms\n",
    "- Provide only the write up, no extra text or explanations \n",
    "    -Example of what not to include: \"Here's a comprehensive model specifaction section following your guidelines\"\n",
    "- Use active voice (e.g., \"We find\" instead of \"It is found\")\n",
    "- Maintain formal academic tone\n",
    "- Include 2-3 citations per paragraph\n",
    "- Cite papers from top accounting and finance journals such as:\n",
    "    The Accounting Review, Journal of Accounting Research, \n",
    "    Journal of Accounting and Economics, Contemporary Accounting Research, Accounting, Organizations, and Society,\n",
    "    and Review of Accounting Studies\n",
    "- Use precise statistical language\n",
    "- Make clear connections between variables and theoretical predictions\n",
    "- Do not include Latex format\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting model specification: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def create_model_specifications(self, base_dir: str, csv_file: str, output_dir: str):\n",
    "        \"\"\"Generate and save model specification sections for all laws\"\"\"\n",
    "        # Create main directory\n",
    "        main_dir = os.path.join(output_dir, 'model_specification')\n",
    "        os.makedirs(main_dir, exist_ok=True)\n",
    "        \n",
    "        # Get regression results for existing panels\n",
    "        regression_results = self.read_regression_results(base_dir)\n",
    "        total_panels = len(regression_results)\n",
    "        processed = 0\n",
    "        \n",
    "        print(f\"\\nFound {total_panels} panels in regression folder\")\n",
    "\n",
    "        \n",
    "        # Process each panel that exists\n",
    "        for panel_name, results in regression_results.items():\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Processing panel {processed + 1} of {total_panels}: {panel_name}\")\n",
    "        \n",
    "            try:\n",
    "                # Parse panel name to get law and mechanism\n",
    "                parts = panel_name.replace('panel_', '').split('_')\n",
    "                mechanism = parts[-1]  # Last part is the mechanism\n",
    "                law_name = '_'.join(parts[:-1])  # Everything else is the law name\n",
    "            \n",
    "                # Read the original panel file to get law details\n",
    "                panel_file = os.path.join(base_dir, panel_name, \"filtered_data_with_trends.csv\")\n",
    "                df = pd.read_csv(panel_file)\n",
    "            \n",
    "                law = {\n",
    "                    'title': df['Regulation Title'].iloc[0],\n",
    "                    'year': df['Year'].iloc[0],\n",
    "                    'body': df['Regulatory Body'].iloc[0] if 'Regulatory Body' in df.columns else 'Unknown',\n",
    "                    'description': df['Description'].iloc[0] if 'Description' in df.columns else 'Not available',\n",
    "                    'impact': df['Impact'].iloc[0] if 'Impact' in df.columns else 'Not available',\n",
    "                    'mechanisms': [mechanism]\n",
    "                }\n",
    "            \n",
    "                # Generate model specification\n",
    "                content = self.get_model_specification(law, mechanism, results)\n",
    "                \n",
    "                # Clean markdown formatting\n",
    "                clean_content = self.clean_markdown_formatting(content)\n",
    "            \n",
    "                # Create filename\n",
    "                filename = f\"{panel_name}_model_specification.txt\"\n",
    "                file_path = os.path.join(main_dir, filename)\n",
    "            \n",
    "                # Save model specification\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(clean_content)\n",
    "                print(f\"Saved model specification for {panel_name}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing panel {panel_name}: {str(e)}\")\n",
    "        \n",
    "            processed += 1\n",
    "            print(f\"\\nProgress: {processed}/{total_panels} ({(processed/total_panels)*100:.1f}%)\")\n",
    "    \n",
    "        print(\"\\nModel specification generation complete!\")\n",
    "    \n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"\n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    CSV_FILE = \"enter file path here\"\n",
    "    OUTPUT_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        analyzer = ComprehensiveAnalyzer(API_KEY)\n",
    "        analyzer.create_model_specifications(BASE_DIR, CSV_FILE, OUTPUT_DIR)\n",
    "        print(\"\\nModel specification sections complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# 12. Ask Claude to write a conclusion \n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "import glob\n",
    "import re\n",
    "\n",
    "class ComprehensiveAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "\n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        # Remove headers (##, ###, etc.)\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove bold/italic formatting - this handles **text**, ***text***, ****text****\n",
    "        content = re.sub(r'\\*{1,4}([^*]*?)\\*{1,4}', r'\\1', content)\n",
    "        \n",
    "        # Clean up any remaining asterisks\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        \n",
    "        # Clean up any extra whitespace that might be left\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def read_regression_results(self, base_dir: str) -> dict:\n",
    "        \"\"\"Read regression results from all panel subfolders\"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        panel_dirs = [d for d in glob.glob(os.path.join(base_dir, 'panel_*_*')) if os.path.isdir(d)]\n",
    "        \n",
    "        for panel_dir in panel_dirs:\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            results_file = os.path.join(panel_dir, 'regression_results.json')\n",
    "            \n",
    "            if os.path.exists(results_file):\n",
    "                print(f\"Reading results from {panel_name}\")\n",
    "                with open(results_file, 'r') as f:\n",
    "                    results = json.load(f)\n",
    "                all_results[panel_name] = results\n",
    "            else:\n",
    "                print(f\"No results file found in {panel_name}\")\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "    def format_regression_results(self, results: dict) -> str:\n",
    "        \"\"\"Format regression results for a specific panel\"\"\"\n",
    "        formatted_text = \"\\nRegression Results:\\n\\n\"\n",
    "        \n",
    "        for spec_name, spec_results in results.items():\n",
    "            formatted_text += f\"\\nSpecification {spec_name}:\\n\"\n",
    "            formatted_text += f\"Treatment Effect: {spec_results['coefficients']['treatment_effect']:.4f}\\n\"\n",
    "            formatted_text += f\"T-statistic: {abs(spec_results['t_stats']['treatment_effect']):.2f}\\n\"\n",
    "            formatted_text += f\"P-value: {spec_results['pvalues']['treatment_effect']:.4f}\\n\"\n",
    "            formatted_text += f\"R-squared: {spec_results['r_squared']:.4f}\\n\"\n",
    "            \n",
    "            if spec_results['controls']:\n",
    "                formatted_text += \"\\nControl Variables:\\n\"\n",
    "                for control in spec_results['controls']:\n",
    "                    coef = spec_results['coefficients'][control]\n",
    "                    tstat = spec_results['t_stats'][control]\n",
    "                    pvalue = spec_results['pvalues'][control]\n",
    "                    formatted_text += f\"{control.replace('_', ' ')}: coef={coef:.4f}, t={tstat:.2f}, p={pvalue:.4f}\\n\"\n",
    "            \n",
    "            formatted_text += \"\\n\" + \"-\"*50 + \"\\n\"\n",
    "        \n",
    "        return formatted_text\n",
    "\n",
    "    def get_laws_analysis(self, csv_file: str) -> list:\n",
    "        \"\"\"Read and analyze laws from CSV file\"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "        laws = []\n",
    "        \n",
    "        # Define economic mechanisms\n",
    "        mechanisms = [\n",
    "            'Litigation Risk', \n",
    "            'Corporate Governance', \n",
    "            'Proprietary Costs', \n",
    "            'Information Asymmetry',\n",
    "            'Unsophisticated Investors', \n",
    "            'Equity Issuance', \n",
    "            'Reputation Risk'\n",
    "        ]\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Get active mechanisms (where value is 'Yes')\n",
    "            active_mechanisms = [mech for mech in mechanisms if row[mech] == 'Yes']\n",
    "            \n",
    "            law = {\n",
    "                'title': row['Regulation Title'],\n",
    "                'year': row['Year'],\n",
    "                'body': row['Regulatory Body'],\n",
    "                'description': row['Description'],\n",
    "                'impact': row['Impact'],\n",
    "                'mechanisms': active_mechanisms\n",
    "            }\n",
    "            laws.append(law)\n",
    "        \n",
    "        return laws\n",
    "\n",
    "    def get_conclusion(self, law: dict, mechanism: str, regression_results: dict) -> str:\n",
    "        \"\"\"Get conclusion section for a law and specific mechanism with regression results\"\"\"\n",
    "        regression_text = self.format_regression_results(regression_results) if regression_results else \"No regression results available.\"\n",
    "        \n",
    "        prompt = f\"\"\"You are an accounting academic writing a research paper examining {law['title']} and its \n",
    "        impact on voluntary disclosure in the U.S. through the {mechanism} channel. \n",
    "        Please write a conclusion section for an academic journal in accounting.\n",
    "\n",
    "Law Details:\n",
    "Title: {law['title']} ({law['year']})\n",
    "Description: {law['description']}\n",
    "Impact: {law['impact']}\n",
    "Economic Mechanism: {mechanism}\n",
    "\n",
    "Empirical Results:\n",
    "{regression_text}\n",
    "\n",
    "Please write a comprehensive conclusion following these guidelines:\n",
    "\n",
    "1. Summary of Main Findings (2-3 paragraphs):\n",
    "    - Restate the research question, focusing on the {mechanism} channel\n",
    "    - Summarize key empirical findings\n",
    "    - Discuss statistical and economic significance\n",
    "    - Interpret the results in the context of {law['title']} and {mechanism}\n",
    "\n",
    "2. Implications (1-2 paragraphs):\n",
    "    - Discuss implications for regulators\n",
    "    - Discuss implications for managers\n",
    "    - Discuss implications for investors\n",
    "    - Connect findings to broader literature on {mechanism}\n",
    "\n",
    "3. Limitations and Future Research (1-2 paragraphs):\n",
    "    - Acknowledge key limitations\n",
    "    - Suggest promising avenues for future research\n",
    "    - Discuss potential extensions, particularly related to {mechanism}\n",
    "    \n",
    "Writing Guidelines:\n",
    "- Use active voice (e.g., \"We find\" instead of \"It is found\")\n",
    "- Maintain formal academic tone\n",
    "- Use past tense for your specific results\n",
    "- Use present tense for implications\n",
    "- Make clear distinctions between correlation and causation\n",
    "- Focus on the practical significance of the findings\n",
    "- Cite papers from top accounting and finance journals such as:\n",
    "    The Accounting Review, Journal of Accounting Research, \n",
    "    Journal of Accounting and Economics, Contemporary Accounting Research, Accounting, Organizations, and Society,\n",
    "    and Review of Accounting Studies\n",
    "- Do not include section headers\n",
    "- Do not include journal names after the citation. For example, for these citations: (Christensen et al., 2013,\n",
    "  Journal of Accounting and Economics; Shroff et al., 2013, The Accounting Review). \n",
    "  The Journal of Accounting and Economics and The Accounting Review names\n",
    "  should not be included.\n",
    "- Length: approximately 750 words\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting conclusion: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def create_conclusions(self, base_dir: str, csv_file: str, output_dir: str):\n",
    "        \"\"\"Generate and save conclusion sections for all laws\"\"\"\n",
    "        # Create main directory\n",
    "        main_dir = os.path.join(output_dir, 'conclusion')\n",
    "        os.makedirs(main_dir, exist_ok=True)\n",
    "    \n",
    "        # Get regression results for existing panels\n",
    "        regression_results = self.read_regression_results(base_dir)\n",
    "        total_panels = len(regression_results)\n",
    "        processed = 0\n",
    "    \n",
    "        print(f\"\\nFound {total_panels} panels in regression folder\")\n",
    "    \n",
    "        # Process each panel that exists\n",
    "        for panel_name, results in regression_results.items():\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Processing panel {processed + 1} of {total_panels}: {panel_name}\")\n",
    "        \n",
    "            try:\n",
    "                # Parse panel name to get law and mechanism\n",
    "                parts = panel_name.replace('panel_', '').split('_')\n",
    "                mechanism = parts[-1]  # Last part is the mechanism\n",
    "                law_name = '_'.join(parts[:-1])  # Everything else is the law name\n",
    "                \n",
    "                # Read the original panel file to get law details\n",
    "                panel_file = os.path.join(base_dir, panel_name, \"filtered_data_with_trends.csv\")\n",
    "                if not os.path.exists(panel_file):\n",
    "                    print(f\"No filtered data file found for {panel_name}\")\n",
    "                    continue\n",
    "                \n",
    "                df = pd.read_csv(panel_file)\n",
    "            \n",
    "                law = {\n",
    "                    'title': df['Regulation Title'].iloc[0] if 'Regulation Title' in df.columns else 'Unknown',\n",
    "                    'year': df['Year'].iloc[0] if 'Year' in df.columns else 'Unknown',\n",
    "                    'body': df['Regulatory Body'].iloc[0] if 'Regulatory Body' in df.columns else 'Unknown',\n",
    "                    'description': df['Description'].iloc[0] if 'Description' in df.columns else 'Not available',\n",
    "                    'impact': df['Impact'].iloc[0] if 'Impact' in df.columns else 'Not available',\n",
    "                    'mechanisms': [mechanism]\n",
    "                }\n",
    "            \n",
    "                # Generate conclusion for that specific law-mechanism combination\n",
    "                content = self.get_conclusion(law, mechanism, results)\n",
    "            \n",
    "                # Clean markdown formatting\n",
    "                clean_conclusion = self.clean_markdown_formatting(content)\n",
    "            \n",
    "                # Create filename\n",
    "                filename = f\"{panel_name}_conclusion.txt\"\n",
    "                file_path = os.path.join(main_dir, filename)\n",
    "            \n",
    "                # Check if file already exists\n",
    "                if os.path.exists(file_path):\n",
    "                    print(f\"Skipping {panel_name}: File already exists\")\n",
    "                    continue\n",
    "            \n",
    "                # Save conclusion\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(clean_conclusion)\n",
    "                print(f\"Saved conclusion for {panel_name}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing panel {panel_name}: {str(e)}\")\n",
    "        \n",
    "            processed += 1\n",
    "            print(f\"\\nProgress: {processed}/{total_panels} ({(processed/total_panels)*100:.1f}%)\")\n",
    "    \n",
    "        print(\"\\nConclusion generation complete!\")\n",
    "    \n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"\n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    CSV_FILE = \"enter file path here\"\n",
    "    OUTPUT_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        analyzer = ComprehensiveAnalyzer(API_KEY)\n",
    "        analyzer.create_conclusions(BASE_DIR, CSV_FILE, OUTPUT_DIR)\n",
    "        print(\"\\nConclusion sections complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "# 13. Ask Claude to write an abstract\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from anthropic import Anthropic\n",
    "import re\n",
    "\n",
    "class AbstractGenerator:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize abstract generator with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        \n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        # Remove headers (##, ###, etc.)\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove bold/italic formatting - this handles **text**, ***text***, ****text****\n",
    "        content = re.sub(r'\\*{1,4}([^*]*?)\\*{1,4}', r'\\1', content)\n",
    "        \n",
    "        # Clean up any remaining asterisks\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        \n",
    "        # Clean up any extra whitespace that might be left\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def read_laws_data(self, csv_file: str) -> pd.DataFrame:\n",
    "        \"\"\"Read the laws data CSV file\"\"\"\n",
    "        return pd.read_csv(csv_file)\n",
    "    \n",
    "    def generate_abstract(self, introduction_content: str) -> str:\n",
    "        \"\"\"Generate an abstract based on an existing introduction\"\"\"\n",
    "        prompt = f\"\"\"As an accounting academic, please convert the following introduction into a concise academic abstract.\n",
    "\n",
    "Guidelines:\n",
    "- Maintain the key points from the introduction\n",
    "- Condense the content to 150-250 words\n",
    "- Include background, research objective, methodology, key findings, and contribution\n",
    "- Use a formal academic tone\n",
    "- Avoid adding new information not present in the original text\n",
    "- Use present tense for established findings\n",
    "- Use past tense for specific results\n",
    "- Do not include citations in the abstract\n",
    "- Do not use the label \"Abstract\"\n",
    "- Write in one paragraph\n",
    "\n",
    "Introduction to Convert:\n",
    "{introduction_content}\n",
    "\n",
    "Please provide a structured abstract that captures the essence of the original introduction.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=4000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating abstract: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "    \n",
    "    def process_introductions(self, input_dir: str, output_dir: str):\n",
    "        \"\"\"Process introduction files and generate corresponding abstracts\"\"\"\n",
    "        # Create abstracts directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "        # Find all introduction files\n",
    "        introduction_files = [f for f in os.listdir(input_dir) if f.endswith('_introduction.txt')]\n",
    "    \n",
    "        # Process each introduction file\n",
    "        for intro_file in introduction_files:\n",
    "            try:\n",
    "                # Create abstract filename (replace 'introduction' with 'abstract')\n",
    "                abstract_filename = intro_file.replace('_introduction.txt', '_abstract.txt')\n",
    "                abstract_path = os.path.join(output_dir, abstract_filename)\n",
    "            \n",
    "                # CHECK FOR EXISTING FILES\n",
    "                if os.path.exists(abstract_path):\n",
    "                    print(f\"Skipping {intro_file}: Abstract already exists\")\n",
    "                    continue\n",
    "            \n",
    "                # Read introduction content\n",
    "                with open(os.path.join(input_dir, intro_file), 'r', encoding='utf-8') as f:\n",
    "                    introduction_content = f.read()\n",
    "            \n",
    "                # Generate abstract\n",
    "                abstract = self.generate_abstract(introduction_content)\n",
    "            \n",
    "                # Clean markdown formatting\n",
    "                clean_abstract = self.clean_markdown_formatting(abstract)\n",
    "            \n",
    "                # Save abstract\n",
    "                with open(abstract_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(clean_abstract)\n",
    "            \n",
    "                # PROGRESS FEEDBACK\n",
    "                print(f\"Generated abstract for {intro_file}\")\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {intro_file}: {str(e)}\")\n",
    "            \n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"  # Replace with your actual API key\n",
    "    \n",
    "    # Directories\n",
    "    INPUT_DIR = r\"enter folder path here\"\n",
    "    OUTPUT_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize and run abstract generator\n",
    "        generator = AbstractGenerator(API_KEY)\n",
    "        generator.process_introductions(INPUT_DIR, OUTPUT_DIR)\n",
    "        print(\"Abstract generation complete!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "# 14. Combine AI-generated content from Txt. files\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def find_background_file(back_hypo_dir: str, law_name: str, clean_mechanism: str) -> str:\n",
    "    \"\"\"Find the background file handling different naming patterns including EU regulations with acronyms\"\"\"\n",
    "    \n",
    "    # Try standard pattern first\n",
    "    standard_file = os.path.join(back_hypo_dir, f\"{law_name}_{clean_mechanism}_background_hypothesis.txt\")\n",
    "    if os.path.exists(standard_file):\n",
    "        return standard_file\n",
    "    \n",
    "    # Enhanced fuzzy matching for EU regulations and other complex names\n",
    "    if os.path.exists(back_hypo_dir):\n",
    "        all_bg_files = [f for f in os.listdir(back_hypo_dir) if f.endswith('_background_hypothesis.txt')]\n",
    "        \n",
    "        # Debug: Show relevant files for specific problematic cases\n",
    "        if law_name in ['Alternative_Investment_Fund_Managers_Directive_AIFMD_European_Union', \n",
    "                        'National_Instrument_31103_Registration_Requirements_Canada']:\n",
    "            print(f\"\\n=== DEBUG for {law_name} ===\")\n",
    "            print(f\"Looking for mechanism: {clean_mechanism}\")\n",
    "            relevant_files = [f for f in all_bg_files if any(word in f.lower() for word in \n",
    "                             ['aifmd', 'alternative', 'national', '31103', 'instrument'])]\n",
    "            print(f\"Relevant files found ({len(relevant_files)}):\")\n",
    "            for f in relevant_files[:10]:  # Show first 10 relevant files\n",
    "                print(f\"  - {f}\")\n",
    "            print(\"=== END DEBUG ===\\n\")\n",
    "    \n",
    "        target_suffix = f\"_{clean_mechanism}_background_hypothesis.txt\"\n",
    "        \n",
    "        # Create matching variants of the law name based on actual file patterns\n",
    "        def create_matching_variants(name):\n",
    "            \"\"\"Create various matching versions of the regulation name\"\"\"\n",
    "            variants = set([name])\n",
    "            \n",
    "            # Handle European Union regulations with different abbreviations\n",
    "            eu_variations = {\n",
    "                'European_Union': ['European_Union', 'European', 'Europe', 'Eu', 'Eur', 'European_Uni', 'Europea', ''],\n",
    "                'Canada': ['Canada', 'Can', 'CA', 'Cana', '']\n",
    "            }\n",
    "            \n",
    "            for original, replacements in eu_variations.items():\n",
    "                if original in name:\n",
    "                    for replacement in replacements:\n",
    "                        if replacement == '':\n",
    "                            # When replacing with empty string, clean up double underscores\n",
    "                            new_name = name.replace('_' + original, '')  # Remove with preceding underscore\n",
    "                            if new_name == name:  # If no preceding underscore, try without\n",
    "                                new_name = name.replace(original + '_', '')  # Remove with following underscore\n",
    "                            if new_name == name:  # If still no change\n",
    "                                new_name = name.replace(original, '')  # Just remove it\n",
    "                            variants.add(new_name)\n",
    "                        else:\n",
    "                            variants.add(name.replace(original, replacement))\n",
    "                \n",
    "            # Handle acronym truncations \n",
    "            acronym_variations = {\n",
    "                'AIFMD': ['AIFMD', 'AIFM', 'AIF'],\n",
    "                'EMIR': ['EMIR'],\n",
    "                'MiFID': ['MiFID', 'MiFI']\n",
    "            }\n",
    "    \n",
    "            for original, replacements in acronym_variations.items():\n",
    "                if original in name:\n",
    "                    for replacement in replacements:\n",
    "                        variants.add(name.replace(original, replacement))\n",
    "                        \n",
    "             # Handle truncated words more aggressively\n",
    "            # Split by underscores and try partial matches\n",
    "            parts = name.split('_')\n",
    "            truncated_variants = []\n",
    "            \n",
    "            # Special handling for known problematic cases\n",
    "            if 'AIFMD' in name:\n",
    "                # Try without the 'D' at the end\n",
    "                truncated_variants.append(name.replace('AIFMD', 'AIFM'))\n",
    "    \n",
    "            if 'Canada' in name and 'National_Instrument_31103' in name:\n",
    "                # Try without Canada at the end\n",
    "                truncated_variants.append(name.replace('_Canada', ''))\n",
    "    \n",
    "            for i, part in enumerate(parts):\n",
    "                # Try truncating longer words\n",
    "                if len(part) > 6:\n",
    "                    truncated_variants.append('_'.join(parts[:i] + [part[:6]] + parts[i+1:]))\n",
    "                    truncated_variants.append('_'.join(parts[:i] + [part[:8]] + parts[i+1:]))\n",
    "    \n",
    "            variants.update(truncated_variants)\n",
    "            \n",
    "            # Remove underscores version\n",
    "            variants.add(name.replace('_', ''))\n",
    "            \n",
    "            return list(variants)\n",
    "        \n",
    "        # Get all variants to search for\n",
    "        search_variants = create_matching_variants(law_name)\n",
    "        \n",
    "        print(f\"Searching for background file for: {law_name}\")\n",
    "        print(f\"Looking for mechanism: {clean_mechanism}\")\n",
    "        print(f\"Search variants: {search_variants[:3]}...\")  # Show first 3\n",
    "        \n",
    "        # Try exact matches with variants\n",
    "        for variant in search_variants:\n",
    "            variant_file = f\"{variant}_{clean_mechanism}_background_hypothesis.txt\"\n",
    "            full_path = os.path.join(back_hypo_dir, variant_file)\n",
    "            if os.path.exists(full_path):\n",
    "                print(f\"Found exact match: {variant_file}\")\n",
    "                return full_path\n",
    "        \n",
    "        # Enhanced fuzzy matching with scoring\n",
    "        def calculate_match_score(file_name, target_law, target_mechanism):\n",
    "            \"\"\"Calculate similarity score between file name and target\"\"\"\n",
    "            score = 0\n",
    "            \n",
    "            # Remove the suffix to get the core name\n",
    "            if file_name.endswith('_background_hypothesis.txt'):\n",
    "                core_name = file_name[:-28]  # Remove '_background_hypothesis.txt'\n",
    "            else:\n",
    "                core_name = file_name\n",
    "            \n",
    "            # Check if this file is for the right mechanism\n",
    "            if not core_name.endswith(f'_{target_mechanism}'):\n",
    "                return 0  # Wrong mechanism, skip\n",
    "            \n",
    "            # Get the law part (everything before the mechanism)\n",
    "            law_part = core_name[:-len(f'_{target_mechanism}')]\n",
    "            \n",
    "            # Normalize both names for comparison\n",
    "            file_normalized = law_part.replace('_', '').lower()\n",
    "            target_normalized = target_law.replace('_', '').lower()\n",
    "            \n",
    "            # Exact match gets highest score\n",
    "            if file_normalized == target_normalized:\n",
    "                return 100\n",
    "            \n",
    "            # Handle truncation scenarios more aggressively\n",
    "            def normalize_for_truncation(text):\n",
    "                \"\"\"Normalize text for truncation matching\"\"\"\n",
    "                # Handle common truncations\n",
    "                text = text.replace('europeanunion', 'europea')\n",
    "                text = text.replace('european', 'europea')\n",
    "                text = text.replace('infrastructure', 'infrastr')\n",
    "                text = text.replace('regulation', 'regulat')\n",
    "                text = text.replace('directive', 'direct')\n",
    "                text = text.replace('instrument', 'instrum')\n",
    "                text = text.replace('requirements', 'require')\n",
    "                text = text.replace('registration', 'registr')\n",
    "                text = text.replace('national', 'nation')\n",
    "                text = text.replace('aifmd', 'aifm')  \n",
    "                text = text.replace('canada', '')  \n",
    "                return text\n",
    "            \n",
    "            file_truncated = normalize_for_truncation(file_normalized)\n",
    "            target_truncated = normalize_for_truncation(target_normalized)\n",
    "            \n",
    "            # Check truncated versions\n",
    "            if file_truncated == target_truncated:\n",
    "                score += 80\n",
    "            elif file_truncated in target_truncated or target_truncated in file_truncated:\n",
    "                score += 60\n",
    "            \n",
    "            # Check for EU abbreviation matches (more variations)\n",
    "            eu_mappings = {\n",
    "                'europeanunion': ['european', 'europe', 'eu', 'eur', 'europea', 'europeanuni'],\n",
    "                'canada': ['can', 'ca']\n",
    "            }\n",
    "            \n",
    "            for full_form, abbreviations in eu_mappings.items():\n",
    "                for abbrev in abbreviations:\n",
    "                    if full_form in target_normalized and abbrev in file_normalized:\n",
    "                        score += 40\n",
    "                    if abbrev in target_normalized and full_form in file_normalized:\n",
    "                        score += 40\n",
    "            \n",
    "            # Check if one contains the other\n",
    "            if target_normalized in file_normalized or file_normalized in target_normalized:\n",
    "                score += 30\n",
    "            \n",
    "            # Check for key regulation identifiers with partial matching\n",
    "            key_identifiers = [\n",
    "                ('aifmd', 'aifmd'),\n",
    "                ('aifmd', 'aifm'),\n",
    "                ('aifm', 'aifm'),\n",
    "                ('emir', 'emir'), \n",
    "                ('mifid', 'mifid'),\n",
    "                ('alternative', 'altern'),\n",
    "                ('investment', 'invest'),\n",
    "                ('fund', 'fund'),\n",
    "                ('managers', 'manag'),\n",
    "                ('market', 'market'),\n",
    "                ('infrastructure', 'infrastr'),\n",
    "                ('regulation', 'regulat'),\n",
    "                ('markets', 'market'),\n",
    "                ('financial', 'financ'),\n",
    "                ('instruments', 'instrum'),\n",
    "                ('directive', 'direct'),\n",
    "                ('national', 'nation'),\n",
    "                ('instrument', 'instrum'),\n",
    "                ('31103', '31103'),\n",
    "                ('registration', 'registr'),\n",
    "                ('requirements', 'require')\n",
    "            ]\n",
    "            \n",
    "            # Score based on matching key identifiers (both full and partial)\n",
    "            for full_identifier, partial_identifier in key_identifiers:\n",
    "                if full_identifier in target_normalized and full_identifier in file_normalized:\n",
    "                    score += 15\n",
    "                elif full_identifier in target_normalized and partial_identifier in file_normalized:\n",
    "                    score += 10\n",
    "                elif partial_identifier in target_normalized and full_identifier in file_normalized:\n",
    "                    score += 10\n",
    "            \n",
    "            return score\n",
    "        \n",
    "        # Score all files and find best match\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        print(f\"Trying fuzzy matching for {len(all_bg_files)} files...\")\n",
    "        \n",
    "        for bg_file in all_bg_files:\n",
    "            score = calculate_match_score(bg_file, law_name, clean_mechanism)\n",
    "            \n",
    "            if score > best_score and score >= 40:  # Minimum threshold\n",
    "                best_score = score\n",
    "                best_match = bg_file\n",
    "                print(f\"New best match: {bg_file} (score: {score})\")\n",
    "        \n",
    "        if best_match:\n",
    "            print(f\"Final match found: {best_match} (score: {best_score})\")\n",
    "            return os.path.join(back_hypo_dir, best_match)\n",
    "        else:\n",
    "            print(\"No suitable match found\")\n",
    "    \n",
    "    # Return None if not found\n",
    "    return None\n",
    "\n",
    "def get_available_laws_from_files(base_dir: str) -> dict:\n",
    "    \"\"\"Get laws and mechanisms that actually have files available\"\"\"\n",
    "    abs_dir = os.path.join(base_dir, 'abstracts')\n",
    "    if not os.path.exists(abs_dir):\n",
    "        return {}\n",
    "    \n",
    "    abstract_files = [f for f in os.listdir(abs_dir) if f.endswith('_abstract.txt')]\n",
    "    law_mechanisms = {}\n",
    "    mechanisms = ['Information_Asymmetry', 'Unsophisticated_Investors', 'Corporate_Governance', \n",
    "                 'Proprietary_Costs', 'Litigation_Risk', 'Equity_Issuance', 'Reputation_Risk']\n",
    "    \n",
    "    for file in abstract_files:\n",
    "        filename_base = file.replace('_abstract.txt', '')\n",
    "        law_name = None\n",
    "        mechanism = None\n",
    "        \n",
    "        for mech in mechanisms:\n",
    "            if filename_base.endswith('_' + mech):\n",
    "                law_name = filename_base.replace('_' + mech, '')\n",
    "                mechanism = mech.replace('_', ' ')\n",
    "                break\n",
    "        \n",
    "        if law_name and mechanism:\n",
    "            if law_name not in law_mechanisms:\n",
    "                law_mechanisms[law_name] = []\n",
    "            law_mechanisms[law_name].append(mechanism)\n",
    "    \n",
    "    return law_mechanisms\n",
    "\n",
    "def combine_single_law_mechanism(base_dir: str, law_name: str, mechanism: str) -> str:\n",
    "    \"\"\"Combine text sections for a specific law and mechanism\"\"\"\n",
    "    \n",
    "    clean_mechanism = mechanism.replace(' ', '_')\n",
    "    output_filename = f\"{law_name}_{clean_mechanism}_combined.txt\"\n",
    "    \n",
    "    # Define folder paths\n",
    "    folders = {\n",
    "        'abstracts': os.path.join(base_dir, 'abstracts'),\n",
    "        'introduction': os.path.join(base_dir, 'introduction'),\n",
    "        'background': os.path.join(base_dir, 'background and hypothesis development'),\n",
    "        'model_specification': os.path.join(base_dir, 'model_specification'),\n",
    "        'descriptive_stats': os.path.join(base_dir, 'descriptive_stats'),\n",
    "        'regression_analyses': os.path.join(base_dir, 'regression_analyses'),\n",
    "        'conclusion': os.path.join(base_dir, 'conclusion'),\n",
    "        'output': os.path.join(base_dir, 'combined_sections')\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== PROCESSING {law_name} - {mechanism} ===\")\n",
    "    \n",
    "    # Define file paths\n",
    "    files = {\n",
    "        'abstract': os.path.join(folders['abstracts'], f\"{law_name}_{clean_mechanism}_abstract.txt\"),\n",
    "        'introduction': os.path.join(folders['introduction'], f\"{law_name}_{clean_mechanism}_introduction.txt\"),\n",
    "        'model': os.path.join(folders['model_specification'], f\"panel_{law_name}_{clean_mechanism}_model_specification.txt\"),\n",
    "        'conclusion': os.path.join(folders['conclusion'], f\"panel_{law_name}_{clean_mechanism}_conclusion.txt\"),\n",
    "        'descriptive_stats': os.path.join(folders['descriptive_stats'], f\"panel_{law_name}_{clean_mechanism}\", 'descriptive_stats_analysis.txt'),\n",
    "        'regression': os.path.join(folders['regression_analyses'], f\"panel_{law_name}_{clean_mechanism}\", 'claude_interpretation.txt')\n",
    "    }\n",
    "    \n",
    "    # Find background file with enhanced matching\n",
    "    background_file = find_background_file(folders['background'], law_name, clean_mechanism)\n",
    "    if background_file:\n",
    "        files['background'] = background_file\n",
    "        print(f\"Background found: {background_file}\")\n",
    "    else:\n",
    "        print(f\"Background missing for {law_name} - {mechanism}\")\n",
    "        # Continue processing anyway - you can change this behavior if needed\n",
    "        print(\"Proceeding without background file...\")\n",
    "    \n",
    "    # Check all files exist (except background which we handle separately)\n",
    "    missing_files = []\n",
    "    for section, filepath in files.items():\n",
    "        if section == 'background' and not background_file:\n",
    "            continue  # Skip background if not found\n",
    "        if not os.path.exists(filepath):\n",
    "            missing_files.append(section)\n",
    "            print(f\"MISSING: {section} - {filepath}\")\n",
    "        else:\n",
    "            print(f\"FOUND: {section} - {filepath}\")\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"Skipping {law_name} - {mechanism}: Missing {', '.join(missing_files)}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"All required sections found. Proceeding with combination...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(folders['output'], exist_ok=True)\n",
    "    \n",
    "    # Combine content\n",
    "    combined_text = f\"Analysis of {law_name} through {mechanism} channel\\n\\n\"\n",
    "    \n",
    "    # Add each section\n",
    "    sections = [\n",
    "        ('abstract', 'Abstract: '),\n",
    "        ('introduction', 'INTRODUCTION\\n' + '='*50 + '\\n\\n'),\n",
    "        ('background', 'BACKGROUND AND HYPOTHESIS DEVELOPMENT\\n' + '='*50 + '\\n\\n'),\n",
    "        ('model', 'RESEARCH DESIGN\\n' + '='*50 + '\\n\\n'),\n",
    "        ('descriptive_stats', 'DESCRIPTIVE STATISTICS\\n' + '='*50 + '\\n\\n'),\n",
    "        ('regression', 'RESULTS\\n' + '='*50 + '\\n\\n'),\n",
    "        ('conclusion', 'CONCLUSION\\n' + '='*50 + '\\n\\n')\n",
    "    ]\n",
    "    \n",
    "    for section_name, header in sections:\n",
    "        if section_name == 'background' and not background_file:\n",
    "            continue  # Skip background section if file not found\n",
    "            \n",
    "        filepath = files.get(section_name)\n",
    "        if filepath and os.path.exists(filepath):\n",
    "            print(f\"Adding {section_name} from {filepath}\")\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                content = f.read()\n",
    "                if section_name == 'abstract':\n",
    "                    combined_text += header + content.strip() + \"\\n\\n\"\n",
    "                    combined_text += \"\\f\"  # Page break\n",
    "                else:\n",
    "                    combined_text += header + content + \"\\n\\n\"\n",
    "    \n",
    "    # Save combined file\n",
    "    output_file = os.path.join(folders['output'], output_filename)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(combined_text)\n",
    "    \n",
    "    print(f\"✓ Successfully saved: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def create_law_mechanism_dict(csv_file: str, available_laws: dict) -> dict:\n",
    "    \"\"\"Create dictionary of laws and their active mechanisms\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    csv_law_mechanisms = {}\n",
    "    \n",
    "    mechanisms = [\n",
    "        'Litigation Risk', 'Corporate Governance', 'Proprietary Costs',\n",
    "        'Information Asymmetry', 'Unsophisticated Investors', 'Equity Issuance', 'Reputation Risk'\n",
    "    ]\n",
    "    \n",
    "    # Get laws from CSV that have files\n",
    "    for _, row in df.iterrows():\n",
    "        law_title = row['Regulation Title'].replace(' ', '_')\n",
    "        if law_title in available_laws:\n",
    "            active_mechanisms = [mech for mech in mechanisms if row[mech] == 'Yes']\n",
    "            available_mechanisms_for_law = available_laws[law_title]\n",
    "            filtered_mechanisms = [mech for mech in active_mechanisms if mech in available_mechanisms_for_law]\n",
    "            if filtered_mechanisms:\n",
    "                csv_law_mechanisms[law_title] = filtered_mechanisms\n",
    "    \n",
    "    # Add laws that have files but aren't in CSV\n",
    "    final_law_mechanisms = csv_law_mechanisms.copy()\n",
    "    for law_name, available_mechs in available_laws.items():\n",
    "        if law_name not in final_law_mechanisms:\n",
    "            final_law_mechanisms[law_name] = available_mechs\n",
    "            print(f\"Note: {law_name} has files but not in CSV. Including all mechanisms: {available_mechs}\")\n",
    "    \n",
    "    return final_law_mechanisms\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to combine all laws\"\"\"\n",
    "    \n",
    "    print(\"Script starting...\")\n",
    "    \n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    CSV_FILE = os.path.join(BASE_DIR, \"enter file path here\")\n",
    "    \n",
    "    # Get available laws\n",
    "    available_laws = get_available_laws_from_files(BASE_DIR)\n",
    "    print(f\"Found {len(available_laws)} laws with available files:\")\n",
    "    for law, mechanisms in available_laws.items():\n",
    "        print(f\"  - {law}: {mechanisms}\")\n",
    "    print()\n",
    "    \n",
    "    # Create processing list\n",
    "    law_mechanisms = create_law_mechanism_dict(CSV_FILE, available_laws)\n",
    "    total_combinations = sum(len(mechanisms) for mechanisms in law_mechanisms.values())\n",
    "    print(f\"\\nProcessing {total_combinations} law-mechanism combinations\")\n",
    "    \n",
    "    # Process each combination\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for law_name, mechanisms in law_mechanisms.items():\n",
    "        for mechanism in mechanisms:\n",
    "            try:\n",
    "                result = combine_single_law_mechanism(BASE_DIR, law_name, mechanism)\n",
    "                if result:\n",
    "                    successful += 1\n",
    "                else:\n",
    "                    failed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error processing {law_name} - {mechanism}: {str(e)}\")\n",
    "                failed += 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL SUMMARY:\")\n",
    "    print(f\"Total combinations: {total_combinations}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# 15. Ask Claude to create a reference list\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import anthropic\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_LEFT\n",
    "\n",
    "def create_reference_pdf(references, output_path):\n",
    "    \"\"\"\n",
    "    Creates a PDF with properly formatted references using ReportLab.\n",
    "    \n",
    "    Args:\n",
    "        references (list or str): List of references or string containing references\n",
    "        output_path (str): Path where the PDF will be saved\n",
    "    \"\"\"\n",
    "    doc = SimpleDocTemplate(\n",
    "        output_path,\n",
    "        pagesize=letter,\n",
    "        rightMargin=72,\n",
    "        leftMargin=72,\n",
    "        topMargin=72,\n",
    "        bottomMargin=72\n",
    "    )\n",
    "    \n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    # Create style for references with proper hanging indentation\n",
    "    ref_style = ParagraphStyle(\n",
    "        'Reference',\n",
    "        parent=styles['Normal'],\n",
    "        fontName='Times-Roman',\n",
    "        fontSize=12,\n",
    "        leading=14,\n",
    "        leftIndent=36,  # Overall left indent\n",
    "        firstLineIndent=-36,  # Creates hanging indent\n",
    "        alignment=TA_LEFT,\n",
    "        spaceAfter=12  # Space between references\n",
    "    )\n",
    "    \n",
    "    # Create header style\n",
    "    header_style = ParagraphStyle(\n",
    "        'Header',\n",
    "        parent=styles['Normal'],\n",
    "        fontName='Times-Bold',\n",
    "        fontSize=12,\n",
    "        spaceBefore=0,\n",
    "        spaceAfter=20,\n",
    "        alignment=TA_LEFT\n",
    "    )\n",
    "    \n",
    "    # Initialize story for the PDF\n",
    "    story = []\n",
    "    \n",
    "    # Add References header\n",
    "    story.append(Paragraph(\"References\", header_style))\n",
    "    \n",
    "    # Process references\n",
    "    if isinstance(references, str):\n",
    "        refs = clean_references(references)\n",
    "    else:\n",
    "        refs = references\n",
    "    \n",
    "    # Add each reference\n",
    "    for ref in refs:\n",
    "        if ref.strip():\n",
    "            # Clean and format the reference\n",
    "            ref = clean_reference(ref)\n",
    "            story.append(Paragraph(ref, ref_style))\n",
    "    \n",
    "    # Build PDF\n",
    "    doc.build(story)\n",
    "\n",
    "def clean_reference(ref):\n",
    "    \"\"\"\n",
    "    Cleans and formats a single reference.\n",
    "    \n",
    "    Args:\n",
    "        ref (str): Reference string to clean\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned reference\n",
    "    \"\"\"\n",
    "    # Remove TextBlock formatting more aggressively\n",
    "    ref = re.sub(r'TextBlock\\s*\\([^)]*\\)', '', ref)\n",
    "    ref = re.sub(r'citations=None,?\\s*text=', '', ref)\n",
    "    ref = re.sub(r'type=\\'text\\',?\\s*', '', ref)\n",
    "    \n",
    "    # Remove line breaks and excess whitespace\n",
    "    ref = ' '.join(ref.split())\n",
    "    \n",
    "    # Remove various formatting markers\n",
    "    ref = re.sub(r'\\'|\\\\\\n|\\\\n', '', ref)\n",
    "    \n",
    "    # Fix spacing around periods in author names\n",
    "    ref = re.sub(r'\\.\\s*([A-Z])', r'. \\1', ref)\n",
    "    \n",
    "    # Fix spacing around ampersands\n",
    "    ref = re.sub(r'\\s*&\\s*', ' & ', ref)\n",
    "    \n",
    "    # Fix multiple spaces\n",
    "    ref = re.sub(r'\\s+', ' ', ref)\n",
    "    \n",
    "    # Remove asterisks around journal names while preserving italics in PDF\n",
    "    ref = re.sub(r'\\s*\\*([^*]+)\\*', r' \\1', ref)\n",
    "    \n",
    "    # Ensure proper spacing after commas\n",
    "    ref = re.sub(r',\\s*', ', ', ref)\n",
    "    \n",
    "    # Fix spacing around parentheses\n",
    "    ref = re.sub(r'\\s*\\(\\s*', ' (', ref)\n",
    "    ref = re.sub(r'\\s*\\)', ')', ref)\n",
    "    \n",
    "    # Remove any remaining parenthetical formatting artifacts\n",
    "    ref = re.sub(r'\\([^)]*citations[^)]*\\)', '', ref)\n",
    "    \n",
    "    # Ensure the reference ends with a period\n",
    "    ref = ref.rstrip('.')\n",
    "    ref += '.'\n",
    "    \n",
    "    return ref.strip()\n",
    "\n",
    "def clean_references(text):\n",
    "    \"\"\"\n",
    "    Cleans and splits reference text into individual references.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Full text containing references\n",
    "    \n",
    "    Returns:\n",
    "        list: List of cleaned references\n",
    "    \"\"\"\n",
    "    # First, remove all TextBlock formatting\n",
    "    text = re.sub(r'TextBlock\\s*\\([^)]*\\)\\s*', '', text)\n",
    "    text = re.sub(r'citations=None,?\\s*text=', '', text)\n",
    "    text = re.sub(r'type=\\'text\\',?\\s*', '', text)\n",
    "    \n",
    "    # Remove extra quotes and formatting\n",
    "    text = re.sub(r'[\\'\\\"]', '', text)\n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    \n",
    "    # Split into potential references\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Initialize variables\n",
    "    refs = []\n",
    "    current_ref = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Skip empty lines, headers, and formatting remnants\n",
    "        if not line or line.lower() == 'references' or 'textblock' in line.lower():\n",
    "            continue\n",
    "            \n",
    "        # If line starts with a capital letter and previous reference exists,\n",
    "        # it's probably a new reference\n",
    "        if re.match(r'^[A-Z]', line) and current_ref:\n",
    "            refs.append(' '.join(current_ref))\n",
    "            current_ref = [line]\n",
    "        else:\n",
    "            current_ref.append(line)\n",
    "    \n",
    "    # Add the last reference\n",
    "    if current_ref:\n",
    "        refs.append(' '.join(current_ref))\n",
    "    \n",
    "    # Clean each reference\n",
    "    cleaned_refs = []\n",
    "    for ref in refs:\n",
    "        cleaned = clean_reference(ref)\n",
    "        if cleaned and not cleaned.isspace() and len(cleaned) > 10:  # Filter out very short \"references\"\n",
    "            cleaned_refs.append(cleaned)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_refs = []\n",
    "    for ref in cleaned_refs:\n",
    "        if ref not in seen:\n",
    "            seen.add(ref)\n",
    "            unique_refs.append(ref)\n",
    "    \n",
    "    return unique_refs\n",
    "\n",
    "def get_formatted_references(prompt, max_retries=3):\n",
    "    \"\"\"\n",
    "    Gets formatted references using the Anthropic Claude API with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send to Claude\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted references from Claude, or None if failed\n",
    "    \"\"\"\n",
    "    # Get API key from environment variable\n",
    "    api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "    if not api_key:\n",
    "        # Fallback to hardcoded key if environment variable not set\n",
    "        api_key = \"enter API here\"\n",
    "    \n",
    "    try:\n",
    "        client = anthropic.Anthropic(api_key=api_key)\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Make the API call\n",
    "                message = client.messages.create(\n",
    "                    model=\"claude-sonnet-4-20250514\",\n",
    "                    max_tokens=4000,\n",
    "                    temperature=0.5,\n",
    "                    system=\"You are a helpful research assistant with expertise in academic citations. Format references in proper APA style with full journal names, volumes, and page numbers.\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                # Extract and clean the content - FIXED\n",
    "                if message and hasattr(message, 'content'):\n",
    "                    content = message.content\n",
    "                    if isinstance(content, list):\n",
    "                        # Extract just the text from TextBlock objects\n",
    "                        text_parts = []\n",
    "                        for item in content:\n",
    "                            if hasattr(item, 'text'):\n",
    "                                text_parts.append(item.text)\n",
    "                            else:\n",
    "                                text_parts.append(str(item))\n",
    "                        content = '\\n'.join(text_parts)\n",
    "                    elif hasattr(content, 'text'):\n",
    "                        content = content.text\n",
    "                    return content\n",
    "                \n",
    "                return None\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"API Error on attempt {attempt + 1}: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    return None\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Client initialization error: {e}\")\n",
    "        return None\n",
    "\n",
    "def batch_process_files(input_dir, output_dir, start_from=0, delay_seconds=2):\n",
    "    \"\"\"\n",
    "    Process all text files in a directory and create corresponding reference PDFs.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Path to directory containing input text files\n",
    "        output_dir (str): Path to directory where PDFs will be saved\n",
    "        start_from (int): File index to start from (for resuming)\n",
    "        delay_seconds (int): Delay between API calls to avoid rate limits\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get list of files to process\n",
    "    files_to_process = [f for f in os.listdir(input_dir) if f.endswith('_combined.txt')]\n",
    "    files_to_process.sort()  # Process in consistent order\n",
    "    \n",
    "    # Counter for processed files\n",
    "    processed = 0\n",
    "    errors = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    print(f\"Found {len(files_to_process)} files to process\")\n",
    "    print(f\"Starting from file index {start_from}\")\n",
    "    \n",
    "    # Process each file in the input directory\n",
    "    for i, filename in enumerate(files_to_process):\n",
    "        if i < start_from:\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Construct full input path\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            \n",
    "            # Create output filename\n",
    "            output_filename = filename.replace('_combined.txt', '_references.pdf')\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            # Skip if output already exists\n",
    "            if os.path.exists(output_path):\n",
    "                print(f\"Skipping {filename} - output already exists\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing {i+1}/{len(files_to_process)}: {filename}\")\n",
    "            \n",
    "            # Read input file\n",
    "            with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Create prompt for Claude\n",
    "            prompt = f\"\"\"Based on the following text, generate a reference list in APA format. \n",
    "            Format each reference exactly like these examples:\n",
    "\n",
    "            Leuz, C., & Verrecchia, R. E. (2000). The economic consequences of increased disclosure. Journal of Accounting Research, 91-124.\n",
    "\n",
    "            Bourveau, T., She, G., & Zaldokas, A. (2020). Corporate disclosure as a tacit coordination mechanism: Evidence from cartel enforcement regulations. Journal of Accounting Research, 58(2), 295-332.\n",
    "\n",
    "            Text for analysis:\n",
    "            {text}\n",
    "\n",
    "            Please format each reference following the exact style above, including:\n",
    "            1. Remove any asterisks, TextBlock tags, or other formatting markers \n",
    "            2. Author names with initials\n",
    "            3. Full title in sentence case\n",
    "            4. Journal name in italics (use *journal name* for italics)\n",
    "            5. Volume, issue, and page numbers where applicable\n",
    "            6. Year in parentheses\n",
    "            7. One reference per line \n",
    "            8. Subsequent references should be followed by a space after the previous reference\n",
    "            9. Sort alphabetically by author's last name\n",
    "            10. Provide only the references, no extra text or explanations\"\"\"\n",
    "\n",
    "            \n",
    "            # Get formatted references from Claude\n",
    "            formatted_refs = get_formatted_references(prompt)\n",
    "            \n",
    "            if formatted_refs:\n",
    "                # Create the PDF with the formatted references\n",
    "                create_reference_pdf(formatted_refs, output_path)\n",
    "                processed += 1\n",
    "                print(f\"✓ Successfully processed: {filename}\")\n",
    "            else:\n",
    "                errors += 1\n",
    "                print(f\"✗ Error getting references for: {filename}\")\n",
    "            \n",
    "            # Add delay between API calls to avoid rate limits\n",
    "            if delay_seconds > 0 and i < len(files_to_process) - 1:\n",
    "                time.sleep(delay_seconds)\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            print(f\"✗ Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"PROCESSING COMPLETE!\")\n",
    "    print(f\"Total files found: {len(files_to_process)}\")\n",
    "    print(f\"Skipped: {skipped} files\")\n",
    "    print(f\"Successfully processed: {processed} files\")\n",
    "    print(f\"Errors: {errors} files\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set input and output directories\n",
    "    input_directory = r\"enter folder path here\"\n",
    "    output_directory = r\"enter folder path here\"\n",
    "    \n",
    "    # Process all files\n",
    "    batch_process_files(input_directory, output_directory, start_from=0, delay_seconds=2)\n",
    "    \n",
    "# 16. Combine manuscript files with pdf table files for descriptive statistics and regression analysis\n",
    "\n",
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfMerger\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, KeepTogether, PageBreak\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_JUSTIFY, TA_LEFT, TA_CENTER\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "\n",
    "def clean_mechanism_name(mechanism):\n",
    "    \"\"\"Clean mechanism name for consistent formatting\"\"\"\n",
    "    # Replace spaces and hyphens with underscores, remove extra spaces\n",
    "    cleaned = re.sub(r'[\\s\\-]+', '_', mechanism.strip())\n",
    "    # Remove any trailing underscores\n",
    "    cleaned = cleaned.rstrip('_')\n",
    "    return cleaned\n",
    "\n",
    "def format_title_name(name):\n",
    "    \"\"\"Format name for display in titles with proper spacing\"\"\"\n",
    "    title_fixes = {\n",
    "        'Interactive_Datafor_Financial_Reporting': 'Interactive Data for Financial Reporting',\n",
    "        'Internet_Availabilityof_Proxy_Materials': 'Internet Availability of Proxy Materials',\n",
    "        'Political_Contributionsby_Investment_Advisers': 'Political Contributions by Investment Advisers',\n",
    "        'Proxy_Votingby_Investment_Advisers': 'Proxy Voting by Investment Advisers',\n",
    "        'Regulation_ABAsset_Backed_Securities': 'Regulation AB Asset Backed Securities',\n",
    "        'Regulation_RBank_Securities_Activities': 'Regulation R Bank Securities Activities',\n",
    "        'Regulation_SBSRSecurity_Based_Swap_Reporting': 'Regulation SBSR Security Based Swap Reporting',\n",
    "        'Regulation_SFPSSecurities_Financing_Transaction_Reporting': 'Regulation SFPS Securities Financing Transaction Reporting',\n",
    "        'Regulation_BTRBlackout_Trading_Restriction': 'Regulation BTR Blackout Trading Restriction'\n",
    "    }\n",
    "    \n",
    "    # Check if we have a specific fix for this name\n",
    "    if name in title_fixes:\n",
    "        return title_fixes[name]\n",
    "    \n",
    "    # For other cases, just replace underscores with spaces\n",
    "    return name.replace('_', ' ')\n",
    "\n",
    "def get_actual_law_mechanism_pairs(base_dir):\n",
    "    \"\"\"Get actual law-mechanism pairs from existing combined files\"\"\"\n",
    "    combined_dir = os.path.join(base_dir, 'combined_sections')\n",
    "    if not os.path.exists(combined_dir):\n",
    "        print(f\"Combined sections directory not found: {combined_dir}\")\n",
    "        return []\n",
    "    \n",
    "    # Define known mechanisms to help with parsing\n",
    "    known_mechanisms = [\n",
    "        'Information_Asymmetry',\n",
    "        'Corporate_Governance', \n",
    "        'Unsophisticated_Investors',\n",
    "        'Litigation_Risk',\n",
    "        'Reputation_Risk',\n",
    "        'Proprietary_Costs',\n",
    "        'Equity_Issuance'\n",
    "    ]\n",
    "    \n",
    "    pairs = []\n",
    "    for filename in os.listdir(combined_dir):\n",
    "        if filename.endswith('_combined.txt'):\n",
    "            # Remove _combined.txt suffix\n",
    "            base_name = filename.replace('_combined.txt', '')\n",
    "            \n",
    "            # Try to find the mechanism in the filename\n",
    "            mechanism_found = None\n",
    "            law_name = None\n",
    "            \n",
    "            for mechanism in known_mechanisms:\n",
    "                if mechanism in base_name:\n",
    "                    mechanism_found = mechanism\n",
    "                    # The law name is everything before the mechanism\n",
    "                    law_name = base_name.replace(f'_{mechanism}', '')\n",
    "                    break\n",
    "            \n",
    "            if mechanism_found and law_name:\n",
    "                pairs.append((law_name, mechanism_found))\n",
    "            else:\n",
    "                print(f\"Warning: Could not parse filename: {filename}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def get_descriptive_stats(desc_dir: str, law_name: str, mechanism: str) -> str:\n",
    "    \"\"\"Get descriptive statistics PDF from panel subfolder\"\"\"\n",
    "    # Panel folder uses underscores\n",
    "    panel_name = f\"panel_{law_name}_{mechanism}\"\n",
    "    panel_dir = os.path.join(desc_dir, panel_name)\n",
    "    \n",
    "    # Look for descriptive_stats_table.pdf\n",
    "    desc_file = os.path.join(panel_dir, 'descriptive_stats_table.pdf')\n",
    "    \n",
    "    if os.path.exists(desc_file):\n",
    "        print(f\"Found descriptive statistics table: {desc_file}\")\n",
    "        return desc_file\n",
    "    else:\n",
    "        print(f\"Warning: No descriptive statistics table found in {panel_dir}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_regression_analyses(reg_dir: str, law_name: str, mechanism: str) -> str:\n",
    "    \"\"\"Get regression analyses PDF from panel subfolder\"\"\"\n",
    "    # Panel folder uses underscores\n",
    "    panel_name = f\"panel_{law_name}_{mechanism}\"\n",
    "    panel_dir = os.path.join(reg_dir, panel_name)\n",
    "    \n",
    "    # Look for regression_table.pdf\n",
    "    reg_file = os.path.join(panel_dir, 'regression_table.pdf')\n",
    "    \n",
    "    if os.path.exists(reg_file):\n",
    "        print(f\"Found regression table: {reg_file}\")\n",
    "        return reg_file\n",
    "    else:\n",
    "        print(f\"Warning: No regression table found in {panel_dir}\")\n",
    "        return \"\"\n",
    "\n",
    "def merge_pdf_files(base_dir: str, law_name: str, mechanism: str):\n",
    "    \"\"\"Merge manuscript PDF with regression results, descriptive statistics, and reference PDFs\"\"\"\n",
    "    # Register Times New Roman font\n",
    "    try:\n",
    "        pdfmetrics.registerFont(TTFont('Times New Roman', 'times.ttf'))\n",
    "        pdfmetrics.registerFont(TTFont('Times New Roman Bold', 'timesbd.ttf'))\n",
    "    except:\n",
    "        print(\"Warning: Times New Roman font not found, using default font\")\n",
    "    \n",
    "    # Define file paths\n",
    "    combined_sections_dir = os.path.join(base_dir, 'combined_sections')\n",
    "    reg_dir = os.path.join(base_dir, 'regression_analyses')\n",
    "    desc_dir = os.path.join(base_dir, 'descriptive_stats')\n",
    "    corr_dir = os.path.join(base_dir, 'correlations')\n",
    "    ref_dir = os.path.join(base_dir, 'references')\n",
    "    output_dir = os.path.join(base_dir, 'final_manuscripts')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if manuscript exists (mechanism already has underscores)\n",
    "    manuscript_file = os.path.join(combined_sections_dir, f\"{law_name}_{mechanism}_combined.txt\")\n",
    "    if not os.path.exists(manuscript_file):\n",
    "        print(f\"Manuscript file not found: {manuscript_file}\")\n",
    "        return False\n",
    "\n",
    "    # Create intermediate PDF with formatting\n",
    "    temp_pdf_path = os.path.join(output_dir, f'temp_{law_name}_{mechanism}.pdf')\n",
    "    temp_pdf = SimpleDocTemplate(\n",
    "        temp_pdf_path,\n",
    "        pagesize=letter,\n",
    "        rightMargin=72,\n",
    "        leftMargin=72,\n",
    "        topMargin=72,\n",
    "        bottomMargin=72\n",
    "    )\n",
    "\n",
    "    # Create styles\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    custom_title = ParagraphStyle(\n",
    "        name='CustomTitle',\n",
    "        fontName='Times New Roman Bold',\n",
    "        fontSize=16,\n",
    "        spaceAfter=16,\n",
    "        spaceBefore=24,\n",
    "        firstLineIndent=0,\n",
    "        alignment=TA_CENTER,\n",
    "        leading=24\n",
    "    )\n",
    "    \n",
    "    subtitle_style = ParagraphStyle(\n",
    "        name='CustomSubtitle',\n",
    "        fontName='Times New Roman',\n",
    "        fontSize=12,\n",
    "        spaceAfter=24,\n",
    "        spaceBefore=12,\n",
    "        firstLineIndent=0,\n",
    "        alignment=TA_CENTER,\n",
    "        leading=24\n",
    "    )\n",
    "    \n",
    "    abstract_style = ParagraphStyle(\n",
    "        name='Abstract',\n",
    "        fontName='Times New Roman',\n",
    "        fontSize=12,\n",
    "        firstLineIndent=0,\n",
    "        spaceAfter=60,\n",
    "        leading=14,\n",
    "        alignment=TA_JUSTIFY\n",
    "    )\n",
    "    \n",
    "    regular_style = ParagraphStyle(\n",
    "        name='CustomRegular',\n",
    "        fontName='Times New Roman',\n",
    "        fontSize=12,\n",
    "        spaceAfter=12,\n",
    "        firstLineIndent=36,\n",
    "        leading=24,\n",
    "        alignment=TA_JUSTIFY\n",
    "    )\n",
    "    \n",
    "    heading_style = ParagraphStyle(\n",
    "        name='CustomHeading',\n",
    "        fontName='Times New Roman',\n",
    "        fontSize=12,\n",
    "        spaceAfter=6,\n",
    "        spaceBefore=18,\n",
    "        firstLineIndent=0, \n",
    "        alignment=TA_LEFT, \n",
    "        leading=24\n",
    "    )\n",
    "    \n",
    "    subheading_style = ParagraphStyle(\n",
    "        name='CustomSubheading',\n",
    "        fontName='Times New Roman',\n",
    "        fontSize=12,\n",
    "        spaceAfter=6,\n",
    "        spaceBefore=12,\n",
    "        firstLineIndent=0,  \n",
    "        alignment=TA_LEFT,  \n",
    "        leading=24\n",
    "    )\n",
    "\n",
    "    # Read manuscript\n",
    "    with open(manuscript_file, 'r', encoding='utf-8') as f:\n",
    "        manuscript_text = f.read()\n",
    "\n",
    "    # Create story (content)\n",
    "    story = []\n",
    "    \n",
    "    # Format law name for display in the PDF title using the improved function\n",
    "    formatted_law_name = format_title_name(law_name)\n",
    "\n",
    "    # Add title and subtitle\n",
    "    title = f\"{formatted_law_name} and Voluntary Disclosure\"\n",
    "    story.append(Paragraph(title, custom_title))\n",
    "    story.append(Paragraph(\"Artemis Intelligencia\", subtitle_style))\n",
    "    story.append(Paragraph(\"September 10, 2025\", subtitle_style))\n",
    "    story.append(Spacer(1, 24))\n",
    "\n",
    "    # Common section headers to identify (exact matches)\n",
    "    main_headers = ['INTRODUCTION', 'BACKGROUND AND HYPOTHESIS DEVELOPMENT', \n",
    "                    'RESEARCH DESIGN', 'DESCRIPTIVE STATISTICS', 'RESULTS', 'CONCLUSION']\n",
    "    \n",
    "    # Subheaders - be more specific to avoid false positives\n",
    "    subheaders_exact = ['Background', 'Theoretical Framework','Hypothesis Development', 'Model Explanation', 'Mathematical Model', \n",
    "                        'Regression Analysis','Variable Definitions', 'Empirical Model', 'Sample Selection', 'Sample Construction',\n",
    "                        'Sample Description and Descriptive Statistics', \n",
    "                        'Sample Selection and Regulatory Context', 'Sample Selection and Data Sources',\n",
    "                         'Sample Construction and Regulatory Setting', 'Model Specification', \n",
    "                        'Sample Selection and Regulatory Framework', 'Model Development and Theoretical Framework',\n",
    "                         'Mathematical Specification', 'Sample Construction and Data Sources', 'Variable Definitions and Measurement'\n",
    "                         'Model Development and Theoretical Foundation', 'Model Development', 'Regression Specification']\n",
    "\n",
    "    # Process manuscript text more carefully\n",
    "    lines = manuscript_text.split('\\n')\n",
    "    current_paragraph = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line_stripped = line.strip()\n",
    "    \n",
    "        # Skip lines with just equals signs (separator lines)\n",
    "        if line_stripped and all(c == '=' for c in line_stripped):\n",
    "            continue\n",
    "    \n",
    "        # ADD THIS: Skip \"Research Design\" if previous non-empty line was \"RESEARCH DESIGN\"\n",
    "        # Skip \"Research Design\" if previous non-empty line was \"RESEARCH DESIGN\"\n",
    "        if line_stripped == \"Research Design\":\n",
    "            # Look back for the previous non-empty, non-equals line\n",
    "            skip_this_line = False\n",
    "            for j in range(i-1, -1, -1):\n",
    "                prev_line = lines[j].strip()\n",
    "                if prev_line and not all(c == '=' for c in prev_line):\n",
    "                    if prev_line == \"RESEARCH DESIGN\":\n",
    "                        skip_this_line = True\n",
    "                    break\n",
    "            if skip_this_line:\n",
    "                continue  # Now this continue applies to the main loop\n",
    "        \n",
    "        # Skip empty lines\n",
    "        if not line_stripped:\n",
    "            # If we have accumulated a paragraph, add it before the empty line\n",
    "            if current_paragraph:\n",
    "                paragraph_text = ' '.join(current_paragraph)\n",
    "                story.append(Paragraph(paragraph_text, regular_style))\n",
    "                current_paragraph = []\n",
    "            continue\n",
    "            \n",
    "        # Skip the original title if it appears\n",
    "        if i < 5 and ('Voluntary Disclosure' in line_stripped or 'Analysis of' in line_stripped) and len(line_stripped) < 150:\n",
    "            continue\n",
    "            \n",
    "        # Check if it's a main header (exact match or with = signs)\n",
    "        is_main_header = False\n",
    "        for header in main_headers:\n",
    "            if header == line_stripped.upper().replace('=', '').strip():\n",
    "                is_main_header = True\n",
    "                # Add any accumulated paragraph first\n",
    "                if current_paragraph:\n",
    "                    paragraph_text = ' '.join(current_paragraph)\n",
    "                    story.append(Paragraph(paragraph_text, regular_style))\n",
    "                    current_paragraph = []\n",
    "                # Add the header\n",
    "                story.append(Paragraph(header, heading_style))\n",
    "                break\n",
    "        \n",
    "        if is_main_header:\n",
    "            continue\n",
    "            \n",
    "        # Check if it's a subheader (exact match, short line)\n",
    "        is_subheader = False\n",
    "        if len(line_stripped) < 50:  # Subheaders are typically short\n",
    "            for subheader in subheaders_exact:\n",
    "                if subheader.lower() == line_stripped.lower():\n",
    "                    is_subheader = True\n",
    "                    # Add any accumulated paragraph first\n",
    "                    if current_paragraph:\n",
    "                        paragraph_text = ' '.join(current_paragraph)\n",
    "                        story.append(Paragraph(paragraph_text, regular_style))\n",
    "                        current_paragraph = []\n",
    "                    # Add the subheader\n",
    "                    story.append(Paragraph(line_stripped, subheading_style))\n",
    "                    break\n",
    "        \n",
    "        if is_subheader:\n",
    "            continue\n",
    "            \n",
    "        # It's part of a regular paragraph - accumulate it\n",
    "        current_paragraph.append(line_stripped)\n",
    "    \n",
    "    # Don't forget the last paragraph if there is one\n",
    "    if current_paragraph:\n",
    "        paragraph_text = ' '.join(current_paragraph)\n",
    "        story.append(Paragraph(paragraph_text, regular_style))\n",
    "\n",
    "    # Create the intermediate PDF\n",
    "    temp_pdf.build(story)\n",
    "\n",
    "    # Merge PDFs\n",
    "    merger = PdfMerger()\n",
    "    \n",
    "    try:\n",
    "        # Add formatted manuscript\n",
    "        merger.append(temp_pdf_path)\n",
    "        \n",
    "        # Add references (mechanism already has underscores)\n",
    "        ref_file = os.path.join(ref_dir, f\"{law_name}_{mechanism}_references.pdf\")\n",
    "        if os.path.exists(ref_file):\n",
    "            merger.append(ref_file)\n",
    "            print(f\"Added references from: {ref_file}\")\n",
    "        else:\n",
    "            print(f\"No references file found at: {ref_file}\")\n",
    "    \n",
    "        # Add descriptive statistics table\n",
    "        desc_stats_path = get_descriptive_stats(desc_dir, law_name, mechanism)\n",
    "        if desc_stats_path:\n",
    "            merger.append(desc_stats_path)\n",
    "        else:\n",
    "            print(f\"No descriptive statistics table found for {law_name}_{mechanism}\")\n",
    "            \n",
    "        # Add correlations table (mechanism already has underscores)\n",
    "        corr_file = os.path.join(corr_dir, f\"{law_name}_{mechanism}_correlation_table.pdf\")\n",
    "        if os.path.exists(corr_file):\n",
    "            merger.append(corr_file)\n",
    "            print(f\"Added correlation table from: {corr_file}\")\n",
    "        else:\n",
    "            print(f\"No correlation file found at: {corr_file}\")\n",
    "    \n",
    "        # Add regression table\n",
    "        reg_table_path = get_regression_analyses(reg_dir, law_name, mechanism)\n",
    "        if reg_table_path:\n",
    "            merger.append(reg_table_path)\n",
    "        else:\n",
    "            print(f\"No regression table found for {law_name}_{mechanism}\")\n",
    "\n",
    "        # Create output filename with properly formatted law name\n",
    "        formatted_law_name_for_file = format_title_name(law_name)\n",
    "        \n",
    "        # The mechanism keeps its underscores in the filename\n",
    "        output_file = os.path.join(output_dir, \n",
    "                                   f\"{formatted_law_name_for_file} and Voluntary Disclosure_{mechanism}_final.pdf\")\n",
    "        \n",
    "        merger.write(output_file)\n",
    "        merger.close()\n",
    "        \n",
    "        # Clean up temporary file\n",
    "        os.remove(temp_pdf_path)\n",
    "        \n",
    "        print(f\"✓ Successfully created formatted PDF for {law_name} - {mechanism}\")\n",
    "        print(f\"Saved to: {output_file}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating PDF: {str(e)}\")\n",
    "        return False\n",
    "    finally:\n",
    "        merger.close()\n",
    "\n",
    "# Batch processing of multiple laws and mechanisms\n",
    "def batch_merge_pdfs(base_dir):\n",
    "    # Clear any cached paths\n",
    "    import importlib\n",
    "    import sys\n",
    "    \n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Script BASE_DIR: {base_dir}\")\n",
    "    print(f\"Contents of BASE_DIR: {os.listdir(base_dir)}\")\n",
    "    \"\"\"Process all available law-mechanism combinations\"\"\"\n",
    "    \n",
    "    # Get actual pairs from existing files\n",
    "    laws_mechanisms = get_actual_law_mechanism_pairs(base_dir)\n",
    "    \n",
    "    if not laws_mechanisms:\n",
    "        print(\"No law-mechanism pairs found in combined_sections directory\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(laws_mechanisms)} law-mechanism combinations to process:\")\n",
    "    for law, mechanism in laws_mechanisms[:5]:  # Show first 5 as example\n",
    "        print(f\"  - {law} - {mechanism}\")\n",
    "    if len(laws_mechanisms) > 5:\n",
    "        print(f\"  ... and {len(laws_mechanisms) - 5} more\")\n",
    "    print()\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for law, mechanism in laws_mechanisms:\n",
    "        try:\n",
    "            result = merge_pdf_files(base_dir, law, mechanism)\n",
    "            if result:\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {law} - {mechanism}: {str(e)}\")\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"PDF MERGING COMPLETE!\")\n",
    "    print(f\"Total combinations: {len(laws_mechanisms)}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    \n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    batch_merge_pdfs(BASE_DIR)\n",
    "    \n",
    "# 17. Add page numbers \n",
    "import os\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter\n",
    "import io\n",
    "\n",
    "def add_page_numbers(input_path, output_path):\n",
    "    reader = PdfReader(input_path)\n",
    "    writer = PdfWriter()\n",
    "    \n",
    "    for i in range(len(reader.pages)):\n",
    "        page = reader.pages[i]\n",
    "        packet = io.BytesIO()\n",
    "        can = canvas.Canvas(packet, pagesize=(page.mediabox.width, page.mediabox.height))\n",
    "        \n",
    "        if i > 0:\n",
    "            can.setFont('Times-Roman', 12)\n",
    "            can.drawString(page.mediabox.width/2 - 6, 40, str(i))\n",
    "        \n",
    "        can.save()\n",
    "        packet.seek(0)\n",
    "        number_pdf = PdfReader(packet)\n",
    "        \n",
    "        if len(number_pdf.pages) > 0:\n",
    "            page.merge_page(number_pdf.pages[0])\n",
    "        writer.add_page(page)\n",
    "    \n",
    "    with open(output_path, \"wb\") as output_file:\n",
    "        writer.write(output_file)\n",
    "\n",
    "def batch_process_pdfs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    pdfs = [f for f in os.listdir(input_dir) if f.endswith('.pdf')]\n",
    "    \n",
    "    for i, filename in enumerate(pdfs, 1):\n",
    "        print(f\"\\nProcessing {i}/{len(pdfs)}: {filename}\")\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # Shorten the filename if it's too long\n",
    "        base_name = filename[:-4]  # Remove .pdf\n",
    "        if len(base_name) > 100:  # If name is too long\n",
    "            # Keep first 50 and last 45 characters\n",
    "            shortened = base_name[:50] + \"...\" + base_name[-45:]\n",
    "            output_filename = f\"numbered_{shortened}.pdf\"\n",
    "        else:\n",
    "            output_filename = f\"numbered_{filename}\"\n",
    "            \n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        try:\n",
    "            add_page_numbers(input_path, output_path)\n",
    "            print(f\"Successfully processed: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "# Usage\n",
    "input_dir = r\"enter folder path here\"\n",
    "output_dir = r\"enter folder path here\"\n",
    "batch_process_pdfs(input_dir, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
