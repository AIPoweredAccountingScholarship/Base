{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a239cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code written in assistance from Claude 3.5-Sonnet\n",
    "#Claude was used to assist with improving documentation and improving handling of errors \n",
    "#Note: the code will run best when executing each section separately\n",
    "#Required inputs: CSV files with securities law data, data with \"GVKEY\" and \"FYEAR\" as identifiers\n",
    "\n",
    "# 1. Claude identify federal securities laws\n",
    "import os\n",
    "import anthropic\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def get_securities_laws(conversation_history=None):\n",
    "    # Initialize the Anthropic client\n",
    "    client = anthropic.Anthropic(\n",
    "        api_key=\"enter API here\"\n",
    "    )\n",
    "    \n",
    "    # Initial prompt \n",
    "    initial_content = \"\"\"Your task is to identify and compile a comprehensive database of at least 100 federal securities \n",
    "    laws. Securities regulation is the field of U.S. law that covers transactions and other dealings with securities. \n",
    "    Securities laws aim at ensuring that investors receive accurate and necessary information regarding the type and value\n",
    "    of the interest under consideration for purchase.\n",
    "\n",
    "The goal is to create a dataset that captures the following key details for each law. \n",
    "\n",
    "Please follow these guidelines:\n",
    "\n",
    "Data Fields to Collect:\n",
    "• Date: The announcement or implementation date of the law (use YYYY-MM-DD format).\n",
    "• Regulation Title or Name: The official name or designation of the regulatory change.\n",
    "• Regulatory Body/Authority: The government entity responsible for the law.\n",
    "• Description: A brief overview of the law, including key provisions and the rationale behind it.\n",
    "• Impact: The potential or observed effects on industries, markets, or stakeholders.\n",
    "•Litigation Risk: Is this law related to the risk of litigation against managers? By risk of litigation we mean the probability that a manager will be sued or face legal action because of this law. Answer this question with Yes or No. If yes, label the entry \"Litigation Risk\".\n",
    "•Corporate Governance: Is this law related to corporate governance of firms? Corporate governance refers to the internal monitoring system charged with overseeing managers and commonly focuses on matters such as board independence or insider trading policy. Answer this question with Yes or No.If yes, label the entry \"Corporate Governance\".\n",
    "•Proprietary Costs: Is this law related to proprietary costs of firms? By proprietary costs, we mean costs that result from the disclosure of information to competitors which could harm a firm’s competitive position. Answer this question with Yes or No.If yes, label the entry \"Proprietary Costs\".\n",
    "•Information Asymmetry: Is this law related to information asymmetry between owners and managers? By information asymmetry we mean that one party has more or better information than the other party. Answer this question with Yes or No. If yes, label the entry \"Information Asymmetry\".\n",
    "•Unsophisticated Investors: Is the law related to protecting unsophisticated investors? By unsophisticated investors, we mean investors that are either new to investing or are not well informed. Answer this question with Yes or No. If yes, label the entry \"Unsophisticated Investors\".\n",
    "•Equity Issuance in Public vs. Private Markets: Is this law related to the costs and benefits of issuing equity in public versus private markets? Answer this question with Yes or No. If yes, label the entry \"Equity Issuance in Public vs. Private Markets\".\n",
    "•Reputation Risk: Is this law related to the reputation of firm managers? By of firm manager, we mean the career prospects and prestige of an individual manager. Answer this question with Yes or No. If yes, label the entry \"Reputation Risk\".\n",
    "\n",
    "• References: Links to official documents or credible news sources.\n",
    "\n",
    "Requirements:\n",
    "• Scope: Cover as many laws as possible that were announced or implemented in the last 25 years.\n",
    "• Consistency: Ensure uniform formatting for all entries in the dataset.\n",
    "• Dates must be in YYYY-MM-DD format (e.g., 2002-07-30).\n",
    "\n",
    "Output:\n",
    "Provide data in a tabular format with rows for each law and columns for the data fields listed above. \n",
    "Use credible, authoritative sources such as government websites, legal databases, academic journals, or credible news sources.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        if conversation_history:\n",
    "            messages = conversation_history\n",
    "        else:\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": initial_content\n",
    "            }]\n",
    "\n",
    "        response = client.messages.create(\n",
    "            max_tokens=8192,\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.content[0].text, messages + [\n",
    "            {\"role\": \"assistant\", \"content\": response.content[0].text}\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error making API call: {e}\")\n",
    "        return None, messages\n",
    "\n",
    "def add_follow_up_prompt(conversation_history, follow_up_prompt):\n",
    "    \"\"\"Add a follow-up prompt to the conversation history\"\"\"\n",
    "    return conversation_history + [{\"role\": \"user\", \"content\": follow_up_prompt}]\n",
    "\n",
    "def standardize_date(date_str):\n",
    "    \"\"\"Attempt to standardize date format to YYYY-MM-DD\"\"\"\n",
    "    try:\n",
    "\n",
    "        return pd.to_datetime(date_str).strftime('%Y-%m-%d')\n",
    "    except:\n",
    "\n",
    "        return date_str\n",
    "\n",
    "def parse_response_to_dataframe(response_text: str) -> pd.DataFrame:\n",
    "    \"\"\"Parse the response text into a pandas DataFrame.\"\"\"\n",
    "    print(\"\\nParsing response...\")\n",
    "    \n",
    "    data = []\n",
    "    current_entry = None\n",
    "    entry_number = None\n",
    "    \n",
    "    # Split into lines and clean\n",
    "    lines = [line.strip() for line in response_text.split('\\n') if line.strip()]\n",
    "    \n",
    "    for line in lines:\n",
    "        # Check for new entry by looking for numbered entries (e.g., \"1.\" or \"101.\")\n",
    "        number_match = re.match(r'^(\\d+)\\.', line)\n",
    "        if number_match:\n",
    "            # Save previous entry if it exists\n",
    "            if current_entry and len(current_entry) > 0:\n",
    "                if 'Regulation Title' not in current_entry and entry_number:\n",
    "                    current_entry['Regulation Title'] = f\"Law {entry_number}\"\n",
    "                data.append(current_entry)\n",
    "            current_entry = {}\n",
    "            entry_number = number_match.group(1)\n",
    "            continue\n",
    "            \n",
    "        # Process key-value pairs\n",
    "        if ':' in line:\n",
    "            key, value = [x.strip() for x in line.split(':', 1)]\n",
    "            \n",
    "            # Map keys to column names\n",
    "            key_mapping = {\n",
    "                'Date': 'Date',\n",
    "                'Title': 'Regulation Title',\n",
    "                'Authority': 'Regulatory Body',\n",
    "                'Description': 'Description',\n",
    "                'Impact': 'Impact',\n",
    "                'Litigation Risk': 'Litigation Risk',\n",
    "                'Corporate Governance': 'Corporate Governance',\n",
    "                'Proprietary Costs': 'Proprietary Costs',\n",
    "                'Information Asymmetry': 'Information Asymmetry',\n",
    "                'Unsophisticated Investors': 'Unsophisticated Investors',\n",
    "                'Equity Issuance': 'Equity Issuance',\n",
    "                'Reputation Risk': 'Reputation Risk',\n",
    "                'References': 'References'\n",
    "            }\n",
    "            \n",
    "            if key in key_mapping:\n",
    "                column_name = key_mapping[key]\n",
    "                if column_name == 'Date':\n",
    "                    current_entry[column_name] = standardize_date(value)\n",
    "                else:\n",
    "                    current_entry[column_name] = value.strip()\n",
    "\n",
    "    # Add the last entry\n",
    "    if current_entry and len(current_entry) > 0:\n",
    "        if 'Regulation Title' not in current_entry and entry_number:\n",
    "            current_entry['Regulation Title'] = f\"Law {entry_number}\"\n",
    "        data.append(current_entry)\n",
    "    \n",
    "    print(f\"\\nFound {len(data)} entries\")\n",
    "    \n",
    "    \n",
    "    # Create DataFrame\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        required_columns = ['Date', 'Regulation Title', 'Regulatory Body', 'Description', 'Impact',\n",
    "                          'Litigation Risk', 'Corporate Governance', 'Proprietary Costs',\n",
    "                          'Information Asymmetry', 'Unsophisticated Investors', 'Equity Issuance',\n",
    "                          'Reputation Risk', 'References']\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                print(f\"Adding missing column: {col}\")\n",
    "                df[col] = None\n",
    "        \n",
    "        # Clean up titles \n",
    "        df['Regulation Title'] = df['Regulation Title'].fillna('Unknown')\n",
    "        df['Regulation Title'] = df['Regulation Title'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "        \n",
    "        # Create a composite key for deduplication\n",
    "        df['dedup_key'] = df.apply(lambda row: f\"{row['Date']}_{row['Regulation Title']}\", axis=1)\n",
    "        df = df.drop_duplicates(subset=['dedup_key'], keep='first')\n",
    "        df = df.drop('dedup_key', axis=1)\n",
    "        \n",
    "        # Reorder columns\n",
    "        df = df[required_columns]\n",
    "        print(f\"Created DataFrame with {len(df)} rows\")\n",
    "        return df.copy()\n",
    "    else:\n",
    "        print(\"No valid data to create DataFrame\")\n",
    "        return pd.DataFrame()\n",
    "                \n",
    "\n",
    "def compile_all_responses() -> pd.DataFrame:\n",
    "    \"\"\"Compile multiple API responses into a single DataFrame.\"\"\"\n",
    "    all_responses = []\n",
    "    conversation_history = None\n",
    "\n",
    "    # Get initial response\n",
    "    initial_response, conversation_history = get_securities_laws()\n",
    "    if initial_response:\n",
    "        print(\"\\nInitial response:\")\n",
    "        print(initial_response)\n",
    "        all_responses.append(initial_response)\n",
    "\n",
    "        follow_up_prompts = [\n",
    "            \"\"\"Starting with number {last_num}, list 20 more federal securities laws using this exact format for each:\n",
    "Date: YYYY-MM-DD\n",
    "Title: [title]\n",
    "Authority: [body]\n",
    "Description: [brief]\n",
    "Impact: [impact]\n",
    "Litigation Risk: Yes/No\n",
    "Corporate Governance: Yes/No\n",
    "Proprietary Costs: Yes/No\n",
    "Information Asymmetry: Yes/No\n",
    "Unsophisticated Investors: Yes/No\n",
    "Equity Issuance: Yes/No\n",
    "Reputation Risk: Yes/No\n",
    "References: [link]\"\"\",\n",
    "\n",
    "            \"Continue from number {last_num}. Provide 20 more laws using the exact same format.\",\n",
    "            \n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \n",
    "            \"Continue with 20 more laws from {last_num}. Same format.\",\n",
    "            \n",
    "            \"Add 20 more laws starting at {last_num}. Same format.\",\n",
    "            \n",
    "            \"List 20 more laws from number {last_num}. Same format.\",\n",
    "            \n",
    "            \"Provide 20 more laws starting at {last_num}. Same format.\",\n",
    "            \n",
    "            \"Add final set of laws starting at {last_num}. Same format.\",\n",
    "            \n",
    "            \"List 20 more laws from number {last_num}. Same format.\",\n",
    "            \n",
    "            \"Provide 20 more laws starting at {last_num}. Same format.\",\n",
    "            \n",
    "            \"Add final set of laws starting at {last_num}. Same format.\"\n",
    "            \n",
    "            \"List 20 more laws from number {last_num}. Same format.\",\n",
    "            \n",
    "            \"Provide 20 more laws starting at {last_num}. Same format.\",\n",
    "            \n",
    "            \"Add final set of laws starting at {last_num}. Same format.\"\n",
    "            \n",
    "            \"Recall that you have to identify at least 100 federal securities laws. Recall securities regulation is the field of U.S. law that covers transactions and other dealings with securities. Securities laws aim at ensuring that investors receive accurate and necessary information regarding the type and value of the interest under consideration for purchase.\"\n",
    "        ]\n",
    "        last_num = len(parse_response_to_dataframe(initial_response)) + 1\n",
    "        \n",
    "        for i, prompt_template in enumerate(follow_up_prompts, 1):\n",
    "            prompt = prompt_template.format(last_num=last_num)\n",
    "            conversation_history = add_follow_up_prompt(conversation_history, prompt)\n",
    "            response, conversation_history = get_securities_laws(conversation_history)\n",
    "            \n",
    "            if response:\n",
    "                print(f\"\\nFollow-up response {i}:\")\n",
    "                print(response)\n",
    "                all_responses.append(response)\n",
    "                df = parse_response_to_dataframe(response)\n",
    "                last_num += len(df)\n",
    "            \n",
    "    # Parse all responses into DataFrames and concatenate\n",
    "    dfs = []\n",
    "    for response in all_responses:\n",
    "        df = parse_response_to_dataframe(response)\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"No valid data frames were created!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Remove duplicates using multiple fields to better identify unique laws\n",
    "    final_df['Title_clean'] = final_df['Regulation Title'].fillna('').str.lower().str.strip()\n",
    "    final_df['Description_clean'] = final_df['Description'].fillna('').str.lower().str.strip()\n",
    "    \n",
    "    # Create composite key for deduplication\n",
    "    final_df['dedup_key'] = final_df.apply(\n",
    "        lambda row: f\"{row['Date']}_{row['Title_clean']}_{row['Description_clean'][:50]}\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Remove duplicates and cleanup\n",
    "    final_df = final_df.drop_duplicates(subset=['dedup_key'], keep='first')\n",
    "    final_df = final_df.drop(['Title_clean', 'Description_clean', 'dedup_key'], axis=1)\n",
    "\n",
    "    # Sort by date\n",
    "    try:\n",
    "        final_df['DateSort'] = pd.to_datetime(final_df['Date'], errors='coerce')\n",
    "        final_df = final_df.dropna(subset=['DateSort'])\n",
    "        final_df = final_df.sort_values('DateSort', ascending=False)\n",
    "        final_df = final_df.drop('DateSort', axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not sort by date due to: {e}\")\n",
    "        print(\"Problematic dates:\")\n",
    "        print(final_df['Date'].value_counts())\n",
    "\n",
    "    # Return the final DataFrame\n",
    "    return final_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Compile all responses into a DataFrame\n",
    "    df = compile_all_responses()\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"\\nError: No data was collected!\")\n",
    "    else:\n",
    "        # Display basic statistics\n",
    "        print(f\"\\nTotal number of unique laws: {len(df)}\")\n",
    "        print(\"\\nMost recent laws:\")\n",
    "        print(df.head().to_string())\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_path = \"enter file path here\"\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nDatabase saved to: {output_path}\")\n",
    "\n",
    "# 2. Add column for Year \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"enter file path here\")\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df\n",
    "\n",
    "#Excluding years prior to 2002 since we do not use forecast data for these years. We also exclude years 2018 onwards\n",
    "#from law file because we need 2 years after and we have data forecast data up to 2019\n",
    "filtered_df = df[~df['Year'].isin([1986, 1987, 1988, 1989,1990, 1991, 1992, 1993, 1994, 1995, 1996,\n",
    "                                   1997, 1998, 1999, 2000, 2001,2018, 2019, 2020, 2021, 2022, 2023])]\n",
    "\n",
    "filtered_df_with_titles = filtered_df.dropna(subset=[\"Regulatory Body\"])\n",
    "\n",
    "filtered_df_with_titles.to_csv(\"enter file path here\")\n",
    "\n",
    "# 3. Create Panel Datasets for Each Law and Each Channel\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_channel_panels(laws_file: str, panel_file: str, output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates separate panel datasets for each law and each channel marked as \"Yes\".\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read the input files\n",
    "    laws_df = pd.read_csv(laws_file)\n",
    "    panel_df = pd.read_csv(panel_file)\n",
    "    \n",
    "    # Convert Year columns to int\n",
    "    laws_df['Year'] = pd.to_numeric(laws_df['Year'])\n",
    "    panel_df['FYEAR'] = pd.to_numeric(panel_df['FYEAR'])\n",
    "    \n",
    "    # Define channels to check for \"Yes\"\n",
    "    channels = [\n",
    "        'Litigation Risk',\n",
    "        'Corporate Governance',\n",
    "        'Proprietary Costs',\n",
    "        'Information Asymmetry',\n",
    "        'Unsophisticated Investors',\n",
    "        'Equity Issuance',\n",
    "        'Reputation Risk'\n",
    "    ]\n",
    "    \n",
    "    # List of columns to bring from laws dataset\n",
    "    law_columns = [\n",
    "        'Date', 'Regulation Title', 'Regulatory Body', 'Description', \n",
    "        'Impact', 'Litigation Risk', 'Corporate Governance', \n",
    "        'Proprietary Costs', 'Information Asymmetry', \n",
    "        'Unsophisticated Investors', 'Equity Issuance', \n",
    "        'Reputation Risk', 'References', 'Year'\n",
    "    ]\n",
    "    \n",
    "    # Process each law\n",
    "    for _, law in laws_df.iterrows():\n",
    "        try:\n",
    "            # Check each channel\n",
    "            for channel in channels:\n",
    "                # Only create panel if channel is \"Yes\"\n",
    "                if str(law[channel]).strip().lower() == \"yes\":\n",
    "                    # Create a copy of the panel data\n",
    "                    law_panel = panel_df.copy()\n",
    "                    \n",
    "                    # Add law information to each row\n",
    "                    for col in law_columns:\n",
    "                        law_panel[col] = law[col]\n",
    "                    \n",
    "                    # Create treatment indicator\n",
    "                    law_panel['post_law'] = (law_panel['FYEAR'] >= law['Year']).astype(int)\n",
    "                    law_panel['treated'] = 1\n",
    "                    law_panel['treatment_effect'] = law_panel['post_law'] * law_panel['treated']\n",
    "                    \n",
    "                    # Create filename with both law and channel\n",
    "                    safe_title = law['Regulation Title'].replace('/', '_').replace('\\\\', '_')\n",
    "                    safe_title = ''.join(c for c in safe_title if c.isalnum() or c in ('_', '-'))\n",
    "                    safe_channel = channel.replace(' ', '_')\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    output_file = os.path.join(output_dir, f\"panel_{safe_title}_{safe_channel}.csv\")\n",
    "                    law_panel.to_csv(output_file, index=False)\n",
    "                    \n",
    "                    print(f\"Created panel dataset for: {law['Regulation Title']} - {channel}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing law {law['Regulation Title']}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    laws_file = \"enter file path here\"\n",
    "    panel_file = \"enter folder path here\"\n",
    "    output_dir = \"enter folder path here\"\n",
    "    \n",
    "    # Create panel datasets\n",
    "    create_channel_panels(laws_file, panel_file, output_dir)\n",
    "    \n",
    "    print(\"\\nPanel creation complete!\")\n",
    "\n",
    "# 4. Run Regression analyses and save regression tables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from fpdf import FPDF\n",
    "import traceback\n",
    "\n",
    "\n",
    "class RegressionAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize regression analyzer\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _get_significance_stars(self, pvalue: float) -> str:\n",
    "        \"\"\"Get significance stars based on p-value.\"\"\"\n",
    "        if pvalue < 0.01:\n",
    "            return \"***\"\n",
    "        elif pvalue < 0.05:\n",
    "            return \"**\"\n",
    "        elif pvalue < 0.1:\n",
    "            return \"*\"\n",
    "        return \"\"\n",
    "\n",
    "    def filter_event_window(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Filter data to ±2 years around regulation year\"\"\"\n",
    "        try:\n",
    "            regulation_year = int(df['Year'].iloc[0])\n",
    "            return df[\n",
    "                (df['FYEAR'] >= regulation_year - 2) &\n",
    "                (df['FYEAR'] <= regulation_year + 2)\n",
    "            ]\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing column in DataFrame: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error during filtering: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_regressions(self, df: pd.DataFrame) -> dict:\n",
    "        \"\"\"Run multiple regression specifications without fixed effects\"\"\"\n",
    "        results_dict = {}\n",
    "        specifications = {\n",
    "            '(1)': {\n",
    "                'dep_var': 'freqMF',\n",
    "                'controls': []\n",
    "            },\n",
    "            '(2)': {\n",
    "                'dep_var': 'freqMF',\n",
    "                'controls': ['linstown', 'lsize', 'lbtm', 'lroa', 'lsaret12', 'levol', 'lloss', 'lcalrisk']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for spec_name, spec in specifications.items():\n",
    "            print(f\"\\nRunning regression for specification {spec_name}\")\n",
    "            try:\n",
    "                dep_var = spec['dep_var']\n",
    "                controls = spec.get('controls', [])\n",
    "                variables = controls + ['treatment_effect']\n",
    "\n",
    "                # Clean data\n",
    "                print(\"Getting required columns...\")\n",
    "                required_columns = variables + [dep_var, 'GVKEY']  # Include all needed columns\n",
    "                if not all(col in df.columns for col in required_columns):\n",
    "                    missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "                    print(f\"Missing columns: {missing_cols}\")\n",
    "                    raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "            \n",
    "                reg_data = df[required_columns].copy()\n",
    "                reg_data = reg_data.replace([np.inf, -np.inf], np.nan)\n",
    "                reg_data = reg_data.dropna()\n",
    "            \n",
    "                print(f\"Observations: {len(reg_data)}\")\n",
    "                print(f\"Number of unique firms: {len(reg_data['GVKEY'].unique())}\")\n",
    "\n",
    "                # Simple matrix construction without fixed effects\n",
    "                X = add_constant(reg_data[variables])\n",
    "                y = reg_data[dep_var]\n",
    "\n",
    "                # Fit model with standard errors\n",
    "                model = OLS(y, X)\n",
    "                results = model.fit(cov_type='HC0')  # Using heteroskedasticity-robust standard errors\n",
    "\n",
    "                # Store results\n",
    "                results_dict[spec_name] = {\n",
    "                    'coefficients': results.params.to_dict(),\n",
    "                    'pvalues': results.pvalues.to_dict(),\n",
    "                    't_stats': (results.params / results.bse).to_dict(),\n",
    "                    'r_squared': results.rsquared,\n",
    "                    'n_obs': int(results.nobs),\n",
    "                    'n_firms': len(reg_data['GVKEY'].unique()),\n",
    "                    'controls': controls,\n",
    "                    'fixed_effects': {\n",
    "                    'firm': False,\n",
    "                    'industry_year': False\n",
    "                    }\n",
    "                }\n",
    "                print(f\"Successfully completed regression for specification {spec_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in specification {spec_name}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        if not results_dict:\n",
    "            raise ValueError(\"No successful regressions completed\")\n",
    "        \n",
    "        return results_dict\n",
    "\n",
    "    def save_regression_table_as_pdf(self, results: dict, regulation_title: str, output_path: str):\n",
    "        \"\"\"Save regression table as PDF matching the target format\"\"\"\n",
    "        try:\n",
    "            pdf = FPDF(format='A4', orientation='P')  # Changed to portrait and A4\n",
    "            pdf.set_margins(30, 20, 30)\n",
    "            pdf.add_page()\n",
    "            pdf.set_font('Times', size=12)  # Changed to Times\n",
    "        \n",
    "        \n",
    "            # Title - all in bold Times New Roman\n",
    "            try:\n",
    "                # First try Windows standard folder for Times New Roman\n",
    "                pdf.add_font('Times New Roman', '', r'C:\\Windows\\Fonts\\times.ttf', uni=True)\n",
    "                pdf.add_font('Times New Roman', 'B', r'C:\\Windows\\Fonts\\timesbd.ttf', uni=True)\n",
    "                pdf.set_font('Times New Roman', 'B', 12)  # Set bold font first\n",
    "            except:\n",
    "                try:\n",
    "                    # Try alternative paths for Times New Roman\n",
    "                    pdf.add_font('Times New Roman', '', 'times.ttf', uni=True)\n",
    "                    pdf.add_font('Times New Roman', 'B', 'timesbd.ttf', uni=True)\n",
    "                    pdf.set_font('Times New Roman', 'B', 12)\n",
    "                except:\n",
    "                    print(\"Times New Roman font not found, using Arial Bold\")\n",
    "                    pdf.set_font('Arial', 'B', 12)\n",
    "\n",
    "            # Both title and table number in bold\n",
    "            pdf.cell(0, 10, \"Table 3\", ln=True, align='C')\n",
    "            pdf.cell(0, 10, f\"The Impact of {regulation_title} on Management Forecast Frequency\", ln=True, align='C')\n",
    "            pdf.ln(5)\n",
    "\n",
    "            # Switch back to regular font for table content\n",
    "            try:\n",
    "                pdf.set_font('Times New Roman', '', 12)\n",
    "            except:\n",
    "                pdf.set_font('Arial', '', 12)\n",
    "            \n",
    "\n",
    "            # Calculate column widths for 3 columns\n",
    "            col_width = (pdf.w - 60) / 3  # -60 for margins and first column\n",
    "            first_col_width = 60\n",
    "\n",
    "            # Table header\n",
    "            pdf.cell(first_col_width, 8, \"\", 1)\n",
    "            for i in range(1, 3):  # Only 2 specifications now\n",
    "                pdf.cell(col_width, 8, f\"({i})\", 1, align='C')\n",
    "            pdf.ln()\n",
    "\n",
    "            # Treatment Effect\n",
    "            pdf.cell(first_col_width, 8, \"Treatment Effect\", 1)\n",
    "            for i in range(1, 3):\n",
    "                spec = f'({i})'\n",
    "                coef = results[spec]['coefficients']['treatment_effect']\n",
    "                tstat = abs(results[spec]['t_stats']['treatment_effect'])\n",
    "                stars = self._get_significance_stars(results[spec]['pvalues']['treatment_effect'])\n",
    "                pdf.cell(col_width, 8, f\"{coef:.4f}{stars} ({tstat:.2f})\", 1, align='C')\n",
    "            pdf.ln()\n",
    "\n",
    "            # Control variables for specification (2)\n",
    "            control_labels = {\n",
    "                'linstown': 'Institutional ownership',\n",
    "                'lsize': 'Firm size',\n",
    "                'lbtm': 'Book-to-market',\n",
    "                'lroa': 'ROA',\n",
    "                'lsaret12': 'Stock return',\n",
    "                'levol': 'Earnings volatility',\n",
    "                'lloss': 'Loss',\n",
    "                'lcalrisk': 'Class action litigation risk'\n",
    "            }\n",
    "\n",
    "            for var, label in control_labels.items():\n",
    "                pdf.cell(first_col_width, 8, label, 1)\n",
    "                for i in range(1, 3):\n",
    "                    spec = f'({i})'\n",
    "                    if var in results[spec]['coefficients']:\n",
    "                        coef = results[spec]['coefficients'][var]\n",
    "                        tstat = abs(results[spec]['t_stats'][var])\n",
    "                        stars = self._get_significance_stars(results[spec]['pvalues'][var])\n",
    "                        pdf.cell(col_width, 8, f\"{coef:.4f}{stars} ({tstat:.2f})\", 1, align='C')\n",
    "                    else:\n",
    "                        pdf.cell(col_width, 8, \"\", 1, align='C')\n",
    "                pdf.ln()\n",
    "\n",
    "\n",
    "            # N and R²\n",
    "            for stat in ['N', 'R²']:\n",
    "                pdf.cell(first_col_width, 8, stat, 1)\n",
    "                for i in range(1, 3):\n",
    "                    spec = f'({i})'\n",
    "                    value = results[spec]['n_obs'] if stat == 'N' else results[spec]['r_squared']\n",
    "                    text = f\"{value:,}\" if stat == 'N' else f\"{value:.4f}\"\n",
    "                    pdf.cell(col_width, 8, text, 1, align='C')\n",
    "                pdf.ln()\n",
    "\n",
    "            # Notes\n",
    "            pdf.ln(10)\n",
    "            pdf.set_font('Times', size=10)\n",
    "            notes = \"Notes: t-statistics in parentheses. *, **, and *** represent significance at the 10%, 5%, and 1% level, respectively.\"\n",
    "            pdf.multi_cell(0, 5, notes)\n",
    "\n",
    "            pdf.output(output_path)\n",
    "            print(f\"PDF saved at {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving PDF: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def analyze_panel(self, panel_file: str, output_dir: str):\n",
    "        \"\"\"Analyze a single panel dataset\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nStarting analysis of {os.path.basename(panel_file)}...\")\n",
    "            print(\"Reading data...\")\n",
    "            df = pd.read_csv(panel_file)\n",
    "            \n",
    "            print(\"Filtering event window...\")\n",
    "            df_filtered = self.filter_event_window(df)\n",
    "            \n",
    "            print(\"Running regressions...\")\n",
    "            results = self.run_regressions(df_filtered)\n",
    "            \n",
    "            print(\"Saving results...\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Save filtered data and results\n",
    "            print(\"Saving filtered data...\")\n",
    "            df_filtered.to_csv(os.path.join(output_dir, 'filtered_data.csv'), index=False)\n",
    "            with open(os.path.join(output_dir, 'regression_results.json'), 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            \n",
    "            # Save regression table\n",
    "            table_path = os.path.join(output_dir, 'regression_table.pdf')\n",
    "            self.save_regression_table_as_pdf(\n",
    "                results,\n",
    "                df['Regulation Title'].iloc[0],\n",
    "                table_path\n",
    "            )\n",
    "            print(f\"Saved regression table to {table_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing panel: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "def analyze_all_panels(input_dir: str, output_dir: str):\n",
    "    \"\"\"Analyze all panel datasets in a directory\"\"\"\n",
    "    analyzer = RegressionAnalyzer()\n",
    "    panel_files = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "    total_files = len(panel_files)\n",
    "    successful_runs = 0\n",
    "    failed_runs = 0\n",
    "    print(f\"\\nFound {total_files} panel files to analyze\")\n",
    "    \n",
    "    for i, panel_file in enumerate(panel_files, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing panel {i} of {total_files}: {os.path.basename(panel_file)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            analyzer.analyze_panel(panel_file, os.path.join(output_dir, os.path.basename(panel_file).replace('.csv', '')))\n",
    "            successful_runs += 1\n",
    "            print(f\"Successfully processed panel {i}\")\n",
    "        except Exception as e:\n",
    "            failed_runs += 1\n",
    "            print(f\"Failed to process panel {i}: {str(e)}\")\n",
    "            \n",
    "        # Print progress summary\n",
    "        print(f\"\\nProgress Summary:\")\n",
    "        print(f\"Processed: {i}/{total_files} ({(i/total_files)*100:.1f}%)\")\n",
    "        print(f\"Successful: {successful_runs}\")\n",
    "        print(f\"Failed: {failed_runs}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    INPUT_DIR = \"enter folder path here\"\n",
    "    OUTPUT_DIR = \"enter folder path here\"\n",
    "    \n",
    "    # Run analysis on all panels\n",
    "    analyze_all_panels(INPUT_DIR, OUTPUT_DIR)\n",
    "\n",
    "# 5. Check and keep significant results \n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def process_significance(base_dir, delete_nonsig=False):\n",
    "    \"\"\"\n",
    "    Process panels based on significance and either delete or move non-significant results.\n",
    "    t-stat >= 1.96 is considered significant.\n",
    "    \"\"\"\n",
    "    # Create directory for non-significant results if not deleting\n",
    "    if not delete_nonsig:\n",
    "        nonsig_dir = os.path.join(os.path.dirname(base_dir), 'nonsignificant_results')\n",
    "        os.makedirs(nonsig_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all panel directories\n",
    "    panel_dirs = glob.glob(os.path.join(base_dir, 'panel_*'))\n",
    "    print(f\"Found {len(panel_dirs)} panel directories\")\n",
    "    \n",
    "    # Track results\n",
    "    significant_count = 0\n",
    "    not_significant_count = 0\n",
    "    \n",
    "    # Process each panel\n",
    "    for panel_dir in panel_dirs:\n",
    "        panel_name = os.path.basename(panel_dir)\n",
    "        json_file = os.path.join(panel_dir, 'regression_results.json')\n",
    "        \n",
    "        try:\n",
    "            # Read regression results\n",
    "            with open(json_file, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Check specification (2)\n",
    "            if '(2)' in results:\n",
    "                t_stat = abs(results['(2)']['t_stats']['treatment_effect'])\n",
    "                is_significant = t_stat >= 1.96\n",
    "                \n",
    "                if is_significant:\n",
    "                    print(f\"{panel_name}: t-stat = {t_stat:.2f} (significant - keeping)\")\n",
    "                    significant_count += 1\n",
    "                else:\n",
    "                    print(f\"{panel_name}: t-stat = {t_stat:.2f} (not significant - {'deleting' if delete_nonsig else 'moving'})\")\n",
    "                    if delete_nonsig:\n",
    "                        shutil.rmtree(panel_dir)\n",
    "                    else:\n",
    "                        shutil.move(panel_dir, os.path.join(nonsig_dir, panel_name))\n",
    "                    not_significant_count += 1\n",
    "            else:\n",
    "                print(f\"{panel_name}: No specification (2) found - {'deleting' if delete_nonsig else 'moving'}\")\n",
    "                if delete_nonsig:\n",
    "                    shutil.rmtree(panel_dir)\n",
    "                else:\n",
    "                    shutil.move(panel_dir, os.path.join(nonsig_dir, panel_name))\n",
    "                not_significant_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {panel_name}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    total_processed = significant_count + not_significant_count\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Total panels processed: {total_processed}\")\n",
    "    print(f\"Significant results: {significant_count} ({(significant_count/total_processed)*100:.1f}%)\")\n",
    "    print(f\"Not significant results: {not_significant_count} ({(not_significant_count/total_processed)*100:.1f}%)\")\n",
    "    if not delete_nonsig:\n",
    "        print(f\"\\nNon-significant results moved to: {nonsig_dir}\")\n",
    "    else:\n",
    "        print(\"\\nNon-significant results deleted\")\n",
    "\n",
    "\n",
    "base_dir = \"enter folder path here\"\n",
    "\n",
    "\n",
    "delete_nonsig = False  \n",
    "process_significance(base_dir, delete_nonsig)\n",
    "\n",
    "# 6. Ask Claude to write a background, theoretical framework, and hypothesis development section\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "import glob\n",
    "\n",
    "class ComprehensiveAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    def get_laws_analysis(self, csv_file: str) -> list:\n",
    "        \"\"\"Read and analyze laws from CSV file\"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "        laws = []\n",
    "        \n",
    "        # Define economic mechanisms\n",
    "        mechanisms = [\n",
    "            'Litigation Risk', \n",
    "            'Corporate Governance', \n",
    "            'Proprietary Costs', \n",
    "            'Information Asymmetry',\n",
    "            'Unsophisticated Investors', \n",
    "            'Equity Issuance', \n",
    "            'Reputation Risk'\n",
    "        ]\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Get active mechanisms (where value is 'Yes')\n",
    "            active_mechanisms = [mech for mech in mechanisms if row[mech] == 'Yes']\n",
    "            \n",
    "            law = {\n",
    "                'title': row['Regulation Title'],\n",
    "                'year': row['Year'],\n",
    "                'body': row['Regulatory Body'],\n",
    "                'description': row['Description'],\n",
    "                'impact': row['Impact'],\n",
    "                'mechanisms': active_mechanisms\n",
    "            }\n",
    "            laws.append(law)\n",
    "        \n",
    "        return laws\n",
    "\n",
    "    def get_background_hypothesis(self, law: dict, mechanism: str) -> str:\n",
    "        \"\"\"Get background, theoretical framework, and hypothesis development for a law and specific mechanism\"\"\"\n",
    "        prompt = f\"\"\"You are an accounting academic writing a research paper examining {law['title']} and its impact on \n",
    "        voluntary disclosure through the {mechanism} channel.\n",
    "Please write the background, theoretical framework, and hypothesis development section following these guidelines:\n",
    "\n",
    "Law Details:\n",
    "Title: {law['title']} ({law['year']})\n",
    "Regulatory Body: {law['body']}\n",
    "Description: {law['description']}\n",
    "Impact: {law['impact']}\n",
    "Economic Mechanism: {mechanism}\n",
    "\n",
    "Please structure your response as follows:\n",
    "\n",
    "1. Background (3 paragraphs, ~400 words total):\n",
    "    - Label this subsection \"Background\"\n",
    "    - Describe the relevant U.S. federal securities law in  {law['title']}\n",
    "    -Include the date that the change is effective ({law['year']}), which firms are affected, and why the change was instituted.  \n",
    "    -Discuss the effective date ({law['year']}) and implementation details\n",
    "    -Please also discuss whether there were other contemporaneous securities law adoptions. \n",
    "    -Support each claim with citations to foundational papers \n",
    "\n",
    "2. Theoretical Framework\n",
    "    - Begin with a brief introduction connecting the law to the relevant theoretical perspective {mechanism}\n",
    "    - Explain core concepts of {mechanism}\n",
    "    - Connect to voluntary disclosure decisions\n",
    "    - Link to the specific {mechanism} being studied\n",
    "    - Support with 2-3 seminal citations\n",
    "\n",
    "3. Hypothesis Development (3 paragraphs, ~800 words total):\n",
    "    - Label this subsection \"Hypothesis Development\"\n",
    "    - Present economic mechanisms linking {law['title']} to voluntary disclosure decisions through the {mechanism} channel\n",
    "    - Draw on established theoretical frameworks specifically related to {mechanism}\n",
    "    - Propose a theoretically supported hypothesis about the relationship between the U.S. federal \n",
    "    securities law from file {law['title']} and voluntary disclosure for the specific {mechanism} channel\n",
    "    - Build logical arguments step by step think through whether prior literature suggests competing theoretical \n",
    "    predictions or if the literature suggests only one direction for the relationship. \n",
    "    - Present the formal hypothesis statement on its own line, clearly labeled \"H1:\"\n",
    "    - Support each claim with citations to foundational papers \n",
    "\n",
    "Writing Guidelines:\n",
    "- Use active voice (e.g., \"We examine\" instead of \"This paper examines\")\n",
    "- Maintain formal academic tone suitable for a top journal\n",
    "- Include 2-3 citations per paragraph \n",
    "- Use present tense for established findings\n",
    "- Make clear distinctions between correlation and causation\n",
    "- Cite papers from top accounting and finance journals such as:\n",
    "    The Accounting Review, Journal of Accounting Research, \n",
    "    Journal of Accounting and Economics, Contemporary Accounting Research, Accounting, Organizations, and Society,\n",
    "    and Review of Financial Studies\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting background and hypothesis: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def create_background_hypothesis_files(self, csv_file: str, output_dir: str):\n",
    "        \"\"\"Generate and save background and hypothesis sections for all laws\"\"\"\n",
    "        # Create main directory\n",
    "        main_dir = os.path.join(output_dir, 'background and hypothesis development')\n",
    "        os.makedirs(main_dir, exist_ok=True)\n",
    "        \n",
    "        # Get laws\n",
    "        laws = self.get_laws_analysis(csv_file)\n",
    "        \n",
    "        # Process each law and mechanism\n",
    "        for law in laws:\n",
    "            print(f\"\\nProcessing law: {law['title']}\")\n",
    "            \n",
    "            # Generate separate background and hypothesis for each mechanism\n",
    "            for mechanism in law['mechanisms']:\n",
    "                print(f\"Processing mechanism: {mechanism}\")\n",
    "                \n",
    "                # Create filename with both law and mechanism\n",
    "                clean_mechanism = mechanism.replace(' ', '_')\n",
    "                filename = f\"{law['title'].replace(' ', '_')}_{clean_mechanism}_background_hypothesis.txt\"\n",
    "                file_path = os.path.join(main_dir, filename)\n",
    "            \n",
    "                # Check if file already exists\n",
    "                if os.path.exists(file_path):\n",
    "                    print(f\"Skipping {law['title']} - {mechanism}: File already exists\")\n",
    "                    continue\n",
    "                \n",
    "                # Get background and hypothesis content\n",
    "                content = self.get_background_hypothesis(law, mechanism)\n",
    "            \n",
    "                \n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "                print(f\"Saved background and hypothesis for {law['title']} - {mechanism}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"\n",
    "    CSV_FILE = \"enter file path here\"\n",
    "    OUTPUT_DIR = \"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        analyzer = ComprehensiveAnalyzer(API_KEY)\n",
    "        analyzer.create_background_hypothesis_files(CSV_FILE, OUTPUT_DIR)\n",
    "        print(\"\\nBackground and hypothesis development sections complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# 7. Send regression results to Claude for interpretation\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from typing import Dict, List\n",
    "from anthropic import Anthropic\n",
    "\n",
    "class RegressionInterpreter:\n",
    "    def __init__(self, input_dir: str, output_dir: str, api_key: str):\n",
    "        \"\"\"Initialize interpreter with input and output directories\"\"\"\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        \n",
    "    def _get_significance_stars(self, pvalue: float) -> str:\n",
    "        \"\"\"Get significance stars based on p-value.\"\"\"\n",
    "        if pvalue < 0.01:\n",
    "            return \"***\"\n",
    "        elif pvalue < 0.05:\n",
    "            return \"**\"\n",
    "        elif pvalue < 0.1:\n",
    "            return \"*\"\n",
    "        return \"\"\n",
    "    \n",
    "    def _get_significance_level(self, pvalue: float) -> str:\n",
    "        \"\"\"Convert p-value to significance level description\"\"\"\n",
    "        if pvalue < 0.01:\n",
    "            return \"at the 1% level\"\n",
    "        elif pvalue < 0.05:\n",
    "            return \"at the 5% level\"\n",
    "        elif pvalue < 0.1:\n",
    "            return \"at the 10% level\"\n",
    "        return \"not statistically significant\"\n",
    "\n",
    "    def read_regression_results(self, regulation_name: str) -> Dict:\n",
    "        \"\"\"Read regression results JSON file for a specific regulation\"\"\"\n",
    "        results_path = os.path.join(self.output_dir, regulation_name, 'regression_results.json')\n",
    "        \n",
    "        try:\n",
    "            with open(results_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No results file found for {regulation_name}\")\n",
    "            return {}\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error reading results file for {regulation_name}\")\n",
    "            return {}\n",
    "\n",
    "    def read_hypothesis(self, regulation_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Read hypothesis file with flexible matching across various filename formats.\n",
    "    \n",
    "        Args:\n",
    "            regulation_name (str): Name of the regulation to find a hypothesis for\n",
    "    \n",
    "        Returns:\n",
    "            str: Hypothesis text or empty string if no file found\n",
    "        \"\"\"\n",
    "        # Remove 'panel_' prefix if present\n",
    "        if regulation_name.startswith('panel_'):\n",
    "            regulation_name = regulation_name[6:]  # Remove 'panel_' prefix\n",
    "    \n",
    "        # Set up hypothesis directory\n",
    "        hypothesis_dir = os.path.join(os.path.dirname(self.output_dir), \n",
    "                                'background and hypothesis development')\n",
    "    \n",
    "        # Print debugging information\n",
    "        print(f\"Searching for hypothesis file for: {regulation_name}\")\n",
    "        print(f\"Hypothesis directory: {hypothesis_dir}\")\n",
    "    \n",
    "        # Prepare search terms\n",
    "        search_terms = [\n",
    "            # Original input\n",
    "            regulation_name,\n",
    "            # Remove underscores\n",
    "            regulation_name.replace('_', ''),\n",
    "            # Replace underscores with spaces\n",
    "            regulation_name.replace('_', ' '),\n",
    "        ]\n",
    "    \n",
    "        # Try to list files for debugging\n",
    "        try:\n",
    "            all_files = os.listdir(hypothesis_dir)\n",
    "            print(\"Files in directory:\")\n",
    "            for file in all_files:\n",
    "                print(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing directory: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "        # Find matching files\n",
    "        matching_files = [\n",
    "            file for file in all_files\n",
    "            if (any(term.lower() in file.lower() for term in search_terms) and \n",
    "                file.lower().endswith('_background_hypothesis.txt'))\n",
    "        ]\n",
    "    \n",
    "        # If no matches found, try more relaxed matching\n",
    "        if not matching_files:\n",
    "            matching_files = [\n",
    "                file for file in all_files\n",
    "                if any(\n",
    "                    term.lower() in file.lower().replace('_', '').replace(' ', '') \n",
    "                    for term in search_terms\n",
    "                ) and file.lower().endswith('_background_hypothesis.txt')\n",
    "            ]\n",
    "    \n",
    "        # Process the first matching file\n",
    "        for filename in matching_files:\n",
    "            hypothesis_file = os.path.join(hypothesis_dir, filename)\n",
    "        \n",
    "            print(f\"Found matching file: {filename}\")\n",
    "            print(f\"Full path: {hypothesis_file}\")\n",
    "        \n",
    "            try:\n",
    "                with open(hypothesis_file, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "            \n",
    "                # Extract hypothesis development section\n",
    "                if \"Hypothesis Development\" in content:\n",
    "                    # Split by \"Hypothesis Development\" and take everything after it\n",
    "                    hypothesis_section = content.split(\"Hypothesis Development\")[1]\n",
    "            \n",
    "                    # If there's a formal H1 statement, include everything up to that\n",
    "                    if \"H1:\" in hypothesis_section:\n",
    "                        hypothesis_development = hypothesis_section.split(\"H1:\")[0].strip()\n",
    "                        h1_statement = \"H1:\" + hypothesis_section.split(\"H1:\")[1].strip()\n",
    "                        return f\"Hypothesis Development:\\n\\n{hypothesis_development}\\n\\n{h1_statement}\"\n",
    "                    else:\n",
    "                        return f\"Hypothesis Development:\\n\\n{hypothesis_section.strip()}\"\n",
    "                else:\n",
    "                    print(f\"No Hypothesis Development section found in {filename}\")\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading hypothesis file {filename}: {str(e)}\")\n",
    "    \n",
    "        print(f\"No hypothesis file found for {regulation_name}\")\n",
    "        return \"\"\n",
    "\n",
    "    def format_results_text(self, regulation_title: str, regulation_year: int, results: Dict) -> str:\n",
    "        \"\"\"Format regression results into text for the academic prompt\"\"\"\n",
    "        results_text = f\"Regression Analysis for {regulation_title} (Year: {regulation_year})\\n\\n\"\n",
    "        \n",
    "        for spec_name, res in results.items():\n",
    "            results_text += f\"\\nSpecification {spec_name}:\\n\"\n",
    "            results_text += f\"Treatment Effect: {res['coefficients']['treatment_effect']:.4f}\\n\"\n",
    "            results_text += f\"T-statistic: {res['t_stats']['treatment_effect']:.2f}\\n\"\n",
    "            results_text += f\"P-value: {res['pvalues']['treatment_effect']:.4f}\\n\"\n",
    "            results_text += f\"R-squared: {res['r_squared']:.4f}\\n\"\n",
    "            results_text += f\"Number of observations: {int(res['n_obs'])}\\n\"\n",
    "            results_text += f\"Number of firms: {res['n_firms']}\\n\"\n",
    "            \n",
    "            if res['controls']:\n",
    "                results_text += \"\\nControl Variables:\\n\"\n",
    "                for control in res['controls']:\n",
    "                    coef = res['coefficients'][control]\n",
    "                    tstat = res['t_stats'][control]\n",
    "                    pvalue = res['pvalues'][control]\n",
    "                    stars = self._get_significance_stars(pvalue)\n",
    "                    results_text += f\"{control}: {coef:.4f}{stars} (t={tstat:.2f}, p={pvalue:.4f})\\n\"\n",
    "            \n",
    "            results_text += \"\\nFixed Effects:\\n\"\n",
    "            for fe, included in res['fixed_effects'].items():\n",
    "                results_text += f\"{fe}: {'Yes' if included else 'No'}\\n\"\n",
    "            \n",
    "            results_text += \"-\" * 50 + \"\\n\"\n",
    "        \n",
    "        return results_text\n",
    "\n",
    "    def generate_claude_interpretation(self, regulation_title: str, regulation_year: int, results_text: str, hypothesis_text: str) -> str:\n",
    "        \"\"\"Generate interpretation using Claude API\"\"\"\n",
    "        prompt = f\"\"\"You are an accounting academic with a PhD in accounting. \n",
    "        You should use active voice (e.g. \"We find\" instead of \"It is found\"). \n",
    "        Use present tense for all established findings. \n",
    "        Distinguish between correlation and causation. \n",
    "        Write the results description for this analysis as if you were writing an academic paper for an accounting journal, \n",
    "        you are studying the association between a change in mandatory disclosure and voluntary disclosure. \n",
    "        \n",
    "        Here is the hypothesis that was developed:\n",
    "        {hypothesis_text}\n",
    "        \n",
    "        Please provide a detailed academic analysis of these regression results:\n",
    "\n",
    "{results_text}\n",
    "\n",
    "Please structure your analysis as follows (3 paragraphs, ~600 words total):\n",
    "1. Label this section Regression Analysis\n",
    "2. Main finding (treatment effect interpretation)\n",
    "3. Statistical significance and economic magnitude\n",
    "4. Model specification comparison\n",
    "5. Control variable effects\n",
    "   Describe whether the relationship is consistent with prior literature\n",
    "6. Explain whether the results support the hypothesis stated in the Hypothesis section above\n",
    "\n",
    "Write in an academic style suitable for a top accounting journal.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting Claude interpretation: {str(e)}\")\n",
    "            return f\"Error in Claude analysis: {str(e)}\"\n",
    "\n",
    "    def interpret_regulation_impact(self, regulation_name: str) -> str:\n",
    "        \"\"\"Generate interpretation for a single regulation's results\"\"\"\n",
    "        results = self.read_regression_results(regulation_name)\n",
    "        \n",
    "        # Read the original panel file to get regulation title\n",
    "        panel_file = os.path.join(self.input_dir, f\"{regulation_name}.csv\")\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = pd.read_csv(panel_file)\n",
    "            regulation_title = df['Regulation Title'].iloc[0]\n",
    "            regulation_year = df['Year'].iloc[0]\n",
    "        except:\n",
    "            regulation_title = regulation_name\n",
    "            regulation_year = \"N/A\"\n",
    "        \n",
    "        # Format results text\n",
    "        results_text = self.format_results_text(regulation_title, regulation_year, results)\n",
    "        \n",
    "        # Read hypothesis text\n",
    "        hypothesis_text = self.read_hypothesis(regulation_name)\n",
    "        \n",
    "        # Generate interpretation using Claude\n",
    "        interpretation = self.generate_claude_interpretation(\n",
    "            regulation_title, \n",
    "            regulation_year, \n",
    "            results_text,\n",
    "            hypothesis_text\n",
    "        )\n",
    "        \n",
    "        # Create subfolder if it doesn't exist\n",
    "        regulation_dir = os.path.join(self.output_dir, regulation_name)\n",
    "        os.makedirs(regulation_dir, exist_ok=True)\n",
    "        \n",
    "        # Save interpretation to file\n",
    "        claude_path = os.path.join(regulation_dir, 'claude_interpretation.txt')\n",
    "        try:\n",
    "            with open(claude_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(interpretation)\n",
    "            print(f\"Saved interpretation to {claude_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving interpretation to file: {str(e)}\")\n",
    "        \n",
    "        return interpretation\n",
    "\n",
    "    def analyze_all_regulations(self) -> None:\n",
    "        \"\"\"Analyze results for all regulations in the directory\"\"\"\n",
    "        panel_files = glob.glob(os.path.join(self.input_dir, \"panel_*_*.csv\"))\n",
    "        \n",
    "        for panel_file in panel_files:\n",
    "            regulation_name = os.path.splitext(os.path.basename(panel_file))[0]\n",
    "            try:\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(self.interpret_regulation_impact(regulation_name))\n",
    "                print(\"=\"*80 + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing {regulation_name}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"  \n",
    "    BASE_DIR = \"enter folder path here\"\n",
    "    INPUT_DIR = os.path.join(BASE_DIR, \"law_panels\")\n",
    "    OUTPUT_DIR = os.path.join(BASE_DIR, \"regression_analyses\")\n",
    "    \n",
    "    interpreter = RegressionInterpreter(INPUT_DIR, OUTPUT_DIR, API_KEY)\n",
    "    interpreter.analyze_all_regulations()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "## 8. Create Correlation tables\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_CENTER, TA_LEFT \n",
    "from scipy import stats\n",
    "from reportlab.lib.pagesizes import letter, landscape\n",
    "\n",
    "def create_correlation_table(data_path, output_dir):\n",
    "    \"\"\"\n",
    "    Creates a clean correlation table PDF in the style of academic papers.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the panel data CSV\n",
    "        output_dir (str): Path to save output files\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Select numerical variables for correlation\n",
    "    numeric_vars = ['treatment_effect','freqMF','linstown', 'lsize', 'lbtm', 'lroa', 'lsaret12', 'levol', 'lloss', \n",
    "                    'lcalrisk']\n",
    "                   \n",
    "    # Create shorter variable names for the table\n",
    "    var_mapping = {\n",
    "        'treatment_effect': 'Treatment Effect',\n",
    "        'freqMF': 'FreqMF',\n",
    "        'linstown': 'Institutional ownership',\n",
    "        'lsize': 'Firm size',\n",
    "        'lbtm': 'Book-to-market',\n",
    "        'lroa': 'ROA',\n",
    "        'lsaret12': 'Stock return',\n",
    "        'levol': 'Earnings volatility',\n",
    "        'lloss': 'Loss',\n",
    "        'lcalrisk': 'Class action litigation risk'\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df[numeric_vars].corr()\n",
    "    \n",
    "    # Calculate p-values for significance testing\n",
    "    def calculate_pvalue(x, y):\n",
    "        return stats.pearsonr(x.dropna(), y.dropna())[1]\n",
    "    \n",
    "    p_values = pd.DataFrame(index=numeric_vars, columns=numeric_vars)\n",
    "    for i in numeric_vars:\n",
    "        for j in numeric_vars:\n",
    "            p_values.loc[i,j] = calculate_pvalue(df[i], df[j])\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get panel name from path\n",
    "    panel_name = os.path.basename(os.path.dirname(data_path))\n",
    "    \n",
    "    # Create PDF\n",
    "    clean_name = panel_name.replace('panel_', '')\n",
    "    pdf_path = os.path.join(output_dir, f'{clean_name}_correlation_table.pdf')\n",
    "    doc = SimpleDocTemplate(pdf_path, pagesize=landscape(letter), rightMargin=30, leftMargin=30, topMargin=50, bottomMargin=50)\n",
    "    \n",
    "    # Prepare table data\n",
    "    table_data = [['']]  # First cell empty\n",
    "    \n",
    "    # Add column headers\n",
    "    for var in numeric_vars:\n",
    "        table_data[0].append(var_mapping[var])\n",
    "    \n",
    "    # Add rows\n",
    "    for i, var1 in enumerate(numeric_vars, 1):\n",
    "        row = [var_mapping[var1]]  # Row header\n",
    "        for var2 in numeric_vars:\n",
    "            if var1 == var2:\n",
    "                row.append('1.00')\n",
    "            else:\n",
    "                value = corr_matrix.loc[var1, var2]\n",
    "                # Format to 2 decimal places\n",
    "                formatted_value = f'{value:.2f}'\n",
    "                row.append(formatted_value)\n",
    "        table_data.append(row)\n",
    "    \n",
    "    # Create table style\n",
    "    style = [\n",
    "        ('FONTNAME', (0,0), (-1,-1), 'Times-Roman'),\n",
    "        ('FONTSIZE', (0,0), (-1,-1), 8),\n",
    "        ('ALIGN', (0,0), (-1,-1), 'CENTER'),\n",
    "        ('TOPPADDING', (0,0), (-1,-1), 3),\n",
    "        ('BOTTOMPADDING', (0,0), (-1,-1), 3),\n",
    "        ('GRID', (0,0), (-1,-1), 0.25, colors.black),  # Lighter grid lines\n",
    "        ('BOX', (0,0), (-1,-1), 0.25, colors.black),\n",
    "        # Make column headers and row headers bold\n",
    "        ('FONTNAME', (0,0), (-1,0), 'Times-Bold'),\n",
    "        ('FONTNAME', (0,0), (0,-1), 'Times-Bold'),\n",
    "    ]\n",
    "    \n",
    "    # Add bold style for significant correlations\n",
    "    for i in range(1, len(table_data)):\n",
    "        for j in range(1, len(table_data[0])):\n",
    "            if i != j:  # Skip diagonal\n",
    "                var1 = numeric_vars[i-1]\n",
    "                var2 = numeric_vars[j-1]\n",
    "                if p_values.loc[var1,var2] < 0.05:  # 5% significance level\n",
    "                    style.append(('FONTNAME', (j,i), (j,i), 'Times-Bold'))\n",
    "    \n",
    "    # Create table\n",
    "    table = Table(table_data)\n",
    "    table.setStyle(TableStyle(style))\n",
    "    \n",
    "    # Create title\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=12,\n",
    "        alignment=TA_CENTER,\n",
    "        spaceBefore=12,\n",
    "        spaceAfter=20,\n",
    "        fontName='Times-Bold'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Create Panel title in smaller text if needed\n",
    "    panel_title = \"\"\n",
    "    if panel_name:\n",
    "        clean_panel_name = panel_name.replace('panel_', '').replace('_', ' ')\n",
    "        # Format the law name with proper spacing\n",
    "        if 'NominatingCommitteeRequirements' in clean_panel_name:\n",
    "            law_name = 'Nominating Committee Requirements'\n",
    "        elif 'ResourceExtractionDisclosureRules' in clean_panel_name:\n",
    "            law_name = 'Resource Extraction Disclosure Rules'\n",
    "        elif 'PayRatioDisclosureRule' in clean_panel_name:\n",
    "            law_name = 'Pay Ratio Disclosure Rule'\n",
    "        else:\n",
    "            law_name = clean_panel_name\n",
    "    \n",
    "        panel_title = f\"<br/>{law_name}\"\n",
    "    \n",
    "    title = Paragraph(f\"Table 2<br/>Pearson Correlations{panel_title}\", title_style)\n",
    "    \n",
    "    # Add footnote\n",
    "    footnote_style = ParagraphStyle(\n",
    "        'Footnote',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=8,\n",
    "        alignment=TA_LEFT,\n",
    "        fontName='Times-Roman',\n",
    "        spaceBefore=6,\n",
    "        leading=10  # Controls line spacing\n",
    "    )\n",
    "    footnote = Paragraph(\"This table shows the Pearson correlations for the sample. \"\n",
    "                        \"Correlations that are significant at the 0.05 level or better are highlighted in bold. \", footnote_style)\n",
    "    \n",
    "    # Build PDF\n",
    "    doc.build([title, table, Spacer(1, 12), footnote])\n",
    "    \n",
    "    print(f\"Created correlation table PDF for {panel_name}\")\n",
    "    return pdf_path\n",
    "\n",
    "def batch_process_panels(base_dir,output_base_dir):\n",
    "    \"\"\"\n",
    "    Process all panel folders and create correlation tables, skipping existing ones.\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): Base directory containing panel folders\n",
    "        output_base_dir (str): Base directory where correlations folder should be created\n",
    "    \"\"\"\n",
    "    print(f\"Starting to process panels in: {base_dir}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = os.path.join(\"enter folder path here\", \"correlations\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "    \n",
    "    # Count total panels\n",
    "    panel_folders = [f for f in os.listdir(base_dir) if f.startswith('panel_')]\n",
    "    total_panels = len(panel_folders)\n",
    "    processed = 0\n",
    "    skipped = 0\n",
    "    errors = 0\n",
    "    \n",
    "    print(f\"\\nFound {total_panels} panel folders to process\")\n",
    "    \n",
    "    # Process each panel folder\n",
    "    for i, panel_folder in enumerate(panel_folders, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing panel {i} of {total_panels}: {panel_folder}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        panel_path = os.path.join(base_dir, panel_folder)\n",
    "        \n",
    "        # Check if correlation table already exists\n",
    "        clean_name = panel_folder.replace('panel_', '')\n",
    "        existing_table = os.path.join(output_dir, f'{clean_name}_correlation_table.pdf')\n",
    "        \n",
    "        if os.path.exists(existing_table):\n",
    "            print(f\"Skipping {panel_folder}: Correlation table already exists\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        # Look for the data file\n",
    "        data_file = 'filtered_data.csv'\n",
    "        data_path = os.path.join(panel_path, data_file)\n",
    "            \n",
    "        if os.path.exists(data_path):\n",
    "            try:\n",
    "                table_path = create_correlation_table(data_path, output_dir)\n",
    "                print(f\"Created correlation table for {panel_folder}\")\n",
    "                print(f\"Table saved to: {table_path}\")\n",
    "                processed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {panel_folder}: {str(e)}\")\n",
    "                errors += 1\n",
    "        else:\n",
    "            print(f\"No data file found in {panel_folder}\")\n",
    "            errors += 1\n",
    "        \n",
    "        # Print progress summary\n",
    "        print(f\"\\nProgress Summary:\")\n",
    "        print(f\"Processed: {i}/{total_panels} ({(i/total_panels)*100:.1f}%)\")\n",
    "        print(f\"Successfully created: {processed}\")\n",
    "        print(f\"Skipped (already exist): {skipped}\")\n",
    "        print(f\"Errors: {errors}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Base directory containing panel folders\n",
    "    BASE_DIR = \"enter folder path here\"\n",
    "    OUTPUT_BASE_DIR = \"enter folder path here\"\n",
    "    \n",
    "    # Process all panels\n",
    "    batch_process_panels(BASE_DIR, OUTPUT_BASE_DIR)\n",
    "\n",
    "# 9. Send sample and descriptive statistics results to Claude for interpretation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from typing import Dict, List\n",
    "import json\n",
    "from anthropic import Anthropic\n",
    "import traceback\n",
    "from fpdf import FPDF\n",
    "\n",
    "class DescriptiveStatsAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    def calculate_descriptive_stats(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Calculate descriptive statistics for the dataset\"\"\"\n",
    "        # List of numeric columns to analyze (excluding GVKEY, FYEAR, etc.)\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col not in ['GVKEY', 'FYEAR', 'sic3', 'Year']]\n",
    "        \n",
    "        # Sort columns to match example order if possible\n",
    "        preferred_order = [\n",
    "            'linstown', 'lsize', 'lbtm', 'lroa', 'lsaret12', 'levol', 'lloss', 'lcalrisk'\n",
    "        ]\n",
    "        sorted_cols = sorted(numeric_cols, key=lambda x: \n",
    "                           preferred_order.index(x) if x in preferred_order else float('inf'))\n",
    "        \n",
    "        stats = {}\n",
    "        for col in sorted_cols:  # Use sorted_cols instead of numeric_cols\n",
    "            col_stats = {\n",
    "                'n': len(df[col].dropna()),\n",
    "                'mean': df[col].mean(),\n",
    "                'median': df[col].median(),\n",
    "                'std': df[col].std(),\n",
    "                'p25': df[col].quantile(0.25),\n",
    "                'p75': df[col].quantile(0.75),\n",
    "                'min': df[col].min(),\n",
    "                'max': df[col].max()\n",
    "            }\n",
    "            stats[col] = col_stats\n",
    "        \n",
    "        # Add additional summary statistics\n",
    "        stats['summary'] = {\n",
    "            'total_observations': len(df),\n",
    "            'unique_firms': len(df['GVKEY'].unique()),\n",
    "            'year_range': f\"{df['FYEAR'].min()} to {df['FYEAR'].max()}\",\n",
    "            'industries': len(df['sic3'].unique())\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_claude_interpretation(self, stats: Dict, regulation_title: str) -> str:\n",
    "        \"\"\"Get Claude's interpretation of descriptive statistics\"\"\"\n",
    "        # Format statistics for Claude\n",
    "        stats_text = f\"Descriptive Statistics for {regulation_title}\\n\\n\"\n",
    "        \n",
    "        # Add summary information\n",
    "        summary = stats['summary']\n",
    "        stats_text += \"Sample Characteristics:\\n\"\n",
    "        stats_text += f\"Total observations: {summary['total_observations']:,}\\n\"\n",
    "        stats_text += f\"Number of unique firms: {summary['unique_firms']:,}\\n\"\n",
    "        stats_text += f\"Sample period: {summary['year_range']}\\n\"\n",
    "        stats_text += f\"Number of industries: {summary['industries']}\\n\\n\"\n",
    "        \n",
    "        # Add variable statistics\n",
    "        stats_text += \"Variable Statistics:\\n\"\n",
    "        for var, var_stats in {k: v for k, v in stats.items() if k != 'summary'}.items():\n",
    "            stats_text += f\"\\n{var}:\\n\"\n",
    "            stats_text += f\"N: {var_stats['n']:,}\\n\"\n",
    "            stats_text += f\"Mean: {var_stats['mean']:.3f}\\n\"\n",
    "            stats_text += f\"Median: {var_stats['median']:.3f}\\n\"\n",
    "            stats_text += f\"Std Dev: {var_stats['std']:.3f}\\n\"\n",
    "            stats_text += f\"25th percentile: {var_stats['p25']:.3f}\\n\"\n",
    "            stats_text += f\"75th percentile: {var_stats['p75']:.3f}\\n\"\n",
    "            stats_text += f\"Min: {var_stats['min']:.3f}\\n\"\n",
    "            stats_text += f\"Max: {var_stats['max']:.3f}\\n\"\n",
    "        \n",
    "        # Create prompt for Claude\n",
    "        prompt = f\"\"\"You are an accounting academic with a PhD in accounting. \n",
    "        You should use active voice (e.g. \"We find\" instead of \"It is found\"). \n",
    "        Use present tense for all established findings. Write the descriptive statistics section for this analysis as if \n",
    "        you were writing an academic paper for an accounting journal. Here are the descriptive statistics:\n",
    "\n",
    "{stats_text}\n",
    "\n",
    "Please structure your analysis as follows (400 words):\n",
    "1. Label this section \"Sample Description and Descriptive Statistics\"\n",
    "2. Describe the sample characteristics (number of firms, time period, industries)\n",
    "3. Describe the key variables' distributions\n",
    "4. Highlight any notable patterns or potential outliers\n",
    "5. Compare statistics to relevant benchmarks from prior literature where applicable\n",
    "\n",
    "Write in an academic style suitable for a top accounting journal.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting Claude interpretation: {str(e)}\")\n",
    "            return f\"Error in Claude analysis: {str(e)}\"\n",
    "    \n",
    "    def create_descriptive_stats_table(self, stats: Dict, output_path: str, regulation_title: str):\n",
    "        \"\"\"Create a PDF table of descriptive statistics in academic paper format\"\"\"\n",
    "        try:\n",
    "            pdf = FPDF(format='A4', orientation='L')\n",
    "            pdf.add_page()\n",
    "        \n",
    "            # Set Times New Roman font\n",
    "            try:\n",
    "                pdf.add_font('Times', '', 'times.ttf', uni=True)\n",
    "                pdf.add_font('Times', 'B', 'timesbd.ttf', uni=True)\n",
    "                pdf.set_font('Times', size=11)\n",
    "            except:\n",
    "                pdf.set_font('Times', size=11)\n",
    "        \n",
    "            pdf.set_margins(20, 20, 20)\n",
    "        \n",
    "            # Title\n",
    "            pdf.set_font('Times', 'B', 14)\n",
    "            pdf.cell(0, 10, 'Table 1', align='C', ln=True)\n",
    "            pdf.set_font('Times', '', 12)\n",
    "            pdf.cell(0, 10, 'Descriptive Statistics', align='C', ln=True)\n",
    "            pdf.ln(5)\n",
    "        \n",
    "            # Calculate column widths\n",
    "            var_width = 70\n",
    "            num_width = 30\n",
    "        \n",
    "            # Table headers\n",
    "            pdf.set_font('Times', 'B')\n",
    "            headers = ['Variables', 'N', 'Mean', 'Std. Dev.', 'P25', 'Median', 'P75']\n",
    "            pdf.cell(var_width, 8, headers[0], border=1)\n",
    "            for header in headers[1:]:\n",
    "                pdf.cell(num_width, 8, header, border=1, align='C')\n",
    "            pdf.ln()\n",
    "        \n",
    "            # Variable name mappings with ordered display\n",
    "            var_display_names = {\n",
    "                'freqMF': 'FreqMF',\n",
    "                'treatment_effect': 'Treatment Effect',\n",
    "                'linstown': 'Institutional ownership',\n",
    "                'lsize': 'Firm size',\n",
    "                'lbtm': 'Book-to-market',\n",
    "                'lroa': 'ROA',\n",
    "                'lsaret12': 'Stock return',\n",
    "                'levol': 'Earnings volatility',\n",
    "                'lloss': 'Loss',\n",
    "                'lcalrisk': 'Class action litigation risk'\n",
    "            }\n",
    "        \n",
    "            # Excluded variables\n",
    "            excluded_vars = {'sic4', 'permno', 'post-law', 'treated'}\n",
    "        \n",
    "            # Sort variables to ensure FreqMF is first\n",
    "            variables = {k: v for k, v in stats.items() \n",
    "                        if k != 'summary' and k not in excluded_vars}\n",
    "        \n",
    "            # Define display order\n",
    "            display_order = ['freqMF', 'treatment_effect'] + [\n",
    "                k for k in var_display_names.keys() \n",
    "                if k not in ['freqMF', 'treatment_effect']\n",
    "            ]\n",
    "        \n",
    "            pdf.set_font('Times', '')\n",
    "            for var_name in display_order:\n",
    "                if var_name in variables:\n",
    "                    var_stats = variables[var_name]\n",
    "                    display_name = var_display_names.get(var_name, var_name)\n",
    "                    pdf.cell(var_width, 8, display_name, border=1)\n",
    "                \n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['n']:,}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['mean']:.4f}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['std']:.4f}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['p25']:.4f}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['median']:.4f}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['p75']:.4f}\", border=1, align='C')\n",
    "                    pdf.ln()\n",
    "        \n",
    "            # Footnote\n",
    "            pdf.ln(10)\n",
    "            pdf.set_font('Times', '', 10)\n",
    "            footnote = \"This table shows the descriptive statistics. All continuous variables are winsorized at the 1st and 99th percentiles.\"\n",
    "            pdf.multi_cell(0, 5, footnote)\n",
    "        \n",
    "            pdf.output(output_path)\n",
    "            print(f\"Successfully saved descriptive statistics table to {output_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating descriptive statistics table: {str(e)}\")\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        \n",
    "            try:\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Times\", size=12)\n",
    "                pdf.cell(0, 10, \"Error occurred while creating descriptive statistics table\")\n",
    "                pdf.ln()\n",
    "                pdf.cell(0, 10, f\"Error: {str(e)}\")\n",
    "                pdf.output(output_path)\n",
    "            except Exception as e2:\n",
    "                print(f\"Emergency PDF save also failed: {str(e2)}\")\n",
    "    \n",
    "    def analyze_panel(self, panel_dir: str, output_dir: str) -> None:\n",
    "        \"\"\"Analyze descriptive statistics for a single panel dataset\"\"\"\n",
    "        try:\n",
    "            # Get panel name from directory name\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            print(f\"\\nAnalyzing {panel_name}...\")\n",
    "            \n",
    "            # Read filtered data\n",
    "            data_file = os.path.join(panel_dir, 'filtered_data.csv')\n",
    "            if not os.path.exists(data_file):\n",
    "                print(f\"No filtered_data.csv found in {panel_dir}\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Reading data from {data_file}\")\n",
    "            df = pd.read_csv(data_file)\n",
    "            \n",
    "            # Create output directory\n",
    "            panel_output_dir = os.path.join(output_dir, panel_name)\n",
    "            os.makedirs(panel_output_dir, exist_ok=True)\n",
    "            print(f\"Created output directory: {panel_output_dir}\")\n",
    "            \n",
    "            # Calculate descriptive statistics\n",
    "            print(\"Calculating descriptive statistics...\")\n",
    "            stats = self.calculate_descriptive_stats(df)\n",
    "            \n",
    "            # Save descriptive statistics to JSON\n",
    "            stats_path = os.path.join(panel_output_dir, 'descriptive_stats.json')\n",
    "            with open(stats_path, 'w') as f:\n",
    "                json.dump(stats, f, indent=4)\n",
    "            print(f\"Saved descriptive statistics to {stats_path}\")\n",
    "            \n",
    "            # Create and save descriptive statistics table\n",
    "            table_path = os.path.join(panel_output_dir, 'descriptive_stats_table.pdf')\n",
    "            print(f\"Attempting to create PDF table at {table_path}\")\n",
    "            self.create_descriptive_stats_table(stats, table_path, panel_name)\n",
    "            \n",
    "            # Get Claude's interpretation\n",
    "            print(\"Getting Claude's interpretation...\")\n",
    "            interpretation = self.get_claude_interpretation(stats, panel_name)\n",
    "            \n",
    "            # Save Claude's interpretation\n",
    "            interpretation_path = os.path.join(panel_output_dir, 'descriptive_stats_analysis.txt')\n",
    "            with open(interpretation_path, 'w') as f:\n",
    "                f.write(interpretation)\n",
    "            print(f\"Saved Claude's analysis to {interpretation_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {panel_dir}: {str(e)}\")\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "\n",
    "def analyze_all_panels(base_dir: str, output_dir: str, api_key: str):\n",
    "    \"\"\"Analyze all panel datasets in subfolders\"\"\"\n",
    "    analyzer = DescriptiveStatsAnalyzer(api_key)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Find all subfolders that start with 'panel_'\n",
    "    panel_dirs = [d for d in glob.glob(os.path.join(base_dir, 'panel_*_*')) if os.path.isdir(d)]\n",
    "    print(f\"Found {len(panel_dirs)} panel directories to analyze\")\n",
    "\n",
    "    for i, panel_dir in enumerate(panel_dirs, 1):\n",
    "        print(f\"\\nProcessing panel {i} of {len(panel_dirs)}: {panel_dir}\")\n",
    "        analyzer.analyze_panel(panel_dir, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"\n",
    "    # Updated paths for Windows using raw strings to handle backslashes\n",
    "    BASE_DIR = \"enter folder path here\"\n",
    "    OUTPUT_DIR =\"enter folder path here\"\n",
    "    \n",
    "    # Run analysis on all panels\n",
    "    analyze_all_panels(BASE_DIR, OUTPUT_DIR, API_KEY)\n",
    "\n",
    "# 10. Ask Claude to write introduction\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "import glob\n",
    "\n",
    "class ComprehensiveAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    def read_regression_results(self, base_dir: str) -> dict:\n",
    "        \"\"\"Read regression results from all panel subfolders\"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        # Look for panel directories that include mechanism\n",
    "        panel_dirs = [d for d in glob.glob(os.path.join(base_dir, 'panel_*_*')) if os.path.isdir(d)]\n",
    "        \n",
    "        for panel_dir in panel_dirs:\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            results_file = os.path.join(panel_dir, 'regression_results.json')\n",
    "            \n",
    "            if os.path.exists(results_file):\n",
    "                print(f\"Reading results from {panel_name}\")\n",
    "                with open(results_file, 'r') as f:\n",
    "                    results = json.load(f)\n",
    "                all_results[panel_name] = results\n",
    "            else:\n",
    "                print(f\"No results file found in {panel_name}\")\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "    def get_laws_analysis(self, csv_file: str) -> list:\n",
    "        \"\"\"Read and analyze laws from CSV file, return list of law dictionaries\"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "        laws = []\n",
    "        \n",
    "        # Define economic mechanisms\n",
    "        mechanisms = [\n",
    "            'Litigation Risk', \n",
    "            'Corporate Governance', \n",
    "            'Proprietary Costs', \n",
    "            'Information Asymmetry',\n",
    "            'Unsophisticated Investors', \n",
    "            'Equity Issuance', \n",
    "            'Reputation Risk'\n",
    "        ]\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Get active mechanisms (where value is 'Yes')\n",
    "            active_mechanisms = [mech for mech in mechanisms if row[mech] == 'Yes']\n",
    "            \n",
    "            law = {\n",
    "                'title': row['Regulation Title'],\n",
    "                'year': row['Year'],\n",
    "                'body': row['Regulatory Body'],\n",
    "                'description': row['Description'],\n",
    "                'impact': row['Impact'],\n",
    "                'mechanisms': active_mechanisms\n",
    "            }\n",
    "            laws.append(law)\n",
    "        \n",
    "        return laws\n",
    "\n",
    "    def format_regression_results(self, results: dict) -> str:\n",
    "        \"\"\"Format regression results for a specific panel\"\"\"\n",
    "        if not results:\n",
    "            return \"No regression results available.\"\n",
    "            \n",
    "        formatted_text = \"\\nRegression Results:\\n\\n\"\n",
    "        \n",
    "        for spec_name, spec_results in results.items():\n",
    "            formatted_text += f\"\\nSpecification {spec_name}:\\n\"\n",
    "            try:\n",
    "                formatted_text += f\"Treatment Effect: {spec_results['coefficients']['treatment_effect']:.4f}\\n\"\n",
    "                formatted_text += f\"T-statistic: {abs(spec_results['t_stats']['treatment_effect']):.2f}\\n\"\n",
    "                formatted_text += f\"P-value: {spec_results['pvalues']['treatment_effect']:.4f}\\n\"\n",
    "                formatted_text += f\"R-squared: {spec_results['r_squared']:.4f}\\n\"\n",
    "                \n",
    "                if spec_results['controls']:\n",
    "                    formatted_text += \"\\nControl Variables:\\n\"\n",
    "                    for control in spec_results['controls']:\n",
    "                        coef = spec_results['coefficients'][control]\n",
    "                        tstat = spec_results['t_stats'][control]\n",
    "                        pvalue = spec_results['pvalues'][control]\n",
    "                        formatted_text += f\"{control}: coef={coef:.4f}, t={tstat:.2f}, p={pvalue:.4f}\\n\"\n",
    "                \n",
    "                formatted_text += \"\\n\" + \"-\"*50 + \"\\n\"\n",
    "            except KeyError as e:\n",
    "                print(f\"Missing key in regression results: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return formatted_text\n",
    "\n",
    "    def get_comprehensive_introduction(self, law: dict, mechanism: str, regression_results: dict) -> str:\n",
    "        \"\"\"Get comprehensive introduction for a law and specific mechanism\"\"\"\n",
    "        regression_text = self.format_regression_results(regression_results)\n",
    "        print(f\"\\nFormatted regression results for {law['title']} - {mechanism}:\")\n",
    "        print(regression_text)\n",
    "        \n",
    "        prompt = f\"\"\"As an accounting academic, please write a comprehensive introduction section examining {law['title']} \n",
    "        and its impact on voluntary disclosure through the {mechanism} channel.\n",
    "\n",
    "Law Details:\n",
    "Title: {law['title']} ({law['year']})\n",
    "Regulatory Body: {law['body']}\n",
    "Description: {law['description']}\n",
    "Impact: {law['impact']}\n",
    "Economic Mechanism: {mechanism}\n",
    "\n",
    "Empirical Results:\n",
    "{regression_text}\n",
    "\n",
    "Please structure the introduction as follows:\n",
    "\n",
    "1. Motivation (2 paragraphs, ~200 words):\n",
    "   - Begin with the importance of {law['title']}\n",
    "   - Open with a broad statement about {law['title']}\n",
    "   - Focus specifically on how it relates to {mechanism}\n",
    "   - Explain its relevance to voluntary disclosure through this mechanism\n",
    "   - Identify the specific gap or puzzle in the literature\n",
    "   - Identify specific research questions\n",
    "\n",
    "2. Hypothesis Development (3 paragraphs, ~300 words):\n",
    "   - Present the economic mechanism linking the regulation to voluntary disclosure\n",
    "   - Explain how {mechanism} affects voluntary disclosure\n",
    "   - Discuss theoretical underpinnings\n",
    "   - Build on established theoretical frameworks\n",
    "   - Develop clear, testable predictions\n",
    "   - Build logical arguments step by step\n",
    "   - Support each claim with citations to foundational papers\n",
    "   - Support arguments with citations\n",
    "\n",
    "3. Results Summary (3 paragraphs, ~300 words):\n",
    "   - Lead with strongest statistical findings\n",
    "   - Present the treatment effect coefficient of {regression_text}\n",
    "   - Summarize the key findings of the analysis, \n",
    "     discussing the significance of the variable in terms of predictive power: {regression_text}\n",
    "   - Discuss significance of variables and their predictive power\n",
    "   - Present results in order of importance\n",
    "   - Include economic significance\n",
    "   - Use precise statistical language\n",
    "   - Connect findings back to the {mechanism} channel\n",
    "\n",
    "4. Contribution (2 paragraphs, ~200 words):\n",
    "   - Position relative to 3-4 most closely related papers\n",
    "   - Highlight novel findings about {mechanism}\n",
    "   - Discuss broader implications for theory and practice\n",
    "   - Emphasize contributions to understanding this specific economic channel\n",
    "\n",
    "Guidelines:\n",
    "- Do not include headers in the write up\n",
    "- Do not include extra text or explanations\n",
    "    -Example of what not to include: \"Here's a comprehensive introduction section following your guidelines\" or \n",
    "    \"Here's a comprehensive introduction section examining Resource Extraction Disclosure Rules and its impact on voluntary disclosure through the Corporate Governance channel\"\n",
    "- Use active voice (e.g., \"We find\" instead of \"It is found\")\n",
    "- Maintain formal academic tone\n",
    "- Include 2-3 citations per paragraph \n",
    "- Use present tense for established findings\n",
    "- Use past tense for your specific results\n",
    "- Make clear distinctions between correlation and causation\n",
    "- Avoid speculation beyond the data\n",
    "- Cite papers from top accounting and finance journals such as:\n",
    "    The Accounting Review, Journal of Accounting Research, \n",
    "    Journal of Accounting and Economics, Contemporary Accounting Research, Accounting, Organizations, and Society,\n",
    "    and Review of Financial Studies\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting introduction: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def analyze_and_save_introductions(self, base_dir: str, csv_file: str, output_dir: str):\n",
    "        \"\"\"Generate and save comprehensive introductions\"\"\"\n",
    "        # Create introduction directory\n",
    "        intro_dir = os.path.join(output_dir, 'introduction')\n",
    "        os.makedirs(intro_dir, exist_ok=True)\n",
    "        \n",
    "        # Get laws and regression results\n",
    "        laws = self.get_laws_analysis(csv_file)\n",
    "        regression_results = self.read_regression_results(base_dir)\n",
    "        \n",
    "        # Generate introduction for each law and each mechanism\n",
    "        for law in laws:\n",
    "            print(f\"\\nProcessing law: {law['title']}\")\n",
    "            \n",
    "            # Generate separate introduction for each mechanism\n",
    "            for mechanism in law['mechanisms']:\n",
    "                print(f\"Writing introduction for mechanism: {mechanism}\")\n",
    "                \n",
    "                # Find matching regression results if available\n",
    "                clean_mechanism = mechanism.replace(' ', '_')\n",
    "                panel_name = f\"panel_{law['title'].replace(' ', '')}_{clean_mechanism}\"\n",
    "                law_results = regression_results.get(panel_name, {})\n",
    "                \n",
    "                if not law_results:\n",
    "                    print(f\"Warning: No regression results found for {panel_name}\")\n",
    "                \n",
    "                # Generate introduction\n",
    "                intro = self.get_comprehensive_introduction(law, mechanism, law_results)\n",
    "                \n",
    "                # Create filename with both law and mechanism\n",
    "                filename = f\"{law['title'].replace(' ', '_')}_{clean_mechanism}_introduction.txt\"\n",
    "                \n",
    "                # Save introduction\n",
    "                with open(os.path.join(intro_dir, filename), 'w', encoding='utf-8') as f:\n",
    "                    f.write(intro)\n",
    "                \n",
    "                print(f\"Saved introduction for {law['title']} - {mechanism}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"\n",
    "    BASE_DIR = \"enter folder path here\"\n",
    "    CSV_FILE = \"enter file path here\"\n",
    "    OUTPUT_DIR = \"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        analyzer = ComprehensiveAnalyzer(API_KEY)\n",
    "        analyzer.analyze_and_save_introductions(BASE_DIR, CSV_FILE, OUTPUT_DIR)\n",
    "        print(\"Analysis complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# 11. Ask Claude to write the model specification section of a paper \n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "import glob\n",
    "\n",
    "class ComprehensiveAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    def read_regression_results(self, base_dir: str) -> dict:\n",
    "        \"\"\"Read regression results from all panel subfolders\"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        panel_dirs = [d for d in glob.glob(os.path.join(base_dir, 'panel_*_*')) if os.path.isdir(d)]\n",
    "        \n",
    "        for panel_dir in panel_dirs:\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            results_file = os.path.join(panel_dir, 'regression_results.json')\n",
    "            \n",
    "            if os.path.exists(results_file):\n",
    "                print(f\"Reading results from {panel_name}\")\n",
    "                with open(results_file, 'r') as f:\n",
    "                    results = json.load(f)\n",
    "                all_results[panel_name] = results\n",
    "            else:\n",
    "                print(f\"No results file found in {panel_name}\")\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "    def format_regression_results(self, results: dict) -> str:\n",
    "        \"\"\"Format regression results for a specific panel\"\"\"\n",
    "        formatted_text = \"\\nRegression Results:\\n\\n\"\n",
    "        \n",
    "        for spec_name, spec_results in results.items():\n",
    "            formatted_text += f\"\\nSpecification {spec_name}:\\n\"\n",
    "            formatted_text += f\"Treatment Effect: {spec_results['coefficients']['treatment_effect']:.4f}\\n\"\n",
    "            formatted_text += f\"T-statistic: {abs(spec_results['t_stats']['treatment_effect']):.2f}\\n\"\n",
    "            formatted_text += f\"P-value: {spec_results['pvalues']['treatment_effect']:.4f}\\n\"\n",
    "            formatted_text += f\"R-squared: {spec_results['r_squared']:.4f}\\n\"\n",
    "            \n",
    "            if spec_results['controls']:\n",
    "                formatted_text += \"\\nControl Variables:\\n\"\n",
    "                for control in spec_results['controls']:\n",
    "                    coef = spec_results['coefficients'][control]\n",
    "                    tstat = spec_results['t_stats'][control]\n",
    "                    pvalue = spec_results['pvalues'][control]\n",
    "                    formatted_text += f\"{control.replace('_', ' ')}: coef={coef:.4f}, t={tstat:.2f}, p={pvalue:.4f}\\n\"\n",
    "            \n",
    "            formatted_text += \"\\n\" + \"-\"*50 + \"\\n\"\n",
    "        \n",
    "        return formatted_text\n",
    "\n",
    "    def get_laws_analysis(self, csv_file: str) -> list:\n",
    "        \"\"\"Read and analyze laws from CSV file\"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "        laws = []\n",
    "        \n",
    "        # Define economic mechanisms\n",
    "        mechanisms = [\n",
    "            'Litigation Risk', \n",
    "            'Corporate Governance', \n",
    "            'Proprietary Costs', \n",
    "            'Information Asymmetry',\n",
    "            'Unsophisticated Investors', \n",
    "            'Equity Issuance', \n",
    "            'Reputation Risk'\n",
    "        ]\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Get active mechanisms (where value is 'Yes')\n",
    "            active_mechanisms = [mech for mech in mechanisms if row[mech] == 'Yes']\n",
    "            \n",
    "            law = {\n",
    "                'title': row['Regulation Title'],\n",
    "                'year': row['Year'],\n",
    "                'body': row['Regulatory Body'],\n",
    "                'description': row['Description'],\n",
    "                'impact': row['Impact'],\n",
    "                'mechanisms': active_mechanisms\n",
    "            }\n",
    "            laws.append(law)\n",
    "        \n",
    "        return laws\n",
    "\n",
    "    def get_model_specification(self, law: dict, mechanism: str, regression_results: dict) -> str:\n",
    "        \"\"\"Get model specification section for a law and specific mechanism with regression results\"\"\"\n",
    "        regression_text = self.format_regression_results(regression_results) if regression_results else \"No regression results available.\"\n",
    "        \n",
    "        # Get number of observations from regression results\n",
    "        n_obs = None\n",
    "        if regression_results and '(3)' in regression_results:\n",
    "            n_obs = regression_results['(3)'].get('n_obs', 'Not available')\n",
    "        \n",
    "        # Get list of control variables from regression results\n",
    "        controls = []\n",
    "        if regression_results:\n",
    "            for spec in regression_results.values():\n",
    "                if spec.get('controls'):\n",
    "                    controls.extend(spec['controls'])\n",
    "            controls = list(set(controls))  # Remove duplicates\n",
    "        \n",
    "        prompt = f\"\"\"You are an accounting academic writing a research paper examining {law['title']} and its impact \n",
    "        on voluntary disclosure through the {mechanism} channel. \n",
    "        Please write the model specification section for an academic journal in accounting.\n",
    "\n",
    "Law Details:\n",
    "Title: {law['title']} ({law['year']})\n",
    "Description: {law['description']}\n",
    "Impact: {law['impact']}\n",
    "Economic Mechanism: {mechanism}\n",
    "\n",
    "Regression Information:\n",
    "{regression_text}\n",
    "\n",
    "Please follow these detailed guidelines:\n",
    "\n",
    "1. Label this section \"Research Design\" or \"Model Specification\"\n",
    "\n",
    "2. Identifying Firms Affected by {law['title']} \n",
    "    - Explain the step by step process to identify firms affected by {law['title']} \n",
    "    - Describe the regulatory authority that is responsible for the law {law['body']} \n",
    "\n",
    "3. Model Explanation (2-3 paragraphs, ~300 words total):\n",
    "    - Explain the regression model used to examine the relationship between {law['title']} \n",
    "      and voluntary disclosure through the {mechanism} channel\n",
    "          -The model is: FreqMF = β₀ + β₁Treatment Effect + γControls + ε\n",
    "    - Only discuss the control variables that appear in the regression results {regression_text}\n",
    "      These variables are based on prior literature and are: Institutional Ownership, Firm Size, Book-to-Market,\n",
    "      ROA, Stock Return, Earnings volatility, Loss, Class action litigation risk\n",
    "    - Support model choices with citations to foundational papers\n",
    "    - Explain potential endogeneity concerns and how the research design addresses them\n",
    "    - Use clear, academic language\n",
    "    - Avoid using underscores in variable names\n",
    "\n",
    "4. Mathematical Model:\n",
    "    - Present the complete regression equation in proper mathematical notation {regression_text}\n",
    "        - Label the equation as follows: FreqMF = β₀ + β₁Treatment Effect + γControls + ε\n",
    "            - Label the dependent variable \"FreqMF\"\n",
    "            - Label the variable of interest as \"Treatment Effect\"\n",
    "            - Label the control variables in the regression equation as \"Controls\"\n",
    "    - Do no include the subscripts i and t in the regression \n",
    "    - Format the equation professionally\n",
    "\n",
    "5. Variable Definitions (2-3 paragraphs, ~300 words total):\n",
    "    - Define the dependent variable (FreqMF - management forecast frequency)\n",
    "    - Define the \"Treatment Effect\" variable\n",
    "    - Define each control variable used in the model as they appear in {regression_text}\n",
    "      These variables are based on prior literature and are: Institutional Ownership, Firm Size, Book-to-Market,\n",
    "      ROA, Stock Return, Earnings volatility, Loss, Class action litigation risk\n",
    "        -Cite the appropriate paper for these variables from the Journal of Accounting Research\n",
    "    - Do no include the subscripts i and t in the variable definition\n",
    "    - For each control variable, provide detailed explanations about their expected relationships with voluntary disclosure\n",
    "    - Explain how variables relate to the {mechanism} channel\n",
    "    \n",
    "\n",
    "6. Sample Construction (2-3 paragraphs, ~300 words total):\n",
    "    - Describe the event window around {law['year']}\n",
    "        -The time window for this analysis is 2 years before and 2 years after the regulation is implemented. Therefore,\n",
    "         The total number of years of the sample period is 5 years.\n",
    "    - Describe the source of the data from Compustat, I/B/E/S, Audit Analytics, and CRSP\n",
    "    - Describe the sample construction process based on the number of observations: {n_obs if n_obs else 'Not available'}\n",
    "    - Explain the treatment and control groups\n",
    "    - Note any sample restrictions\n",
    "\n",
    "Writing Guidelines:\n",
    "- - Provide only the write up, no extra text or explanations \n",
    "    -Example of what not to include: \"Here's a comprehensive model specifaction section following your guidelines\"\n",
    "- Use active voice (e.g., \"We find\" instead of \"It is found\")\n",
    "- Maintain formal academic tone\n",
    "- Include 2-3 citations per paragraph\n",
    "- Cite papers from top accounting and finance journals such as:\n",
    "    The Accounting Review, Journal of Accounting Research, \n",
    "    Journal of Accounting and Economics, Contemporary Accounting Research, Accounting, Organizations, and Society,\n",
    "    and Review of Financial Studies\n",
    "- Use precise statistical language\n",
    "- Make clear connections between variables and theoretical predictions\n",
    "- Do not include Latex format\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting model specification: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def create_model_specifications(self, base_dir: str, csv_file: str, output_dir: str):\n",
    "        \"\"\"Generate and save model specification sections for all laws\"\"\"\n",
    "        # Create main directory\n",
    "        main_dir = os.path.join(output_dir, 'model_specification')\n",
    "        os.makedirs(main_dir, exist_ok=True)\n",
    "        \n",
    "        # Get regression results for existing panels\n",
    "        regression_results = self.read_regression_results(base_dir)\n",
    "        total_panels = len(regression_results)\n",
    "        processed = 0\n",
    "        \n",
    "        print(f\"\\nFound {total_panels} panels in regression folder\")\n",
    "\n",
    "        \n",
    "        # Process each panel that exists\n",
    "        for panel_name, results in regression_results.items():\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Processing panel {processed + 1} of {total_panels}: {panel_name}\")\n",
    "        \n",
    "            try:\n",
    "                # Parse panel name to get law and mechanism\n",
    "                parts = panel_name.replace('panel_', '').split('_')\n",
    "                mechanism = parts[-1]  # Last part is the mechanism\n",
    "                law_name = '_'.join(parts[:-1])  # Everything else is the law name\n",
    "            \n",
    "                # Read the original panel file to get law details\n",
    "                panel_file = os.path.join(base_dir, panel_name, \"filtered_data.csv\")\n",
    "                df = pd.read_csv(panel_file)\n",
    "            \n",
    "                law = {\n",
    "                    'title': df['Regulation Title'].iloc[0],\n",
    "                    'year': df['Year'].iloc[0],\n",
    "                    'body': df.get('Regulatory Body', ['Unknown']).iloc[0],\n",
    "                    'description': df.get('Description', ['Not available']).iloc[0],\n",
    "                    'impact': df.get('Impact', ['Not available']).iloc[0],\n",
    "                    'mechanisms': [mechanism]\n",
    "                }\n",
    "            \n",
    "                # Generate model specification\n",
    "                content = self.get_model_specification(law, mechanism, results)\n",
    "            \n",
    "                # Create filename\n",
    "                filename = f\"{panel_name}_model_specification.txt\"\n",
    "                file_path = os.path.join(main_dir, filename)\n",
    "            \n",
    "                # Save model specification\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "                print(f\"Saved model specification for {panel_name}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing panel {panel_name}: {str(e)}\")\n",
    "        \n",
    "            processed += 1\n",
    "            print(f\"\\nProgress: {processed}/{total_panels} ({(processed/total_panels)*100:.1f}%)\")\n",
    "    \n",
    "        print(\"\\nModel specification generation complete!\")\n",
    "    \n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"\n",
    "    BASE_DIR = \"enter folder path here\"\n",
    "    CSV_FILE = \"enter file path here\"\n",
    "    OUTPUT_DIR = \"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        analyzer = ComprehensiveAnalyzer(API_KEY)\n",
    "        analyzer.create_model_specifications(BASE_DIR, CSV_FILE, OUTPUT_DIR)\n",
    "        print(\"\\nModel specification sections complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# 12. Ask Claude to write a conclusion \n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "import glob\n",
    "\n",
    "class ComprehensiveAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    def read_regression_results(self, base_dir: str) -> dict:\n",
    "        \"\"\"Read regression results from all panel subfolders\"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        panel_dirs = [d for d in glob.glob(os.path.join(base_dir, 'panel_*_*')) if os.path.isdir(d)]\n",
    "        \n",
    "        for panel_dir in panel_dirs:\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            results_file = os.path.join(panel_dir, 'regression_results.json')\n",
    "            \n",
    "            if os.path.exists(results_file):\n",
    "                print(f\"Reading results from {panel_name}\")\n",
    "                with open(results_file, 'r') as f:\n",
    "                    results = json.load(f)\n",
    "                all_results[panel_name] = results\n",
    "            else:\n",
    "                print(f\"No results file found in {panel_name}\")\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "    def format_regression_results(self, results: dict) -> str:\n",
    "        \"\"\"Format regression results for a specific panel\"\"\"\n",
    "        formatted_text = \"\\nRegression Results:\\n\\n\"\n",
    "        \n",
    "        for spec_name, spec_results in results.items():\n",
    "            formatted_text += f\"\\nSpecification {spec_name}:\\n\"\n",
    "            formatted_text += f\"Treatment Effect: {spec_results['coefficients']['treatment_effect']:.4f}\\n\"\n",
    "            formatted_text += f\"T-statistic: {abs(spec_results['t_stats']['treatment_effect']):.2f}\\n\"\n",
    "            formatted_text += f\"P-value: {spec_results['pvalues']['treatment_effect']:.4f}\\n\"\n",
    "            formatted_text += f\"R-squared: {spec_results['r_squared']:.4f}\\n\"\n",
    "            \n",
    "            if spec_results['controls']:\n",
    "                formatted_text += \"\\nControl Variables:\\n\"\n",
    "                for control in spec_results['controls']:\n",
    "                    coef = spec_results['coefficients'][control]\n",
    "                    tstat = spec_results['t_stats'][control]\n",
    "                    pvalue = spec_results['pvalues'][control]\n",
    "                    formatted_text += f\"{control.replace('_', ' ')}: coef={coef:.4f}, t={tstat:.2f}, p={pvalue:.4f}\\n\"\n",
    "            \n",
    "            formatted_text += \"\\n\" + \"-\"*50 + \"\\n\"\n",
    "        \n",
    "        return formatted_text\n",
    "\n",
    "    def get_laws_analysis(self, csv_file: str) -> list:\n",
    "        \"\"\"Read and analyze laws from CSV file\"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "        laws = []\n",
    "        \n",
    "        # Define economic mechanisms\n",
    "        mechanisms = [\n",
    "            'Litigation Risk', \n",
    "            'Corporate Governance', \n",
    "            'Proprietary Costs', \n",
    "            'Information Asymmetry',\n",
    "            'Unsophisticated Investors', \n",
    "            'Equity Issuance', \n",
    "            'Reputation Risk'\n",
    "        ]\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Get active mechanisms (where value is 'Yes')\n",
    "            active_mechanisms = [mech for mech in mechanisms if row[mech] == 'Yes']\n",
    "            \n",
    "            law = {\n",
    "                'title': row['Regulation Title'],\n",
    "                'year': row['Year'],\n",
    "                'body': row['Regulatory Body'],\n",
    "                'description': row['Description'],\n",
    "                'impact': row['Impact'],\n",
    "                'mechanisms': active_mechanisms\n",
    "            }\n",
    "            laws.append(law)\n",
    "        \n",
    "        return laws\n",
    "\n",
    "    def get_conclusion(self, law: dict, mechanism: str, regression_results: dict) -> str:\n",
    "        \"\"\"Get conclusion section for a law and specific mechanism with regression results\"\"\"\n",
    "        regression_text = self.format_regression_results(regression_results) if regression_results else \"No regression results available.\"\n",
    "        \n",
    "        prompt = f\"\"\"You are an accounting academic writing a research paper examining {law['title']} and its \n",
    "        impact on voluntary disclosure through the {mechanism} channel. \n",
    "        Please write a conclusion section for an academic journal in accounting.\n",
    "\n",
    "Law Details:\n",
    "Title: {law['title']} ({law['year']})\n",
    "Description: {law['description']}\n",
    "Impact: {law['impact']}\n",
    "Economic Mechanism: {mechanism}\n",
    "\n",
    "Empirical Results:\n",
    "{regression_text}\n",
    "\n",
    "Please write a comprehensive conclusion following these guidelines:\n",
    "\n",
    "1. Summary of Main Findings (2-3 paragraphs):\n",
    "    - Restate the research question, focusing on the {mechanism} channel\n",
    "    - Summarize key empirical findings\n",
    "    - Discuss statistical and economic significance\n",
    "    - Interpret the results in the context of {law['title']} and {mechanism}\n",
    "\n",
    "2. Implications (1-2 paragraphs):\n",
    "    - Discuss implications for regulators\n",
    "    - Discuss implications for managers\n",
    "    - Discuss implications for investors\n",
    "    - Connect findings to broader literature on {mechanism}\n",
    "\n",
    "3. Limitations and Future Research (1-2 paragraphs):\n",
    "    - Acknowledge key limitations\n",
    "    - Suggest promising avenues for future research\n",
    "    - Discuss potential extensions, particularly related to {mechanism}\n",
    "    \n",
    "Writing Guidelines:\n",
    "- Use active voice (e.g., \"We find\" instead of \"It is found\")\n",
    "- Maintain formal academic tone\n",
    "- Use past tense for your specific results\n",
    "- Use present tense for implications\n",
    "- Make clear distinctions between correlation and causation\n",
    "- Focus on the practical significance of the findings\n",
    "- Cite papers from top accounting and finance journals such as:\n",
    "    The Accounting Review, Journal of Accounting Research, \n",
    "    Journal of Accounting and Economics, Contemporary Accounting Research, Accounting, Organizations, and Society,\n",
    "    and Review of Financial Studies\n",
    "- Do not include section headers\n",
    "- Length: approximately 750 words\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting conclusion: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def create_conclusions(self, base_dir: str, csv_file: str, output_dir: str):\n",
    "        \"\"\"Generate and save conclusion sections for all laws\"\"\"\n",
    "        # Create main directory\n",
    "        main_dir = os.path.join(output_dir, 'conclusion')\n",
    "        os.makedirs(main_dir, exist_ok=True)\n",
    "        \n",
    "        # Get laws and regression results\n",
    "        laws = self.get_laws_analysis(csv_file)\n",
    "        regression_results = self.read_regression_results(base_dir)\n",
    "        \n",
    "        # Process each law and mechanism\n",
    "        for law in laws:\n",
    "            print(f\"\\nProcessing law: {law['title']}\")\n",
    "            \n",
    "            # Find matching regression results\n",
    "            panel_name = f\"panel_{law['title'].replace(' ', '')}\"\n",
    "            law_results = regression_results.get(panel_name, {})\n",
    "            \n",
    "            # Generate separate conclusion for each mechanism\n",
    "            for mechanism in law['mechanisms']:\n",
    "                print(f\"Processing mechanism: {mechanism}\")\n",
    "                \n",
    "                # Generate conclusion\n",
    "                content = self.get_conclusion(law, mechanism, law_results)\n",
    "                \n",
    "                # Create filename with both law and mechanism\n",
    "                clean_mechanism = mechanism.replace(' ', '_')\n",
    "                filename = f\"{law['title'].replace(' ', '_')}_{clean_mechanism}_conclusion.txt\"\n",
    "                file_path = os.path.join(main_dir, filename)\n",
    "                \n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "                print(f\"Saved conclusion for {law['title']} - {mechanism}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"\n",
    "    BASE_DIR = \"enter folder path here\"\n",
    "    CSV_FILE = \"enter file path here\"\n",
    "    OUTPUT_DIR = \"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        analyzer = ComprehensiveAnalyzer(API_KEY)\n",
    "        analyzer.create_conclusions(BASE_DIR, CSV_FILE, OUTPUT_DIR)\n",
    "        print(\"\\nConclusion sections complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# 13. Ask Claude to write an abstract\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from anthropic import Anthropic\n",
    "\n",
    "class AbstractGenerator:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize abstract generator with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    def read_laws_data(self, csv_file: str) -> pd.DataFrame:\n",
    "        \"\"\"Read the laws data CSV file\"\"\"\n",
    "        return pd.read_csv(csv_file)\n",
    "    \n",
    "    def generate_abstract(self, introduction_content: str) -> str:\n",
    "        \"\"\"Generate an abstract based on an existing introduction\"\"\"\n",
    "        prompt = f\"\"\"As an accounting academic, please convert the following introduction into a concise academic abstract.\n",
    "\n",
    "Guidelines:\n",
    "- Maintain the key points from the introduction\n",
    "- Condense the content to 150-250 words\n",
    "- Include background, research objective, methodology, key findings, and contribution\n",
    "- Use a formal academic tone\n",
    "- Avoid adding new information not present in the original text\n",
    "- Use present tense for established findings\n",
    "- Use past tense for specific results\n",
    "- Do not include citations in the abstract\n",
    "- Do not use the label \"Abstract\"\n",
    "\n",
    "Introduction to Convert:\n",
    "{introduction_content}\n",
    "\n",
    "Please provide a structured abstract that captures the essence of the original introduction.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=3000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating abstract: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "    \n",
    "    def process_introductions(self, input_dir: str, output_dir: str):\n",
    "        \"\"\"Process introduction files and generate corresponding abstracts\"\"\"\n",
    "        # Create abstracts directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Find all introduction files\n",
    "        introduction_files = [f for f in os.listdir(input_dir) if f.endswith('_introduction.txt')]\n",
    "        \n",
    "        # Process each introduction file\n",
    "        for intro_file in introduction_files:\n",
    "            try:\n",
    "                # Read introduction content\n",
    "                with open(os.path.join(input_dir, intro_file), 'r', encoding='utf-8') as f:\n",
    "                    introduction_content = f.read()\n",
    "                \n",
    "                # Generate abstract\n",
    "                abstract = self.generate_abstract(introduction_content)\n",
    "                \n",
    "                # Create abstract filename (replace 'introduction' with 'abstract')\n",
    "                abstract_filename = intro_file.replace('_introduction.txt', '_abstract.txt')\n",
    "                \n",
    "                # Save abstract\n",
    "                abstract_path = os.path.join(output_dir, abstract_filename)\n",
    "                with open(abstract_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(abstract)\n",
    "                \n",
    "                print(f\"Generated abstract for {intro_file}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {intro_file}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API here\"  \n",
    "    \n",
    "    # Directories\n",
    "    INPUT_DIR = \"enter folder path here\"\n",
    "    OUTPUT_DIR = \"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize and run abstract generator\n",
    "        generator = AbstractGenerator(API_KEY)\n",
    "        generator.process_introductions(INPUT_DIR, OUTPUT_DIR)\n",
    "        print(\"Abstract generation complete!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# 14. Combine AI-generated content from Txt. files\n",
    "import os\n",
    "import traceback\n",
    "import pandas as pd\n",
    "\n",
    "def create_law_mechanism_dict(csv_file: str) -> dict:\n",
    "    \"\"\"Create dictionary of laws and their active mechanisms from CSV\"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Define all possible mechanisms\n",
    "    mechanisms = [\n",
    "        'Litigation Risk',\n",
    "        'Corporate Governance',\n",
    "        'Proprietary Costs',\n",
    "        'Information Asymmetry',\n",
    "        'Unsophisticated Investors',\n",
    "        'Equity Issuance',\n",
    "        'Reputation Risk'\n",
    "    ]\n",
    "    \n",
    "    # Create the dictionary\n",
    "    law_mechanisms = {}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Clean the law title for use as a key (replace spaces with underscores)\n",
    "        law_title = row['Regulation Title'].replace(' ', '_')\n",
    "        \n",
    "        # Get active mechanisms for this law (where value is 'Yes')\n",
    "        active_mechanisms = [mech for mech in mechanisms if row[mech] == 'Yes']\n",
    "        \n",
    "        # Add to dictionary if there are active mechanisms\n",
    "        if active_mechanisms:\n",
    "            law_mechanisms[law_title] = active_mechanisms\n",
    "    \n",
    "    return law_mechanisms\n",
    "\n",
    "def get_descriptive_stats(desc_dir: str, law_name: str, mechanism: str) -> str:\n",
    "    \"\"\"Get descriptive statistics from panel subfolder\"\"\"\n",
    "    # Convert law name to panel folder format (e.g., \"panel_ResourceExtractionDisclosureRules_Unsophisticated_Investors\")\n",
    "    clean_mechanism = mechanism.replace(' ', '_')\n",
    "    panel_name = f\"panel_{law_name.replace('_', '')}_{clean_mechanism}\"\n",
    "    panel_dir = os.path.join(desc_dir, panel_name)\n",
    "    \n",
    "    desc_file = os.path.join(panel_dir, 'descriptive_stats_analysis.txt')\n",
    "    \n",
    "    if os.path.exists(desc_file):\n",
    "        with open(desc_file, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        print(f\"Warning: No descriptive statistics found for {panel_name}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_regression_analyses(desc_dir: str, law_name: str, mechanism: str) -> str:\n",
    "    \"\"\"Get regression analyses from panel subfolder\"\"\"\n",
    "    # Convert law name to panel folder format (e.g., \"panel_ResourceExtractionDisclosureRules_Unsophisticated_Investors\")\n",
    "    clean_mechanism = mechanism.replace(' ', '_')\n",
    "    panel_name = f\"panel_{law_name.replace('_', '')}_{clean_mechanism}\"\n",
    "    panel_dir = os.path.join(desc_dir, panel_name)\n",
    "    \n",
    "    desc_file = os.path.join(panel_dir, 'claude_interpretation.txt')\n",
    "    \n",
    "    if os.path.exists(desc_file):\n",
    "        with open(desc_file, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        print(f\"Warning: No regressions analyses found for {panel_name}\")\n",
    "        return \"\"\n",
    "\n",
    "def combine_law_mechanism_sections(base_dir: str, law_name: str, mechanism: str) -> str:\n",
    "    \"\"\"Combine text sections for a specific law and mechanism in the correct order\"\"\"\n",
    "    # Clean law name by removing underscores between words in the law name\n",
    "    clean_law = law_name.replace('_', '')  # This removes underscores in the law name\n",
    "    \n",
    "    # Clean mechanism name for filenames - keep underscores between different components\n",
    "    clean_mechanism = mechanism.replace(' ', '_')\n",
    "    \n",
    "    # New file naming format: Law_Mechanism_combined.txt\n",
    "    output_filename = f\"{clean_law}_{clean_mechanism}_combined.txt\"\n",
    "    \n",
    "    # Define folder paths\n",
    "    abs_dir = os.path.join(base_dir, 'abstracts')\n",
    "    intro_dir = os.path.join(base_dir, 'introduction')\n",
    "    back_hypo_dir = os.path.join(base_dir, 'background and hypothesis development')\n",
    "    model_specification_dir = os.path.join(base_dir, 'model_specification')\n",
    "    desc_dir = os.path.join(base_dir, 'descriptive_stats')\n",
    "    reg_dir = os.path.join(base_dir, 'regression_analyses')\n",
    "    conc_dir = os.path.join(base_dir, 'conclusion')\n",
    "    \n",
    "    # Check if all required sections exist\n",
    "    required_files = {\n",
    "        'abstract': os.path.join(abs_dir, f\"{law_name}_{clean_mechanism}_abstract.txt\"),\n",
    "        'introduction': os.path.join(intro_dir, f\"{law_name}_{clean_mechanism}_introduction.txt\"),\n",
    "        'background': os.path.join(back_hypo_dir, f\"{law_name}_{clean_mechanism}_background_hypothesis.txt\"),\n",
    "        'model': os.path.join(model_specification_dir, f\"{law_name}_{clean_mechanism}_model_specification.txt\"),\n",
    "        'conclusion': os.path.join(conc_dir, f\"{law_name}_{clean_mechanism}_conclusion.txt\")\n",
    "    }\n",
    "    \n",
    "    # Check panel folders for descriptive stats and regression results\n",
    "    panel_name = f\"panel_{clean_law}_{clean_mechanism}\"\n",
    "    panel_files = {\n",
    "        'descriptive_stats': os.path.join(desc_dir, panel_name, 'descriptive_stats_analysis.txt'),\n",
    "        'regression': os.path.join(reg_dir, panel_name, 'claude_interpretation.txt')\n",
    "    }\n",
    "    \n",
    "    # Verify all sections exist\n",
    "    missing_sections = []\n",
    "    for section, filepath in {**required_files, **panel_files}.items():\n",
    "        if not os.path.exists(filepath):\n",
    "            missing_sections.append(section)\n",
    "    \n",
    "    if missing_sections:\n",
    "        print(f\"\\nSkipping {law_name} - {mechanism}: Missing sections: {', '.join(missing_sections)}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nAll sections found for {law_name} - {mechanism}. Proceeding with combination...\")\n",
    "    \n",
    "    # Define output directory\n",
    "    output_dir = os.path.join(base_dir, 'combined_sections')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize combined text\n",
    "    combined_text = f\"Analysis of {law_name} through {mechanism} channel\\n\\n\"\n",
    "    \n",
    "    # 1. Get abstracts\n",
    "    abs_file = os.path.join(abs_dir, f\"{law_name}_{clean_mechanism}_abstract.txt\")\n",
    "    if os.path.exists(abs_file):\n",
    "        print(f\"Adding abstract from {abs_file}\")\n",
    "        with open(abs_file, 'r', encoding='utf-8') as f:\n",
    "            combined_text += \"Abstract: \" + f.read().strip() + \"\\n\\n\"\n",
    "    \n",
    "    # Add a clear page break indicator\n",
    "    combined_text += \"\\f\" # Add form feed for page break\n",
    "    \n",
    "    # 2. Get introduction\n",
    "    intro_file = os.path.join(intro_dir, f\"{law_name}_{clean_mechanism}_introduction.txt\")\n",
    "    try:\n",
    "        if os.path.exists(intro_file):\n",
    "            print(f\"Attempting to read introduction from {intro_file}\")\n",
    "            print(f\"File size: {os.path.getsize(intro_file)} bytes\")\n",
    "            with open(intro_file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                combined_text += \"INTRODUCTION\\n\" + \"=\"*50 + \"\\n\\n\"\n",
    "                combined_text += content + \"\\n\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading introduction file {intro_file}: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "    \n",
    "            \n",
    "    # 3. Get background and hypotheses\n",
    "    back_hypo_file = os.path.join(back_hypo_dir, f\"{law_name}_{clean_mechanism}_background_hypothesis.txt\")\n",
    "    if os.path.exists(back_hypo_file):\n",
    "        print(f\"Adding background and hypotheses from {back_hypo_file}\")\n",
    "        with open(back_hypo_file, 'r', encoding='utf-8') as f:\n",
    "            combined_text += \"BACKGROUND AND HYPOTHESIS DEVELOPMENT\\n\" + \"=\"*50 + \"\\n\\n\"\n",
    "            combined_text += f.read() + \"\\n\\n\"\n",
    "            \n",
    "    # 4. Model specification\n",
    "    model_specification_file = os.path.join(model_specification_dir, f\"{law_name}_{clean_mechanism}_model_specification.txt\")\n",
    "    if os.path.exists(model_specification_file):\n",
    "        print(f\"Adding model specification from {model_specification_file}\")\n",
    "        with open(model_specification_file, 'r', encoding='utf-8') as f:\n",
    "            combined_text += \"MODEL SPECIFICATION\\n\" + \"=\"*50 + \"\\n\\n\"\n",
    "            combined_text += f.read() + \"\\n\\n\"\n",
    "    \n",
    "    # 5. Get descriptive statistics from panel subfolder\n",
    "    desc_text = get_descriptive_stats(desc_dir, law_name, mechanism)\n",
    "    if desc_text:\n",
    "        print(f\"Adding descriptive statistics for {law_name} - {mechanism}\")\n",
    "        combined_text += \"DESCRIPTIVE STATISTICS\\n\" + \"=\"*50 + \"\\n\\n\"\n",
    "        combined_text += desc_text + \"\\n\\n\"\n",
    "    \n",
    "    # 6. Get regression analyses from panel subfolder\n",
    "    reg_text = get_regression_analyses(reg_dir, law_name, mechanism)\n",
    "    if reg_text:\n",
    "        print(f\"Adding regression results for {law_name} - {mechanism}\")\n",
    "        combined_text += \"RESULTS\\n\" + \"=\"*50 + \"\\n\\n\"\n",
    "        combined_text += reg_text + \"\\n\\n\"\n",
    "        \n",
    "    # 7. Get conclusion\n",
    "    conc_file = os.path.join(conc_dir, f\"{law_name}_{clean_mechanism}_conclusion.txt\")\n",
    "    if os.path.exists(conc_file):\n",
    "        print(f\"Adding conclusion from {conc_file}\")\n",
    "        with open(conc_file, 'r', encoding='utf-8') as f:\n",
    "            combined_text += \"CONCLUSION\\n\" + \"=\"*50 + \"\\n\\n\"\n",
    "            combined_text += f.read() + \"\\n\\n\"\n",
    "    \n",
    "    # Save combined text\n",
    "    output_file = os.path.join(output_dir, output_filename)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(combined_text)\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def combine_all_laws(base_dir: str, csv_file: str):\n",
    "    \"\"\"Combine sections for all laws and their mechanisms\"\"\"\n",
    "    law_mechanisms = create_law_mechanism_dict(csv_file)\n",
    "    \n",
    "    complete_papers = 0\n",
    "    incomplete_papers = 0\n",
    "    total_combinations = sum(len(mechanisms) for mechanisms in law_mechanisms.values())\n",
    "    \n",
    "    print(f\"\\nFound {total_combinations} law-mechanism combinations to process\")\n",
    "    \n",
    "    for law_name, mechanisms in law_mechanisms.items():\n",
    "        for mechanism in mechanisms:\n",
    "            try:\n",
    "                output_file = combine_law_mechanism_sections(base_dir, law_name, mechanism)\n",
    "                if output_file:\n",
    "                    complete_papers += 1\n",
    "                    print(f\"Successfully combined complete paper for {law_name} - {mechanism}\")\n",
    "                    print(f\"Saved to: {output_file}\")\n",
    "                else:\n",
    "                    incomplete_papers += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {law_name} - {mechanism}: {str(e)}\")\n",
    "                incomplete_papers += 1\n",
    "    \n",
    "    print(\"\\nCombination Summary:\")\n",
    "    print(f\"Total combinations: {total_combinations}\")\n",
    "    print(f\"Complete papers: {complete_papers}\")\n",
    "    print(f\"Incomplete papers: {incomplete_papers}\")\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Script starting...\")  \n",
    "    BASE_DIR = \"enter folder path here\"\n",
    "    CSV_FILE = os.path.join(BASE_DIR, \"enter file path here\")\n",
    "    combine_all_laws(BASE_DIR, CSV_FILE)\n",
    "\n",
    "# 15. Ask Claude to create a reference list\n",
    "import os\n",
    "import re\n",
    "import anthropic\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_LEFT\n",
    "\n",
    "def create_reference_pdf(references, output_path):\n",
    "    \"\"\"\n",
    "    Creates a PDF with properly formatted references using ReportLab.\n",
    "    \n",
    "    Args:\n",
    "        references (list or str): List of references or string containing references\n",
    "        output_path (str): Path where the PDF will be saved\n",
    "    \"\"\"\n",
    "    doc = SimpleDocTemplate(\n",
    "        output_path,\n",
    "        pagesize=letter,\n",
    "        rightMargin=72,\n",
    "        leftMargin=72,\n",
    "        topMargin=72,\n",
    "        bottomMargin=72\n",
    "    )\n",
    "    \n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    # Create style for references with proper hanging indentation\n",
    "    ref_style = ParagraphStyle(\n",
    "        'Reference',\n",
    "        parent=styles['Normal'],\n",
    "        fontName='Times-Roman',\n",
    "        fontSize=12,\n",
    "        leading=14,\n",
    "        leftIndent=36,  # Overall left indent\n",
    "        firstLineIndent=-36,  # Creates hanging indent\n",
    "        alignment=TA_LEFT,\n",
    "        spaceAfter=12  # Space between references\n",
    "    )\n",
    "    \n",
    "    # Create header style\n",
    "    header_style = ParagraphStyle(\n",
    "        'Header',\n",
    "        parent=styles['Normal'],\n",
    "        fontName='Times-Bold',\n",
    "        fontSize=12,\n",
    "        spaceBefore=0,\n",
    "        spaceAfter=20,\n",
    "        alignment=TA_LEFT\n",
    "    )\n",
    "    \n",
    "    # Initialize story for the PDF\n",
    "    story = []\n",
    "    \n",
    "    # Add References header\n",
    "    story.append(Paragraph(\"References\", header_style))\n",
    "    \n",
    "    # Process references\n",
    "    if isinstance(references, str):\n",
    "        refs = clean_references(references)\n",
    "    else:\n",
    "        refs = references\n",
    "    \n",
    "    # Add each reference\n",
    "    for ref in refs:\n",
    "        if ref.strip():\n",
    "            # Clean and format the reference\n",
    "            ref = clean_reference(ref)\n",
    "            story.append(Paragraph(ref, ref_style))\n",
    "    \n",
    "    # Build PDF\n",
    "    doc.build(story)\n",
    "\n",
    "def clean_reference(ref):\n",
    "    \"\"\"\n",
    "    Cleans and formats a single reference.\n",
    "    \n",
    "    Args:\n",
    "        ref (str): Reference string to clean\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned reference\n",
    "    \"\"\"\n",
    "    # Remove line breaks and excess whitespace\n",
    "    ref = ' '.join(ref.split())\n",
    "    \n",
    "    # Remove TextBlock and other formatting markers\n",
    "    ref = re.sub(r'TextBlock\\(text=|type=\\'text\\'\\)|\\'|\\\\\\n|\\\\n', '', ref)\n",
    "    \n",
    "    # Fix spacing around periods in author names\n",
    "    ref = re.sub(r'\\.\\s*([A-Z])', r'. \\1', ref)\n",
    "    \n",
    "    # Fix spacing around ampersands\n",
    "    ref = re.sub(r'\\s*&\\s*', ' & ', ref)\n",
    "    \n",
    "    # Fix multiple spaces\n",
    "    ref = re.sub(r'\\s+', ' ', ref)\n",
    "    \n",
    "    # Remove asterisks around journal names while preserving italics in PDF\n",
    "    ref = re.sub(r'\\s*\\*([^*]+)\\*', r' \\1', ref)\n",
    "    \n",
    "    # Ensure proper spacing after commas\n",
    "    ref = re.sub(r',\\s*', ', ', ref)\n",
    "    \n",
    "    # Fix spacing around parentheses\n",
    "    ref = re.sub(r'\\s*\\(\\s*', ' (', ref)\n",
    "    ref = re.sub(r'\\s*\\)', ')', ref)\n",
    "    \n",
    "    # Ensure the reference ends with a period\n",
    "    ref = ref.rstrip('.')\n",
    "    ref += '.'\n",
    "    \n",
    "    return ref.strip()\n",
    "\n",
    "def clean_references(text):\n",
    "    \"\"\"\n",
    "    Cleans and splits reference text into individual references.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Full text containing references\n",
    "    \n",
    "    Returns:\n",
    "        list: List of cleaned references\n",
    "    \"\"\"\n",
    "    # First, standardize all newlines\n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    \n",
    "    # Remove formatting markers\n",
    "    text = re.sub(r'TextBlock\\(text=|type=\\'text\\'\\)|\\'', '', text)\n",
    "    \n",
    "    # Split into potential references\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Initialize variables\n",
    "    refs = []\n",
    "    current_ref = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Skip empty lines and headers\n",
    "        if not line or line.lower() == 'references':\n",
    "            continue\n",
    "            \n",
    "        # If line starts with a capital letter and previous reference exists,\n",
    "        # it's probably a new reference\n",
    "        if re.match(r'^[A-Z]', line) and current_ref:\n",
    "            refs.append(' '.join(current_ref))\n",
    "            current_ref = [line]\n",
    "        else:\n",
    "            current_ref.append(line)\n",
    "    \n",
    "    # Add the last reference\n",
    "    if current_ref:\n",
    "        refs.append(' '.join(current_ref))\n",
    "    \n",
    "    # Clean each reference\n",
    "    cleaned_refs = []\n",
    "    for ref in refs:\n",
    "        cleaned = clean_reference(ref)\n",
    "        if cleaned and not cleaned.isspace():\n",
    "            cleaned_refs.append(cleaned)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_refs = []\n",
    "    for ref in cleaned_refs:\n",
    "        if ref not in seen:\n",
    "            seen.add(ref)\n",
    "            unique_refs.append(ref)\n",
    "    \n",
    "    return unique_refs\n",
    "\n",
    "def get_formatted_references(prompt):\n",
    "    \"\"\"\n",
    "    Gets formatted references using the Anthropic Claude API.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send to Claude\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted references from Claude\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = anthropic.Anthropic(\n",
    "            api_key=\"enter API here\"\n",
    "        )\n",
    "        \n",
    "        # Make the API call\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=4000,\n",
    "            temperature=0,\n",
    "            system=\"You are a helpful research assistant with expertise in academic citations. Format references in proper APA style with full journal names, volumes, and page numbers.\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract and clean the content\n",
    "        if message and hasattr(message, 'content'):\n",
    "            content = message.content\n",
    "            if isinstance(content, list):\n",
    "                content = '\\n'.join(str(item) for item in content)\n",
    "            return content\n",
    "            \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"API Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def batch_process_files(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Process all text files in a directory and create corresponding reference PDFs.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Path to directory containing input text files\n",
    "        output_dir (str): Path to directory where PDFs will be saved\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Counter for processed files\n",
    "    processed = 0\n",
    "    errors = 0\n",
    "    \n",
    "    # Process each file in the input directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('_combined.txt'):\n",
    "            try:\n",
    "                # Construct full input path\n",
    "                input_path = os.path.join(input_dir, filename)\n",
    "                \n",
    "                # Create output filename\n",
    "                output_filename = filename.replace('_combined.txt', '_references.pdf')\n",
    "                output_path = os.path.join(output_dir, output_filename)\n",
    "                \n",
    "                # Read input file\n",
    "                with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                \n",
    "                # Create prompt for Claude\n",
    "                prompt = f\"\"\"Based on the following text, generate a reference list in APA format. \n",
    "                Format each reference exactly like these examples:\n",
    "\n",
    "                Leuz, C., & Verrecchia, R. E. (2000). The economic consequences of increased disclosure. Journal of Accounting Research, 91-124.\n",
    "\n",
    "                Bourveau, T., She, G., & Zaldokas, A. (2020). Corporate disclosure as a tacit coordination mechanism: Evidence from cartel enforcement regulations. Journal of Accounting Research, 58(2), 295-332.\n",
    "\n",
    "                Text for analysis:\n",
    "                {text}\n",
    "\n",
    "                Please format each reference following the exact style above, including:\n",
    "                1. Remove any asterisks, TextBlock tags, or other formatting markers \n",
    "                2. Author names with initials\n",
    "                3. Full title in sentence case\n",
    "                4. Journal name in italics (use *journal name* for italics)\n",
    "                5. Volume, issue, and page numbers where applicable\n",
    "                6. Year in parentheses\n",
    "                7. One reference per line \n",
    "                8. Subsequent references should be followed by a space after the previous reference\n",
    "                9. Sort alphabetically by author's last name\n",
    "                10. Provide only the references, no extra text or explanations\"\"\"\n",
    "\n",
    "                \n",
    "                # Get formatted references from Claude\n",
    "                formatted_refs = get_formatted_references(prompt)\n",
    "                \n",
    "                if formatted_refs:\n",
    "                    # Create the PDF with the formatted references\n",
    "                    create_reference_pdf(formatted_refs, output_path)\n",
    "                    processed += 1\n",
    "                    print(f\"Successfully processed: {filename}\")\n",
    "                else:\n",
    "                    errors += 1\n",
    "                    print(f\"Error getting references for: {filename}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Successfully processed: {processed} files\")\n",
    "    print(f\"Errors: {errors} files\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your input and output directories\n",
    "    input_directory = \"enter folder path here\"\n",
    "    output_directory = \"enter folder path here\"\n",
    "    \n",
    "    # Process all files\n",
    "    batch_process_files(input_directory, output_directory)\n",
    "\n",
    "# 16. Combine manuscript files with table files for descriptive statistics and regression analyses\n",
    "\n",
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfMerger\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_JUSTIFY, TA_LEFT, TA_CENTER\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "\n",
    "def get_descriptive_stats(desc_dir: str, law_name: str, mechanism: str) -> str:\n",
    "    \"\"\"Get descriptive statistics PDF from panel subfolder\"\"\"\n",
    "    # Convert law name to panel folder format\n",
    "    clean_mechanism = mechanism.replace(' ', '_')\n",
    "    panel_name = f\"panel_{law_name.replace('_', '')}_{clean_mechanism}\"\n",
    "    panel_dir = os.path.join(desc_dir, panel_name)\n",
    "    \n",
    "    # Look for descriptive_stats_table.pdf\n",
    "    desc_file = os.path.join(panel_dir, 'descriptive_stats_table.pdf')\n",
    "    \n",
    "    if os.path.exists(desc_file):\n",
    "        print(f\"Found descriptive statistics table: {desc_file}\")\n",
    "        return desc_file\n",
    "    else:\n",
    "        print(f\"Warning: No descriptive statistics table found in {panel_dir}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_regression_analyses(reg_dir: str, law_name: str, mechanism: str) -> str:\n",
    "    \"\"\"Get regression analyses PDF from panel subfolder\"\"\"\n",
    "    # Convert law name to panel folder format\n",
    "    clean_mechanism = mechanism.replace(' ', '_')\n",
    "    panel_name = f\"panel_{law_name.replace('_', '')}_{clean_mechanism}\"\n",
    "    panel_dir = os.path.join(reg_dir, panel_name)\n",
    "    \n",
    "    # Look for regression_table.pdf\n",
    "    reg_file = os.path.join(panel_dir, 'regression_table.pdf')\n",
    "    \n",
    "    if os.path.exists(reg_file):\n",
    "        print(f\"Found regression table: {reg_file}\")\n",
    "        return reg_file\n",
    "    else:\n",
    "        print(f\"Warning: No regression table found in {panel_dir}\")\n",
    "        return \"\"\n",
    "\n",
    "def merge_pdf_files(base_dir: str, law_name: str, mechanism: str):\n",
    "    \"\"\"Merge manuscript PDF with regression results, descriptive statistics, and reference PDFs\"\"\"\n",
    "    # Register Times New Roman font\n",
    "    try:\n",
    "        pdfmetrics.registerFont(TTFont('Times New Roman', 'times.ttf'))\n",
    "        pdfmetrics.registerFont(TTFont('Times New Roman Bold', 'timesbd.ttf'))\n",
    "    except:\n",
    "        print(\"Warning: Times New Roman font not found, using default font\")\n",
    "    \n",
    "    # Clean mechanism name for filenames\n",
    "    clean_mechanism = mechanism.replace(' ', '_')\n",
    "    \n",
    "    # Define file paths\n",
    "    combined_sections_dir = os.path.join(base_dir, 'combined_sections')\n",
    "    reg_dir = os.path.join(base_dir, 'regression_analyses')\n",
    "    desc_dir = os.path.join(base_dir, 'descriptive_stats')\n",
    "    corr_dir = os.path.join(base_dir, 'correlations')\n",
    "    ref_dir = os.path.join(base_dir, 'references')\n",
    "    output_dir = os.path.join(base_dir, 'final_manuscripts')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create intermediate PDF with formatting\n",
    "    temp_pdf_path = os.path.join(output_dir, f'temp_{law_name}_{clean_mechanism}.pdf')\n",
    "    temp_pdf = SimpleDocTemplate(\n",
    "        temp_pdf_path,\n",
    "        pagesize=letter,\n",
    "        rightMargin=72,\n",
    "        leftMargin=72,\n",
    "        topMargin=72,\n",
    "        bottomMargin=72\n",
    "    )\n",
    "\n",
    "    # Create styles\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    custom_title = ParagraphStyle(\n",
    "        name='CustomTitle',\n",
    "        fontName='Times New Roman Bold',\n",
    "        fontSize=16,\n",
    "        spaceAfter=16,\n",
    "        spaceBefore=24,\n",
    "        firstLineIndent=0,\n",
    "        alignment=TA_CENTER,\n",
    "        leading=24\n",
    "    )\n",
    "    \n",
    "    subtitle_style = ParagraphStyle(\n",
    "        name='CustomSubtitle',\n",
    "        fontName='Times New Roman',\n",
    "        fontSize=12,\n",
    "        spaceAfter=24,\n",
    "        spaceBefore=12,\n",
    "        firstLineIndent=0,\n",
    "        alignment=TA_CENTER,\n",
    "        leading=24\n",
    "    )\n",
    "    \n",
    "    abstract_style = ParagraphStyle(\n",
    "        name='Abstract',\n",
    "        fontName='Times New Roman',\n",
    "        fontSize=12,\n",
    "        firstLineIndent=0,\n",
    "        spaceAfter=60,\n",
    "        leading=14,  # Tighter line spacing for abstract\n",
    "        alignment=TA_JUSTIFY\n",
    "    )\n",
    "    \n",
    "    \n",
    "    regular_style = ParagraphStyle(\n",
    "        name='CustomRegular',\n",
    "        fontName='Times New Roman',\n",
    "        fontSize=12,\n",
    "        spaceAfter=12,\n",
    "        firstLineIndent=36,\n",
    "        leading=24,\n",
    "        alignment=TA_JUSTIFY\n",
    "    )\n",
    "    \n",
    "    heading_style = ParagraphStyle(\n",
    "        name='CustomHeading',\n",
    "        fontName='Times New Roman',\n",
    "        fontSize=12,\n",
    "        spaceAfter=24,\n",
    "        spaceBefore=24,\n",
    "        firstLineIndent=0,\n",
    "        alignment=TA_LEFT,\n",
    "        leading=24\n",
    "    )\n",
    "\n",
    "    # Read manuscript\n",
    "    manuscript_file = os.path.join(combined_sections_dir, f\"{law_name}_{clean_mechanism}_combined.txt\")\n",
    "    with open(manuscript_file, 'r', encoding='utf-8') as f:\n",
    "        manuscript_text = f.read()\n",
    "\n",
    "    # Create story (content)\n",
    "    story = []\n",
    "    \n",
    "    # Format law name\n",
    "    formatted_law_name = \" \".join(re.findall(r'[A-Z][^A-Z]*', law_name))\n",
    "\n",
    "    # Add title and subtitle\n",
    "    title = f\"{formatted_law_name} and Voluntary Disclosure\"\n",
    "    story.append(Paragraph(title, custom_title))\n",
    "    story.append(Paragraph(\"Artemis Intelligencia\", subtitle_style))\n",
    "    story.append(Paragraph(\"February 1, 2025\", subtitle_style))\n",
    "    story.append(Spacer(1, 24))\n",
    "\n",
    "    # Add manuscript content (skip the original title)\n",
    "    sections = manuscript_text.split('\\n\\n')\n",
    "    for section in sections[1:]:  # Skip the first section which contains the old title\n",
    "        if section.strip():\n",
    "            if '=' in section:  # Section heading\n",
    "                heading = section.split('\\n')[0]\n",
    "                story.append(Paragraph(heading, heading_style))\n",
    "            else:\n",
    "                paragraphs = section.split('\\n')\n",
    "                for paragraph in paragraphs:\n",
    "                    if paragraph.strip():\n",
    "                        story.append(Paragraph(paragraph, regular_style))\n",
    "\n",
    "    # Create the intermediate PDF\n",
    "    temp_pdf.build(story)\n",
    "\n",
    "    # Merge PDFs\n",
    "    merger = PdfMerger()\n",
    "    \n",
    "    try:\n",
    "        # Add formatted manuscript\n",
    "        merger.append(temp_pdf_path)\n",
    "        \n",
    "        # Add references\n",
    "        ref_file = os.path.join(ref_dir, f\"{law_name}_{clean_mechanism}_references.pdf\")\n",
    "        if os.path.exists(ref_file):\n",
    "            merger.append(ref_file)\n",
    "            print(f\"Added references from: {ref_file}\")\n",
    "        else:\n",
    "            print(f\"No references file found at: {ref_file}\")\n",
    "    \n",
    "        # Add descriptive statistics table\n",
    "        desc_stats_path = get_descriptive_stats(desc_dir, law_name, mechanism)\n",
    "        if desc_stats_path:\n",
    "            merger.append(desc_stats_path)\n",
    "        else:\n",
    "            print(f\"No descriptive statistics table found for {law_name}_{mechanism}\")\n",
    "            \n",
    "        #Add correlations table\n",
    "        corr_file= os.path.join(corr_dir, f\"{law_name}_{clean_mechanism}_correlation_table.pdf\")\n",
    "        if os.path.exists(corr_file):\n",
    "            merger.append(corr_file)\n",
    "            print(f\"Added correlation table from: {corr_file}\")\n",
    "        else:\n",
    "            print(f\"No correlation file found at: {corr_file}\")\n",
    "    \n",
    "        # Add regression table\n",
    "        reg_table_path = get_regression_analyses(reg_dir, law_name, mechanism)\n",
    "        if reg_table_path:\n",
    "            merger.append(reg_table_path)\n",
    "        else:\n",
    "            print(f\"No regression table found for {law_name}_{mechanism}\")\n",
    "\n",
    "        # Save final merged PDF\n",
    "        output_file = os.path.join(output_dir, f\"{'_'.join(law_name.split('_'))} and Voluntary Disclosure_{clean_mechanism}_final.pdf\")\n",
    "        merger.write(output_file)\n",
    "        merger.close()\n",
    "        \n",
    "        # Clean up temporary file\n",
    "        os.remove(temp_pdf_path)\n",
    "        \n",
    "        print(f\"Successfully created formatted PDF for {law_name} - {mechanism}\")\n",
    "        print(f\"Saved to: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating PDF: {str(e)}\")\n",
    "    finally:\n",
    "        merger.close()\n",
    "\n",
    "# Batch processing of multiple laws and mechanisms\n",
    "def batch_merge_pdfs(base_dir):\n",
    "    # Define the list of laws and mechanisms\n",
    "    laws_mechanisms = [\n",
    "(\"Law 1\", \"Mechanism 1\"),\n",
    "(\"Law 1\", \"Mechanism 2\"),\n",
    "(\"Law 2\", \"Mechanism 1\"),\n",
    "(\"Other laws\", \"Other mechanisms\")\n",
    "    ]\n",
    "    \n",
    "    for law, mechanism in laws_mechanisms:\n",
    "        try:\n",
    "            merge_pdf_files(base_dir, law, mechanism)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {law} - {mechanism}: {str(e)}\")\n",
    "\n",
    "\n",
    "BASE_DIR = \"enter folder path here\"\n",
    "batch_merge_pdfs(BASE_DIR)\n",
    "\n",
    "\n",
    "# 17. Add page numbers \n",
    "import os\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter\n",
    "import io\n",
    "\n",
    "def add_page_numbers(input_path, output_path):\n",
    "    reader = PdfReader(input_path)\n",
    "    writer = PdfWriter()\n",
    "    \n",
    "    for i in range(len(reader.pages)):\n",
    "        page = reader.pages[i]\n",
    "        packet = io.BytesIO()\n",
    "        can = canvas.Canvas(packet, pagesize=(page.mediabox.width, page.mediabox.height))\n",
    "        \n",
    "        if i > 0:\n",
    "            can.setFont('Times-Roman', 12)\n",
    "            can.drawString(page.mediabox.width/2 - 6, 40, str(i))\n",
    "        \n",
    "        can.save()\n",
    "        packet.seek(0)\n",
    "        number_pdf = PdfReader(packet)\n",
    "        \n",
    "        if len(number_pdf.pages) > 0:\n",
    "            page.merge_page(number_pdf.pages[0])\n",
    "        writer.add_page(page)\n",
    "    \n",
    "    with open(output_path, \"wb\") as output_file:\n",
    "        writer.write(output_file)\n",
    "\n",
    "def batch_process_pdfs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    pdfs = [f for f in os.listdir(input_dir) if f.endswith('.pdf')]\n",
    "    \n",
    "    for i, filename in enumerate(pdfs, 1):\n",
    "        print(f\"\\nProcessing {i}/{len(pdfs)}: {filename}\")\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, f\"numbered_{filename}\")\n",
    "        try:\n",
    "            add_page_numbers(input_path, output_path)\n",
    "            print(f\"Successfully processed: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "\n",
    "input_dir = \"enter folder path here\"\n",
    "output_dir = \"enter folder path here\"\n",
    "batch_process_pdfs(input_dir, output_dir)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
