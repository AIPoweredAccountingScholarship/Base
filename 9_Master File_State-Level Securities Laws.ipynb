{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code was developed in collaboration with Claude4-Sonnet\n",
    "#Claude was used for code generation, documentation, and error handling\n",
    "#Note: the code will run best when executing each section separately\n",
    "#Required inputs: CSV files with securities law data, data with \"GVKEY\", \"FYEAR\", and \"STATE\" as identifiers\n",
    "\n",
    "# 1. Claude identify federal securities laws\n",
    "import os\n",
    "import anthropic\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def get_securities_laws(conversation_history=None):\n",
    "    # Initialize the Anthropic client\n",
    "    client = anthropic.Anthropic(\n",
    "        api_key=\"enter API key here\"\n",
    "    )\n",
    "    \n",
    "    # Phase 1: Data Collection\n",
    "    initial_content = \"\"\"Your task is to identify and compile a comprehensive database of at least 1000 state laws. \n",
    "\n",
    "IMPORTANT: Only identify new state laws. Exclude amendments, updates, or revisions to existing rules. \n",
    "Focus on major new laws only.\n",
    "\n",
    "IMPORTANT: Do not include laws with titles containing the following words: \"Amendment\", \"Update\", or \"Revision\"\n",
    "\n",
    "IMPORTANT: Provide laws evenly distributed across ALL years from 2000-2025:\n",
    "- At least 200 laws from 2000-2005\n",
    "- At least 200 laws from 2006-2010\n",
    "- At least 200 laws from 2011-2015\n",
    "- At least 200 laws from 2016-2020\n",
    "- At least 200 laws from 2021-2025\n",
    "\n",
    "The goal is to create a dataset that captures the following key details for each law. \n",
    "\n",
    "Data Fields to Collect:\n",
    "- Date: The announcement or implementation date of the law (use YYYY-MM-DD format).\n",
    "- Regulation Title or Name: The official name or designation of the regulatory change.\n",
    "- Regulatory Body/Authority: The jurisdiction/entity responsible for the law.\n",
    "- Description: A brief overview of the law, including key provisions and the rationale behind it.\n",
    "- Impact: The potential or observed effects on industries, markets, or stakeholders.\n",
    "- Litigation Risk: Is this law related to the risk of litigation against managers? By risk of litigation we mean \n",
    "  the probability that a manager will be sued or face legal action because of this law. Answer this question with \n",
    "  Yes or No. If yes, label the entry \"Litigation Risk\".\n",
    "- Corporate Governance: Is this law related to corporate governance of firms? Corporate governance refers to the\n",
    "  internal monitoring system charged with overseeing managers and commonly focuses on matters such as board \n",
    "  independence or insider trading policy. Answer this question with Yes or No. If yes, label the entry \n",
    "  \"Corporate Governance\".\n",
    "- Proprietary Costs: Is this law related to proprietary costs of firms? By proprietary costs, we mean costs that result\n",
    "  from the disclosure of information to competitors which could harm a firm's competitive position. \n",
    "  Answer this question with Yes or No. If yes, label the entry \"Proprietary Costs\".\n",
    "- Information Asymmetry: Is this law related to information asymmetry between owners and managers? By information\n",
    "  asymmetry we mean that one party has more or better information than the other party. Answer this question with\n",
    "  Yes or No. If yes, label the entry \"Information Asymmetry\".\n",
    "- Unsophisticated Investors: Is the law related to protecting unsophisticated investors? By unsophisticated investors, \n",
    "  we mean investors that are either new to investing or are not well informed. Answer this question with Yes or No. \n",
    "  If yes, label the entry \"Unsophisticated Investors\".\n",
    "- Equity Issuance in Public vs. Private Markets: Is this law related to the costs and benefits of issuing equity in \n",
    "  public versus private markets? Answer this question with Yes or No. If yes, label the entry \"Equity Issuance in \n",
    "  Public vs. Private Markets\".\n",
    "- Reputation Risk: Is this law related to the reputation of firm managers? By reputation of firm manager, we mean\n",
    "  the career prospects and prestige of an individual manager. Answer this question with Yes or No. If yes, label the \n",
    "  entry \"Reputation Risk\".\n",
    "- State: The state where the law was announced or implemented. Answer this question with the state's abbreviation \n",
    "  (e.g., FL for Florida, or TX for Texas)\n",
    "- References: Links to official documents or credible news sources.\n",
    "\n",
    "Requirements:\n",
    "- Include laws from various years (2000-2025), not just recent ones.\n",
    "- Scope: Cover as many laws as possible that were announced or implemented in the last 25 years.\n",
    "- Consistency: Ensure uniform formatting for all entries in the dataset.\n",
    "- Dates must be in YYYY-MM-DD format (e.g., 2002-07-30).\n",
    "\n",
    "Output:\n",
    "Provide data in a tabular format with rows for each law and columns for the data fields listed above. \n",
    "Use credible, authoritative sources such as state government websites, legal databases, academic journals, \n",
    "or credible news sources.\n",
    "Do not include duplicate laws.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        if conversation_history:\n",
    "            messages = conversation_history\n",
    "        else:\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": initial_content\n",
    "            }]\n",
    "\n",
    "        response = client.messages.create(\n",
    "            max_tokens=8000,\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            temperature=0.5,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.content[0].text, messages + [\n",
    "            {\"role\": \"assistant\", \"content\": response.content[0].text}\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error making API call: {e}\")\n",
    "        return None, messages\n",
    "\n",
    "def categorize_laws(conversation_history):\n",
    "    \"\"\"Phase 2: Ask Claude to group similar laws together\"\"\"\n",
    "    client = anthropic.Anthropic(\n",
    "        api_key=\"enter API key here\"\n",
    "    )\n",
    "    \n",
    "    # Phase 2: Categorization\n",
    "    categorization_prompt = \"\"\"Review the 1000 state laws you identified in the previous step. Your task is to group similar laws together.\n",
    "Requirements:\n",
    "- Here's an example of similar laws: \"Idaho Personal Information Protection Act\" and \n",
    "  \"Nevada Privacy of Customer Information Act\".\n",
    "- Here's an example of laws that are not similar: \"Vermont Renewable Energy Standard\" and \n",
    "  \"Florida Investor Protection Act\"\n",
    "- Create categories of related laws. Do not assign category names yet. \n",
    "\n",
    "Output:\n",
    "For each law, identify which other laws in the dataset are similar to it. Create groupings of related laws without\n",
    "assigning category names yet.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        messages = conversation_history + [{\"role\": \"user\", \"content\": categorization_prompt}]\n",
    "        response = client.messages.create(\n",
    "            max_tokens=8000,\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            temperature=0.5,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.content[0].text, messages + [\n",
    "            {\"role\": \"assistant\", \"content\": response.content[0].text}\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error making API call for categorization: {e}\")\n",
    "        return None, messages\n",
    "\n",
    "def name_categories(conversation_history):\n",
    "    \"\"\"Phase 3: Ask Claude to assign category names\"\"\"\n",
    "    client = anthropic.Anthropic(\n",
    "        api_key=\"enter API key here\"\n",
    "    )\n",
    "    \n",
    "    # Phase 3: Naming Categories \n",
    "    naming_prompt = \"\"\"Based on the groupings you created in the previous step, assign clear and descriptive category \n",
    "    names to each group of similar laws.\n",
    "\n",
    "Requirements:\n",
    "- Category names should be concise \n",
    "- Names should capture the primary objective of the regulation\n",
    "- Use standard legal/regulatory terminology where appropriate (e.g., corporate tax law, securities regulation, \n",
    "  environmental regulation, labor and employment law)\n",
    "\n",
    "Output:\n",
    "Provide a final table with ALL 100+ laws including ALL of the following columns:\n",
    "- Date (in YYYY-MM-DD format)\n",
    "- Regulation Title or Name\n",
    "- Regulatory Body/Authority\n",
    "- Description\n",
    "- Impact\n",
    "- Litigation Risk (Yes/No)\n",
    "- Corporate Governance (Yes/No)\n",
    "- Proprietary Costs (Yes/No)\n",
    "- Information Asymmetry (Yes/No)\n",
    "- Unsophisticated Investors (Yes/No)\n",
    "- Equity Issuance in Public vs. Private Markets (Yes/No)\n",
    "- Reputation Risk (Yes/No)\n",
    "- Law Category (assigned category name)\n",
    "- State (abbreviation)\n",
    "- References\n",
    "\n",
    "IMPORTANT: Include ALL data fields from the original collection, not just the law names and categories. \n",
    "Every row must have all columns filled in with the original data you collected.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        messages = conversation_history + [{\"role\": \"user\", \"content\": naming_prompt}]\n",
    "        response = client.messages.create(\n",
    "            max_tokens=8000,  \n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            temperature=0.5,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.content[0].text, messages + [\n",
    "            {\"role\": \"assistant\", \"content\": response.content[0].text}\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error making API call for naming: {e}\")\n",
    "        return None, messages\n",
    "\n",
    "def add_follow_up_prompt(conversation_history, follow_up_prompt):\n",
    "    \"\"\"Add a follow-up prompt to the conversation history\"\"\"\n",
    "    return conversation_history + [{\"role\": \"user\", \"content\": follow_up_prompt}]\n",
    "\n",
    "def standardize_date(date_str):\n",
    "    \"\"\"Attempt to standardize date format to YYYY-MM-DD\"\"\"\n",
    "    try:\n",
    "        # Convert to datetime and then back to string in desired format\n",
    "        return pd.to_datetime(date_str).strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        # If conversion fails, return original string\n",
    "        return date_str\n",
    "\n",
    "def parse_table_fallback(response_text: str) -> pd.DataFrame:\n",
    "    \"\"\"Fallback parser for when Claude returns table format instead of numbered format.\"\"\"\n",
    "    print(\"Debugging table parsing...\")\n",
    "    \n",
    "    lines = response_text.split('\\n')\n",
    "    table_lines = [line.strip() for line in lines if line.strip().startswith('|') and len(line.strip()) > 5]\n",
    "    \n",
    "    print(f\"Found {len(table_lines)} table lines\")\n",
    "    \n",
    "    if len(table_lines) < 2:\n",
    "        print(\"Not enough table lines found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Remove separator lines (containing ---)\n",
    "    data_lines = [line for line in table_lines if '---' not in line]\n",
    "    print(f\"Found {len(data_lines)} data lines (after removing separators)\")\n",
    "    \n",
    "    if len(data_lines) < 2:\n",
    "        print(\"Not enough data lines after removing separators\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Parse header line\n",
    "    header_line = data_lines[0]\n",
    "    raw_headers = header_line.split('|')\n",
    "    headers = [col.strip() for col in raw_headers if col.strip()]\n",
    "    \n",
    "    print(f\"Original headers ({len(headers)}): {headers}\")\n",
    "    \n",
    "    # Parse data rows\n",
    "    data_rows = []\n",
    "    for i, line in enumerate(data_lines[1:], 1):\n",
    "        raw_columns = line.split('|')\n",
    "        columns = [col.strip() for col in raw_columns if col.strip()]\n",
    "        \n",
    "        if len(columns) == len(headers):\n",
    "            data_rows.append(columns)\n",
    "            print(f\"Row {i}: ✓ Added ({len(columns)} columns)\")\n",
    "        else:\n",
    "            print(f\"Row {i}: ✗ Skipped - {len(columns)} columns vs {len(headers)} headers\")\n",
    "    \n",
    "    print(f\"Successfully parsed {len(data_rows)} data rows\")\n",
    "    \n",
    "    if not data_rows:\n",
    "        print(\"No valid data rows found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create DataFrame with original headers first\n",
    "    df = pd.DataFrame(data_rows, columns=headers)\n",
    "    print(f\"Created DataFrame with columns: {list(df.columns)}\")\n",
    "\n",
    "    # Make the mapping more robust for title column\n",
    "    for col in df.columns:\n",
    "        if 'title' in col.lower() or 'name' in col.lower():\n",
    "            df = df.rename(columns={col: 'Regulation Title'})\n",
    "            print(f\"Mapped column '{col}' to 'Regulation Title'\")\n",
    "            break\n",
    "            \n",
    "    # Now standardize the column names to match expected format\n",
    "    # Create a mapping from the table headers to standard column names\n",
    "    standard_columns = {\n",
    "        'Date': 'Date',\n",
    "        'Regulation Title/Name': 'Regulation Title', \n",
    "        'Regulatory Body': 'Regulatory Body',\n",
    "        'Regulatory Body/Authority': 'Regulatory Body',  # Handle variation\n",
    "        'Description': 'Description',\n",
    "        'Impact': 'Impact',\n",
    "        'Litigation Risk': 'Litigation Risk',\n",
    "        'Corporate Governance': 'Corporate Governance', \n",
    "        'Proprietary Costs': 'Proprietary Costs',\n",
    "        'Information Asymmetry': 'Information Asymmetry',\n",
    "        'Unsophisticated Investors': 'Unsophisticated Investors',\n",
    "        'Equity Issuance Public vs Private': 'Equity Issuance',\n",
    "        'Equity Issuance in Public vs. Private Markets': 'Equity Issuance',  # Handle variation\n",
    "        'Reputation Risk': 'Reputation Risk',\n",
    "        'Law Category': 'Law Category',\n",
    "        'State':'State',\n",
    "        'References': 'References'\n",
    "    }\n",
    "    \n",
    "    # Rename columns using the mapping\n",
    "    df_renamed = df.rename(columns=standard_columns)\n",
    "    \n",
    "    # Ensure all required columns exist\n",
    "    required_columns = [\n",
    "        'Date', 'Regulation Title', 'Regulatory Body', 'Description', 'Impact',\n",
    "        'Litigation Risk', 'Corporate Governance', 'Proprietary Costs', \n",
    "        'Information Asymmetry', 'Unsophisticated Investors', 'Equity Issuance',\n",
    "        'Reputation Risk','Law Category','State', 'References'\n",
    "    ]\n",
    "    \n",
    "    # Add missing columns with None values\n",
    "    for col in required_columns:\n",
    "        if col not in df_renamed.columns:\n",
    "            print(f\"Adding missing column: {col}\")\n",
    "            df_renamed[col] = None\n",
    "    \n",
    "    # Select only the required columns in the correct order\n",
    "    final_df = df_renamed[required_columns].copy()\n",
    "    \n",
    "    # Standardize date format\n",
    "    if 'Date' in final_df.columns:\n",
    "        print(\"Standardizing dates...\")\n",
    "        final_df['Date'] = final_df['Date'].apply(lambda x: standardize_date(x) if pd.notna(x) and str(x).strip() else x)\n",
    "    \n",
    "    # Clean up any completely empty rows\n",
    "    final_df = final_df.dropna(how='all')\n",
    "    \n",
    "    print(f\"Final DataFrame: {len(final_df)} rows x {len(final_df.columns)} columns\")\n",
    "    print(f\"Final columns: {list(final_df.columns)}\")\n",
    "    \n",
    "    return final_df\n",
    "    \n",
    "def parse_response_to_dataframe(response_text: str) -> pd.DataFrame:\n",
    "    \"\"\"Parse the response text into a pandas DataFrame.\"\"\"\n",
    "    print(\"\\nParsing response...\")\n",
    "    \n",
    "    data = []\n",
    "    current_entry = None\n",
    "    entry_number = None\n",
    "    \n",
    "    lines = [line.strip() for line in response_text.split('\\n') if line.strip()]\n",
    "    \n",
    "    for line in lines:\n",
    "        # Updated regex to match \"**Entry X:**\" format\n",
    "        number_match = re.match(r'^\\*?\\*?(?:Entry )?(\\d+)[:\\.]?\\*?\\*?$', line.strip())\n",
    "        if number_match:\n",
    "            if current_entry and len(current_entry) > 0:\n",
    "                if 'Regulation Title' not in current_entry and entry_number:\n",
    "                    current_entry['Regulation Title'] = f\"Law {entry_number}\"\n",
    "                data.append(current_entry)\n",
    "            current_entry = {}\n",
    "            entry_number = number_match.group(1)\n",
    "            continue\n",
    "\n",
    "            \n",
    "        if ':' in line and current_entry is not None:\n",
    "            key, value = [x.strip() for x in line.split(':', 1)]\n",
    "            \n",
    "            key_mapping = {\n",
    "                'Date': 'Date',\n",
    "                'Title': 'Regulation Title',\n",
    "                'Authority': 'Regulatory Body',\n",
    "                'Description': 'Description',\n",
    "                'Impact': 'Impact',\n",
    "                'Litigation Risk': 'Litigation Risk',\n",
    "                'Corporate Governance': 'Corporate Governance',\n",
    "                'Proprietary Costs': 'Proprietary Costs',\n",
    "                'Information Asymmetry': 'Information Asymmetry',\n",
    "                'Unsophisticated Investors': 'Unsophisticated Investors',\n",
    "                'Equity Issuance': 'Equity Issuance',\n",
    "                'Equity Issuance in Public vs. Private Markets': 'Equity Issuance', \n",
    "                'Reputation Risk': 'Reputation Risk',\n",
    "                'Law Category':'Law Category',\n",
    "                'State':'State',\n",
    "                'References': 'References'\n",
    "            }\n",
    "            \n",
    "            if key in key_mapping:\n",
    "                column_name = key_mapping[key]\n",
    "                if column_name == 'Date':\n",
    "                    current_entry[column_name] = standardize_date(value)\n",
    "                else:\n",
    "                    current_entry[column_name] = value.strip()\n",
    "\n",
    "    if current_entry and len(current_entry) > 0:\n",
    "        if 'Regulation Title' not in current_entry and entry_number:\n",
    "            current_entry['Regulation Title'] = f\"Law {entry_number}\"\n",
    "        data.append(current_entry)\n",
    "    \n",
    "    print(f\"\\nFound {len(data)} entries in numbered format\")\n",
    "    \n",
    "    # If numbered format parsing found data, use it\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        required_columns = ['Date', 'Regulation Title', 'Regulatory Body', 'Description', 'Impact',\n",
    "                          'Litigation Risk', 'Corporate Governance', 'Proprietary Costs',\n",
    "                          'Information Asymmetry', 'Unsophisticated Investors', 'Equity Issuance',\n",
    "                          'Reputation Risk','Law Category','State','References']\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                print(f\"Adding missing column: {col}\")\n",
    "                df[col] = None\n",
    "        \n",
    "        df['Regulation Title'] = df['Regulation Title'].fillna('Unknown')\n",
    "        df['Regulation Title'] = df['Regulation Title'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "        \n",
    "        df['dedup_key'] = df.apply(lambda row: f\"{row['Date']}_{row['Regulation Title']}\", axis=1)\n",
    "        df = df.drop_duplicates(subset=['dedup_key'], keep='first')\n",
    "        df = df.drop('dedup_key', axis=1)\n",
    "        \n",
    "        df = df[required_columns]\n",
    "        print(f\"Created DataFrame with {len(df)} rows\")\n",
    "        return df.copy()\n",
    "    \n",
    "    # If numbered format failed, try table format as fallback\n",
    "    print(\"Numbered format parsing failed, trying table format...\")\n",
    "    df = parse_table_fallback(response_text)\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(f\"Table format parsing succeeded with {len(df)} rows\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Both parsing methods failed - no valid data to create DataFrame\")\n",
    "        return pd.DataFrame()\n",
    "                \n",
    "def compile_all_responses() -> pd.DataFrame:\n",
    "    \"\"\"Compile multiple API responses into a single DataFrame.\"\"\"\n",
    "    all_phase1_responses = []\n",
    "    conversation_history = None\n",
    "\n",
    "    # PHASE 1: Get initial response (data collection)\n",
    "    print(\"\\n=== PHASE 1: Data Collection ===\")\n",
    "    initial_response, conversation_history = get_securities_laws()\n",
    "    if initial_response:\n",
    "        print(\"\\nInitial response:\")\n",
    "        print(initial_response)\n",
    "        all_phase1_responses.append(initial_response)\n",
    "\n",
    "        follow_up_prompts = [\n",
    "            \"\"\"Starting with number {last_num}, list 20 more state laws using this exact format for each:\n",
    "Date: YYYY-MM-DD\n",
    "Title: [title]\n",
    "Authority: [body]\n",
    "Description: [brief]\n",
    "Impact: [impact]\n",
    "Litigation Risk: Yes/No\n",
    "Corporate Governance: Yes/No\n",
    "Proprietary Costs: Yes/No\n",
    "Information Asymmetry: Yes/No\n",
    "Unsophisticated Investors: Yes/No\n",
    "Equity Issuance: Yes/No\n",
    "Reputation Risk: Yes/No\n",
    "State: [state abbreviation]\n",
    "References: [link]\"\"\",\n",
    "\n",
    "            \"Continue from number {last_num}. Provide 20 more laws using the exact same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "             \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"List 20 more laws starting at number {last_num}. Use the same format.\",\n",
    "            \"Add 20 more laws beginning with number {last_num}. Keep the same format.\",\n",
    "            \"Provide 20 more laws from number {last_num}. Same format.\",\n",
    "            \"Recall that you have to identify at least 1000 state laws.\"\n",
    "        ]\n",
    "        last_num = len(parse_response_to_dataframe(initial_response)) + 1\n",
    "        \n",
    "        for i, prompt_template in enumerate(follow_up_prompts, 1):\n",
    "            prompt = prompt_template.format(last_num=last_num)\n",
    "            conversation_history = add_follow_up_prompt(conversation_history, prompt)\n",
    "            response, conversation_history = get_securities_laws(conversation_history)\n",
    "            \n",
    "            if response:\n",
    "                print(f\"\\nFollow-up response {i}:\")\n",
    "                print(response)\n",
    "                all_phase1_responses.append(response)\n",
    "                df = parse_response_to_dataframe(response)\n",
    "                last_num += len(df)\n",
    "    \n",
    "    # Parse all Phase 1 responses into a DataFrame\n",
    "    phase1_dfs = []\n",
    "    for response in all_phase1_responses:\n",
    "        df = parse_response_to_dataframe(response)\n",
    "        if not df.empty:\n",
    "            phase1_dfs.append(df)\n",
    "    \n",
    "    if not phase1_dfs:\n",
    "        print(\"No valid data frames were created in Phase 1!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Concatenate Phase 1 data\n",
    "    phase1_df = pd.concat(phase1_dfs, ignore_index=True)\n",
    "    print(f\"\\nPhase 1 complete: {len(phase1_df)} laws collected\")\n",
    "    \n",
    "    # PHASE 2: Categorization\n",
    "    print(\"\\n=== PHASE 2: Categorization ===\")\n",
    "    categorization_response, conversation_history = categorize_laws(conversation_history)\n",
    "    if categorization_response:\n",
    "        print(\"\\nCategorization response:\")\n",
    "        print(categorization_response)\n",
    "    \n",
    "    # PHASE 3: Get just the law titles and categories\n",
    "    print(\"\\n=== PHASE 3: Naming Categories ===\")\n",
    "    naming_prompt = \"\"\"Based on the groupings you created, provide a simple table with just 3 columns:\n",
    "1. Regulation Title (exact title from original data)\n",
    "2. Law Category (assigned category name)\n",
    "3. State (abbreviation)\n",
    "\n",
    "Format as a markdown table. Include all 1000+ laws.\"\"\"\n",
    "    \n",
    "    conversation_history = conversation_history + [{\"role\": \"user\", \"content\": naming_prompt}]\n",
    "    client = anthropic.Anthropic(\n",
    "        api_key=\"enter API key here\"\n",
    "    )\n",
    "    response = client.messages.create(\n",
    "        max_tokens=8000,\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        temperature=0.5,\n",
    "        messages=conversation_history\n",
    "    )\n",
    "    naming_response = response.content[0].text\n",
    "    \n",
    "    if naming_response:\n",
    "        print(\"\\nNaming response:\")\n",
    "        print(naming_response)\n",
    "        \n",
    "        # Parse the category mapping\n",
    "        category_df = parse_table_fallback(naming_response)\n",
    "        \n",
    "        if not category_df.empty and 'Law Category' in category_df.columns:\n",
    "            # Merge categories with Phase 1 data\n",
    "            # Clean titles for matching\n",
    "            phase1_df['Title_clean'] = phase1_df['Regulation Title'].str.lower().str.strip()\n",
    "            category_df['Title_clean'] = category_df['Regulation Title'].str.lower().str.strip()\n",
    "            \n",
    "            # Merge on cleaned title\n",
    "            final_df = phase1_df.merge(\n",
    "                category_df[['Title_clean', 'Law Category']], \n",
    "                on='Title_clean',\n",
    "                how='left'\n",
    "            )\n",
    "            final_df = final_df.drop('Title_clean', axis=1)\n",
    "        else:\n",
    "            print(\"Warning: Could not parse categories, proceeding without them\")\n",
    "            final_df = phase1_df\n",
    "    else:\n",
    "        final_df = phase1_df\n",
    "\n",
    "    # Remove duplicates\n",
    "    final_df['Title_clean'] = final_df['Regulation Title'].fillna('').str.lower().str.strip()\n",
    "    final_df['Description_clean'] = final_df['Description'].fillna('').str.lower().str.strip()\n",
    "    final_df['dedup_key'] = final_df.apply(\n",
    "        lambda row: f\"{row['Date']}_{row['Title_clean']}_{row['Description_clean'][:50]}\", \n",
    "        axis=1\n",
    "    )\n",
    "    final_df = final_df.drop_duplicates(subset=['dedup_key'], keep='first')\n",
    "    final_df = final_df.drop(['Title_clean', 'Description_clean', 'dedup_key'], axis=1)\n",
    "\n",
    "    # Sort by date\n",
    "    try:\n",
    "        final_df['DateSort'] = pd.to_datetime(final_df['Date'], errors='coerce')\n",
    "        final_df = final_df.dropna(subset=['DateSort'])\n",
    "        final_df = final_df.sort_values('DateSort', ascending=False)\n",
    "        final_df = final_df.drop('DateSort', axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not sort by date due to: {e}\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Compile all responses into a DataFrame\n",
    "    df = compile_all_responses()\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"\\nError: No data was collected!\")\n",
    "    else:\n",
    "        # Display basic statistics\n",
    "        print(f\"\\nTotal number of unique laws: {len(df)}\")\n",
    "        print(\"\\nMost recent laws:\")\n",
    "        print(df.head().to_string())\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_path = 'enter file path here'\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nDatabase saved to: {output_path}\")\n",
    "\n",
    "# 2. Add column for Year \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"enter file path here\")\n",
    "\n",
    "# Clean parentheses and dashes from text columns\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    df[column] = df[column].str.replace('(', '').str.replace(')', '').str.replace('-', '')\n",
    "    \n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df\n",
    "\n",
    "#Excluding years prior to 2002 2020 and 2021 since we don't have forecast data. We also exclude years 2018, 2019 from law file because we need 2 years after and we have data\n",
    "#up to 2019\n",
    "filtered_df = df[~df['Year'].isin([1986, 1987, 1988, 1989,1990, 1991, 1992, 1993, 1994, 1995, 1996,\n",
    "                                   1997, 1998, 1999, 2000, 2001,2018, 2019, 2020, 2021, 2022, 2023, 2024])]\n",
    "\n",
    "filtered_df_with_titles = filtered_df.dropna(subset=[\"Regulatory Body\",\"Law Category_y\"])\n",
    "\n",
    "filtered_df_with_titles.to_csv(\"enter file path here\")\n",
    "\n",
    "# 3. Create Panel Datasets for Each Law and Each Channel\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def add_underscores_before_capitals(text):\n",
    "    \"\"\"Add underscores before capital letters in a string\"\"\"\n",
    "    return re.sub(r'(?<=[a-z0-9])(?=[A-Z])', '_', text)\n",
    "\n",
    "def create_staggered_treatment_variable(laws_df: pd.DataFrame, panel_df: pd.DataFrame, \n",
    "                                       category: str, channel: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates staggered treatment variable based on when each state first implements \n",
    "    a law in the specified category and channel.\n",
    "    \"\"\"\n",
    "    # Filter laws by category and channel\n",
    "    relevant_laws = laws_df[\n",
    "        (laws_df['Law Category_y'] == category) & \n",
    "        (laws_df[channel].str.strip().str.lower() == 'yes')\n",
    "    ].copy()\n",
    "    \n",
    "    if relevant_laws.empty:\n",
    "        print(f\"No laws found for {category} - {channel}\")\n",
    "        return None\n",
    "    \n",
    "    # For each state, find the FIRST year they implemented this type of law\n",
    "    state_treatment_year = relevant_laws.groupby('State')['Year'].min().reset_index()\n",
    "    state_treatment_year.columns = ['State', 'treatment_year']\n",
    "    \n",
    "    # Create a copy of panel data\n",
    "    panel_with_treatment = panel_df.copy()\n",
    "    \n",
    "    # Merge treatment year information\n",
    "    panel_with_treatment = panel_with_treatment.merge(\n",
    "        state_treatment_year, \n",
    "        left_on='State_ba', \n",
    "        right_on='State', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Create treatment variables\n",
    "    # treated = 1 if state ever gets treated (has a treatment_year)\n",
    "    panel_with_treatment['treated'] = (~panel_with_treatment['treatment_year'].isna()).astype(int)\n",
    "    \n",
    "    # post = 1 if current year >= treatment year\n",
    "    panel_with_treatment['post'] = (\n",
    "        panel_with_treatment['FYEAR'] >= panel_with_treatment['treatment_year']\n",
    "    ).astype(int)\n",
    "    panel_with_treatment.loc[panel_with_treatment['treatment_year'].isna(), 'post'] = 0\n",
    "    \n",
    "    # treatment_effect = treated * post (the DiD estimator)\n",
    "    panel_with_treatment['treatment_effect'] = (\n",
    "        panel_with_treatment['treated'] * panel_with_treatment['post']\n",
    "    )\n",
    "    \n",
    "    # Add relative time variable\n",
    "    panel_with_treatment['years_since_treatment'] = (\n",
    "        panel_with_treatment['FYEAR'] - panel_with_treatment['treatment_year']\n",
    "    )\n",
    "    panel_with_treatment.loc[panel_with_treatment['treatment_year'].isna(), 'years_since_treatment'] = None\n",
    "    \n",
    "    # Add category and channel info\n",
    "    panel_with_treatment['law_category'] = category\n",
    "    panel_with_treatment['channel'] = channel\n",
    "    \n",
    "    # Drop the intermediate 'State' column from merge\n",
    "    if 'State' in panel_with_treatment.columns:\n",
    "        panel_with_treatment = panel_with_treatment.drop('State', axis=1)\n",
    "    \n",
    "    return panel_with_treatment\n",
    "\n",
    "def create_channel_panels(laws_file: str, panel_file: str, output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates staggered DiD panel datasets for each category-channel combination.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read input files\n",
    "    laws_df = pd.read_csv(laws_file)\n",
    "    panel_df = pd.read_csv(panel_file)\n",
    "    \n",
    "    # Convert Year columns to int\n",
    "    laws_df['Year'] = pd.to_numeric(laws_df['Year'])\n",
    "    panel_df['FYEAR'] = pd.to_numeric(panel_df['FYEAR'])\n",
    "    \n",
    "    #Filter panel to 2000-2016\n",
    "    panel_df = panel_df[(panel_df['FYEAR'] >= 2000) & (panel_df['FYEAR'] <= 2016)]\n",
    "    print(f\"Filtered panel to 2000-2016: {len(panel_df)} observations\")\n",
    "    \n",
    "    # Define channels\n",
    "    channels = [\n",
    "        'Litigation Risk',\n",
    "        'Corporate Governance',\n",
    "        'Proprietary Costs',\n",
    "        'Information Asymmetry',\n",
    "        'Unsophisticated Investors',\n",
    "        'Equity Issuance',\n",
    "        'Reputation Risk'\n",
    "    ]\n",
    "    \n",
    "    # ONLY process \"Investor Protection\" category\n",
    "    category = \"Securities Enforcement\"\n",
    "    \n",
    "    # Process each channel for Investor Protection\n",
    "    for channel in channels:\n",
    "        try:\n",
    "            panel_with_treatment = create_staggered_treatment_variable(\n",
    "                laws_df, panel_df, category, channel\n",
    "            )\n",
    "            \n",
    "            if panel_with_treatment is not None:\n",
    "                # Create safe filename\n",
    "                safe_category = category.replace(' ', '_').replace('/', '_')\n",
    "                safe_channel = channel.replace(' ', '_')\n",
    "                \n",
    "                output_file = os.path.join(\n",
    "                    output_dir, \n",
    "                    f\"panel_{safe_category}_{safe_channel}.csv\"\n",
    "                )\n",
    "                \n",
    "                panel_with_treatment.to_csv(output_file, index=False)\n",
    "                \n",
    "                # Print summary statistics\n",
    "                n_treated = panel_with_treatment['treated'].sum()\n",
    "                n_treated_post = panel_with_treatment['treatment_effect'].sum()\n",
    "                print(f\"Created: panel_{safe_category}_{safe_channel}.csv\")\n",
    "                print(f\"  Treated observations: {n_treated}\")\n",
    "                print(f\"  Treatment effect observations: {n_treated_post}\\n\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {category} - {channel}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    laws_file = \"enter file path here\"\n",
    "    panel_file = \"enter file path here\"\n",
    "    output_dir = \"enter folder path here\"\n",
    "    \n",
    "    # === DIAGNOSTIC CODE  ===\n",
    "    laws_df = pd.read_csv(laws_file)\n",
    "    panel_df = pd.read_csv(panel_file)\n",
    "    \n",
    "    print(\"=== DIAGNOSTIC: Checking Iowa ===\")\n",
    "    print(\"\\nIowa in laws dataset:\")\n",
    "    print(laws_df[laws_df['State'] == 'IA'][['State', 'Year', 'Regulation Title']])\n",
    "    \n",
    "    print(\"\\nAll unique states in panel dataset (State_ba column):\")\n",
    "    print(sorted(panel_df['State_ba'].unique()))\n",
    "    \n",
    "    print(\"\\n=== Checking Iowa Investor Protection Laws ===\")\n",
    "    iowa_investor = laws_df[(laws_df['State'] == 'IA') & (laws_df['Law Category_y'] == 'Investor Protection')]\n",
    "    print(f\"Iowa Investor Protection laws:\\n{iowa_investor[['State', 'Year', 'Regulation Title', 'Information Asymmetry', 'Litigation Risk']]}\")\n",
    "    \n",
    "    print(\"\\nLooking for Iowa in panel:\")\n",
    "    iowa_matches = panel_df[panel_df['State_ba'].str.contains('Iowa|IA', case=False, na=False)]['State_ba'].unique()\n",
    "    print(f\"Found: {iowa_matches}\")\n",
    "    \n",
    "    print(\"\\n=== END DIAGNOSTIC ===\\n\")\n",
    "    # === END DIAGNOSTIC CODE ===\n",
    "    \n",
    "    # Original function call\n",
    "    create_channel_panels(laws_file, panel_file, output_dir)\n",
    "    \n",
    "    print(\"\\nStaggered DiD panel creation complete!\")\n",
    "    \n",
    "# 4. Add year fixed effects, run regression analyses and save regression tables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant\n",
    "from linearmodels.iv import AbsorbingLS\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from fpdf import FPDF\n",
    "import traceback\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "\n",
    "def add_underscores_before_capitals(text):\n",
    "    \"\"\"Add underscores before capital letters in a string\"\"\"\n",
    "    return re.sub(r'(?<=[a-z0-9])(?=[A-Z])', '_', text)\n",
    "\n",
    "class RegressionAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize regression analyzer\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _get_significance_stars(self, pvalue: float) -> str:\n",
    "        \"\"\"Get significance stars based on p-value.\"\"\"\n",
    "        if pvalue < 0.01:\n",
    "            return \"***\"\n",
    "        elif pvalue < 0.05:\n",
    "            return \"**\"\n",
    "        elif pvalue < 0.1:\n",
    "            return \"*\"\n",
    "        return \"\"\n",
    "\n",
    "    #def filter_event_window(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "      #  \"\"\"Filter data to ±2 years around regulation year\"\"\"\n",
    "        #try:\n",
    "            #regulation_year = int(df['Year'].iloc[0])\n",
    "            #return df[\n",
    "                #(df['FYEAR'] >= regulation_year - 2) &\n",
    "                #(df['FYEAR'] <= regulation_year + 2)\n",
    "            #]\n",
    "        #except KeyError as e:\n",
    "            #print(f\"Missing column in DataFrame: {e}\")\n",
    "            #raise\n",
    "        #except Exception as e:\n",
    "            #print(f\"Error during filtering: {e}\")\n",
    "            #raise\n",
    "\n",
    "    def prepare_fixed_effects_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare fixed effects columns for AbsorbingLS\"\"\"\n",
    "        df_prep = df.copy()\n",
    "        \n",
    "        # Ensure categorical variables for fixed effects\n",
    "        df_prep['firm_id'] = df_prep['GVKEY'].astype('category')\n",
    "        df_prep['year_id'] = df_prep['FYEAR'].astype('category')\n",
    "        \n",
    "        print(f\"Number of firms for fixed effects: {len(df_prep['firm_id'].unique())}\")\n",
    "        print(f\"Number of years for fixed effects: {len(df_prep['year_id'].unique())}\")\n",
    "        \n",
    "        return df_prep\n",
    "\n",
    "    def run_regressions(self, df: pd.DataFrame) -> dict:\n",
    "        \"\"\"Run multiple regression specifications with and without fixed effects\"\"\"\n",
    "        results_dict = {}\n",
    "        \n",
    "        # Prepare data for fixed effects\n",
    "        df_prep = self.prepare_fixed_effects_data(df.copy())\n",
    "        \n",
    "        specifications = {\n",
    "            '(1)': {\n",
    "                'dep_var': 'freqMF',\n",
    "                'controls': [],\n",
    "                'method': 'OLS',\n",
    "                'absorb': None\n",
    "            },\n",
    "            '(2)': {\n",
    "                'dep_var': 'freqMF',\n",
    "                'controls': ['linstown', 'lsize', 'lbtm', 'lroa', 'lsaret12', 'levol', 'lloss', 'lcalrisk'],\n",
    "                'method': 'OLS',\n",
    "                'absorb': None\n",
    "            },\n",
    "            '(3)': {\n",
    "                'dep_var': 'freqMF',\n",
    "                'controls': ['linstown', 'lsize', 'lbtm', 'lroa', 'lsaret12', 'levol', 'lloss', 'lcalrisk'],\n",
    "                'method': 'AbsorbingLS',\n",
    "                'absorb': ['firm_id', 'year_id']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for spec_name, spec in specifications.items():\n",
    "            print(f\"\\nRunning regression for specification {spec_name}\")\n",
    "            try:\n",
    "                dep_var = spec['dep_var']\n",
    "                controls = spec.get('controls', [])\n",
    "                variables = controls + ['treatment_effect']\n",
    "\n",
    "                # Clean data\n",
    "                print(\"Getting required columns...\")\n",
    "                required_columns = variables + [dep_var, 'GVKEY']\n",
    "                \n",
    "                # Add fixed effects columns if needed\n",
    "                if spec.get('absorb'):\n",
    "                    for absorb_var in spec['absorb']:\n",
    "                        if absorb_var not in required_columns:\n",
    "                            required_columns.append(absorb_var)\n",
    "                \n",
    "                # Check for missing columns\n",
    "                if not all(col in df_prep.columns for col in required_columns):\n",
    "                    missing_cols = [col for col in required_columns if col not in df_prep.columns]\n",
    "                    print(f\"Missing columns: {missing_cols}\")\n",
    "                    raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "            \n",
    "                reg_data = df_prep[required_columns].copy()\n",
    "                reg_data = reg_data.replace([np.inf, -np.inf], np.nan)\n",
    "                reg_data = reg_data.dropna()\n",
    "            \n",
    "                print(f\"Observations: {len(reg_data)}\")\n",
    "                print(f\"Number of unique firms: {len(reg_data['GVKEY'].unique())}\")\n",
    "\n",
    "                # Prepare dependent and independent variables\n",
    "                y = reg_data[dep_var]\n",
    "                X = reg_data[variables]\n",
    "\n",
    "                if spec['method'] == 'OLS':\n",
    "                    # Standard OLS for specifications without fixed effects\n",
    "                    X_with_const = add_constant(X)\n",
    "                    model = OLS(y, X_with_const)\n",
    "                    # Fit with clustered standard errors at firm level\n",
    "                    results = model.fit(cov_type='cluster', cov_kwds={'groups': reg_data['GVKEY']})\n",
    "\n",
    "                    # Store results\n",
    "                    results_dict[spec_name] = {\n",
    "                        'coefficients': results.params.to_dict(),\n",
    "                        'pvalues': results.pvalues.to_dict(),\n",
    "                        't_stats': (results.params / results.bse).to_dict(),\n",
    "                        'r_squared': results.rsquared,\n",
    "                        'n_obs': int(results.nobs),\n",
    "                        'n_firms': len(reg_data['GVKEY'].unique()),\n",
    "                        'controls': controls,\n",
    "                        'fixed_effects': {\n",
    "                            'firm': False,\n",
    "                            'year': False\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                elif spec['method'] == 'AbsorbingLS':\n",
    "                    # Use AbsorbingLS for high-dimensional fixed effects\n",
    "                    absorb_vars = spec['absorb']\n",
    "                    print(f\"Absorbing fixed effects: {absorb_vars}\")\n",
    "                    \n",
    "                    # Ensure absorption variables are always passed as a DF\n",
    "                    absorb_data = reg_data[absorb_vars]\n",
    "                    \n",
    "                    # Create the AbsorbingLS model\n",
    "                    model = AbsorbingLS(\n",
    "                        dependent=y,\n",
    "                        exog=X,\n",
    "                        absorb=absorb_data\n",
    "                    )\n",
    "                    \n",
    "                    # Fit with clustered standard errors at firm level\n",
    "                    results = model.fit(cov_type='clustered', clusters=reg_data['GVKEY'])\n",
    "                    \n",
    "                    results_dict[spec_name] = {\n",
    "                        'coefficients': results.params.to_dict(),\n",
    "                        'pvalues': results.pvalues.to_dict(),\n",
    "                        't_stats': results.tstats.to_dict(),\n",
    "                        'r_squared': results.rsquared,\n",
    "                        'n_obs': int(results.nobs),\n",
    "                        'n_firms': len(reg_data['GVKEY'].unique()),\n",
    "                        'controls': controls,\n",
    "                        'fixed_effects': {\n",
    "                            'firm': 'firm_id' in absorb_vars,\n",
    "                            'year': 'year_id' in absorb_vars\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                print(f\"Successfully completed regression for specification {spec_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in specification {spec_name}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        if not results_dict:\n",
    "            raise ValueError(\"No successful regressions completed\")\n",
    "        \n",
    "        return results_dict\n",
    "\n",
    "    def save_regression_table_as_pdf(self, results: dict, regulation_title: str, output_path: str):\n",
    "        \"\"\"Save regression table as PDF matching the target format for 3 columns\"\"\"\n",
    "        try:\n",
    "            pdf = FPDF(format='A4', orientation='L')  # Landscape for better fit\n",
    "            pdf.set_margins(15, 15, 15)\n",
    "            pdf.add_page()\n",
    "            \n",
    "            # Title - all in bold Times New Roman\n",
    "            try:\n",
    "                # First try Windows standard folder for Times New Roman\n",
    "                pdf.add_font('Times New Roman', '', r'C:\\Windows\\Fonts\\times.ttf', uni=True)\n",
    "                pdf.add_font('Times New Roman', 'B', r'C:\\Windows\\Fonts\\timesbd.ttf', uni=True)\n",
    "                pdf.set_font('Times New Roman', 'B', 11)\n",
    "            except:\n",
    "                try:\n",
    "                    # Try alternative paths for Times New Roman\n",
    "                    pdf.add_font('Times New Roman', '', 'times.ttf', uni=True)\n",
    "                    pdf.add_font('Times New Roman', 'B', 'timesbd.ttf', uni=True)\n",
    "                    pdf.set_font('Times New Roman', 'B', 11)\n",
    "                except:\n",
    "                    print(\"Times New Roman font not found, using Arial Bold\")\n",
    "                    pdf.set_font('Arial', 'B', 11)\n",
    "\n",
    "            # Both title and table number in bold\n",
    "            pdf.cell(0, 8, \"Table 3\", ln=True, align='C')\n",
    "            pdf.cell(0, 8, f\"The Impact of {regulation_title} on Management Forecast Frequency\", ln=True, align='C')\n",
    "            pdf.ln(3)\n",
    "\n",
    "            # Switch back to regular font for table content\n",
    "            try:\n",
    "                pdf.set_font('Times New Roman', '', 10)\n",
    "            except:\n",
    "                pdf.set_font('Arial', '', 10)\n",
    "            \n",
    "            # Calculate column widths for 3 columns (Variable name + 3 specifications)\n",
    "            # Landscape A4: 297mm width - 30mm margins = 267mm available\n",
    "            first_col_width = 70  # Wider for variable names since we have fewer columns\n",
    "            col_width = (pdf.w - 30 - first_col_width) / 3  # Divide remaining space by 3\n",
    "\n",
    "            # Table header\n",
    "            pdf.cell(first_col_width, 7, \"\", 1)\n",
    "            for i in range(1, 4):  # 3 specifications: (1), (2), (3)\n",
    "                pdf.cell(col_width, 7, f\"({i})\", 1, align='C')\n",
    "            pdf.ln()\n",
    "\n",
    "            # Treatment Effect\n",
    "            pdf.cell(first_col_width, 7, \"Treatment Effect\", 1)\n",
    "            for i in range(1, 4):  # Only loop through (1), (2), (3)\n",
    "                spec = f'({i})'\n",
    "                if spec in results:\n",
    "                    coef = results[spec]['coefficients']['treatment_effect']\n",
    "                    tstat = abs(results[spec]['t_stats']['treatment_effect'])\n",
    "                    stars = self._get_significance_stars(results[spec]['pvalues']['treatment_effect'])\n",
    "                    pdf.cell(col_width, 7, f\"{coef:.4f}{stars} ({tstat:.2f})\", 1, align='C')\n",
    "                else:\n",
    "                    pdf.cell(col_width, 7, \"\", 1, align='C')\n",
    "            pdf.ln()\n",
    "\n",
    "            # Control variables for specifications (2) and (3)\n",
    "            control_labels = {\n",
    "                'linstown': 'Institutional ownership',\n",
    "                'lsize': 'Firm size',\n",
    "                'lbtm': 'Book-to-market',\n",
    "                'lroa': 'ROA',\n",
    "                'lsaret12': 'Stock return',\n",
    "                'levol': 'Earnings volatility',\n",
    "                'lloss': 'Loss',\n",
    "                'lcalrisk': 'Class action litigation risk'\n",
    "            }\n",
    "\n",
    "            for var, label in control_labels.items():\n",
    "                pdf.cell(first_col_width, 7, label, 1)\n",
    "                for i in range(1, 4):  # Only loop through (1), (2), (3)\n",
    "                    spec = f'({i})'\n",
    "                    if spec in results and var in results[spec]['coefficients']:\n",
    "                        coef = results[spec]['coefficients'][var]\n",
    "                        tstat = abs(results[spec]['t_stats'][var])\n",
    "                        stars = self._get_significance_stars(results[spec]['pvalues'][var])\n",
    "                        pdf.cell(col_width, 7, f\"{coef:.4f}{stars} ({tstat:.2f})\", 1, align='C')\n",
    "                    else:\n",
    "                        pdf.cell(col_width, 7, \"\", 1, align='C')\n",
    "                pdf.ln()\n",
    "\n",
    "            # Fixed effects rows\n",
    "            pdf.cell(first_col_width, 7, \"Firm fixed effects\", 1)\n",
    "            for i in range(1, 4):  # Only loop through (1), (2), (3)\n",
    "                spec = f'({i})'\n",
    "                if spec in results:\n",
    "                    text = \"Yes\" if results[spec]['fixed_effects']['firm'] else \"No\"\n",
    "                    pdf.cell(col_width, 7, text, 1, align='C')\n",
    "                else:\n",
    "                    pdf.cell(col_width, 7, \"No\", 1, align='C')\n",
    "            pdf.ln()\n",
    "\n",
    "            pdf.cell(first_col_width, 7, \"Year fixed effects\", 1)\n",
    "            for i in range(1, 4):  # Only loop through (1), (2), (3)\n",
    "                spec = f'({i})'\n",
    "                if spec in results:\n",
    "                    text = \"Yes\" if results[spec]['fixed_effects']['year'] else \"No\"\n",
    "                    pdf.cell(col_width, 7, text, 1, align='C')\n",
    "                else:\n",
    "                    pdf.cell(col_width, 7, \"No\", 1, align='C')\n",
    "            pdf.ln()\n",
    "\n",
    "            # N and R²\n",
    "            for stat in ['N', 'R²']:\n",
    "                pdf.cell(first_col_width, 7, stat, 1)\n",
    "                for i in range(1, 4):  # Only loop through (1), (2), (3)\n",
    "                    spec = f'({i})'\n",
    "                    if spec in results:\n",
    "                        value = results[spec]['n_obs'] if stat == 'N' else results[spec]['r_squared']\n",
    "                        text = f\"{value:,}\" if stat == 'N' else f\"{value:.4f}\"\n",
    "                        pdf.cell(col_width, 7, text, 1, align='C')\n",
    "                    else:\n",
    "                        pdf.cell(col_width, 7, \"\", 1, align='C')\n",
    "                pdf.ln()\n",
    "\n",
    "            # Notes\n",
    "            pdf.ln(10)\n",
    "            pdf.set_font('Times', size=10)\n",
    "            notes = \"Notes: t-statistics in parentheses. *, **, and *** represent significance at the 10%, 5%, and 1% level, respectively.\"\n",
    "            pdf.multi_cell(0, 5, notes)\n",
    "\n",
    "            pdf.output(output_path)\n",
    "            print(f\"PDF saved at {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving PDF: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def analyze_panel(self, panel_file: str, output_dir: str):\n",
    "        \"\"\"Analyze a single panel dataset\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nStarting analysis of {os.path.basename(panel_file)}...\")\n",
    "            print(\"Reading data...\")\n",
    "            df = pd.read_csv(panel_file)\n",
    "            \n",
    "            #print(\"Filtering event window...\")\n",
    "            #df_filtered = self.filter_event_window(df)\n",
    "            \n",
    "            print(\"Running regressions...\")\n",
    "            results = self.run_regressions(df)\n",
    "            \n",
    "            print(\"Saving results...\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Save data and results\n",
    "            print(\"Saving filtered data...\")\n",
    "            df.to_csv(os.path.join(output_dir, 'analysis_data.csv'), index=False)\n",
    "            with open(os.path.join(output_dir, 'regression_results.json'), 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            \n",
    "            # Save regression table\n",
    "            table_path = os.path.join(output_dir, 'regression_table.pdf')\n",
    "            self.save_regression_table_as_pdf(\n",
    "                results,\n",
    "                df['Regulation Title'].iloc[0],\n",
    "                table_path\n",
    "            )\n",
    "            print(f\"Saved regression table to {table_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing panel: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "def analyze_all_panels(input_dir: str, output_dir: str):\n",
    "    \"\"\"Analyze all panel datasets in a directory\"\"\"\n",
    "    analyzer = RegressionAnalyzer()\n",
    "    panel_files = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "    total_files = len(panel_files)\n",
    "    successful_runs = 0\n",
    "    failed_runs = 0\n",
    "    print(f\"\\nFound {total_files} panel files to analyze\")\n",
    "    \n",
    "    for i, panel_file in enumerate(panel_files, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing panel {i} of {total_files}: {os.path.basename(panel_file)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Get the base filename without extension\n",
    "            base_filename = os.path.basename(panel_file).replace('.csv', '')\n",
    "            \n",
    "            # Apply the same naming convention (add underscores before capitals)\n",
    "            formatted_filename = add_underscores_before_capitals(base_filename)\n",
    "            \n",
    "            analyzer.analyze_panel(panel_file, os.path.join(output_dir, formatted_filename))\n",
    "            successful_runs += 1\n",
    "            print(f\"Successfully processed panel {i}\")\n",
    "        except Exception as e:\n",
    "            failed_runs += 1\n",
    "            print(f\"Failed to process panel {i}: {str(e)}\")\n",
    "            \n",
    "        # Print progress summary\n",
    "        print(f\"\\nProgress Summary:\")\n",
    "        print(f\"Processed: {i}/{total_files} ({(i/total_files)*100:.1f}%)\")\n",
    "        print(f\"Successful: {successful_runs}\")\n",
    "        print(f\"Failed: {failed_runs}\")\n",
    "\n",
    "\n",
    "def process_significance(base_dir, delete_nonsig=False):\n",
    "    \"\"\"\n",
    "    Process panels based on significance and either delete or move non-significant results.\n",
    "    t-stat >= 1.96 is considered significant.\n",
    "    \"\"\"\n",
    "    # Create directory for non-significant results if not deleting\n",
    "    if not delete_nonsig:\n",
    "        nonsig_dir = os.path.join(os.path.dirname(base_dir), 'nonsignificant_results')\n",
    "        os.makedirs(nonsig_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all panel directories\n",
    "    panel_dirs = glob.glob(os.path.join(base_dir, 'panel_*'))\n",
    "    print(f\"Found {len(panel_dirs)} panel directories\")\n",
    "    \n",
    "    # Track results\n",
    "    significant_count = 0\n",
    "    not_significant_count = 0\n",
    "    \n",
    "    # Process each panel\n",
    "    for panel_dir in panel_dirs:\n",
    "        panel_name = os.path.basename(panel_dir)\n",
    "        json_file = os.path.join(panel_dir, 'regression_results.json')\n",
    "        \n",
    "        try:\n",
    "            # Read regression results\n",
    "            with open(json_file, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Check specification (3) since that's the firm FE specification\n",
    "            if '(3)' in results:\n",
    "                t_stat = abs(results['(3)']['t_stats']['treatment_effect'])\n",
    "                is_significant = t_stat >= 1.96\n",
    "                \n",
    "                if is_significant:\n",
    "                    print(f\"{panel_name}: t-stat = {t_stat:.2f} (significant - keeping)\")\n",
    "                    significant_count += 1\n",
    "                else:\n",
    "                    print(f\"{panel_name}: t-stat = {t_stat:.2f} (not significant - {'deleting' if delete_nonsig else 'moving'})\")\n",
    "                    if delete_nonsig:\n",
    "                        shutil.rmtree(panel_dir)\n",
    "                    else:\n",
    "                        shutil.move(panel_dir, os.path.join(nonsig_dir, panel_name))\n",
    "                    not_significant_count += 1\n",
    "            else:\n",
    "                print(f\"{panel_name}: No specification (3) found - {'deleting' if delete_nonsig else 'moving'}\")\n",
    "                if delete_nonsig:\n",
    "                    shutil.rmtree(panel_dir)\n",
    "                else:\n",
    "                    shutil.move(panel_dir, os.path.join(nonsig_dir, panel_name))\n",
    "                not_significant_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {panel_name}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    total_processed = significant_count + not_significant_count\n",
    "    if total_processed > 0:\n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"Total panels processed: {total_processed}\")\n",
    "        print(f\"Significant results: {significant_count} ({(significant_count/total_processed)*100:.1f}%)\")\n",
    "        print(f\"Not significant results: {not_significant_count} ({(not_significant_count/total_processed)*100:.1f}%)\")\n",
    "        if not delete_nonsig:\n",
    "            print(f\"\\nNon-significant results moved to: {nonsig_dir}\")\n",
    "        else:\n",
    "            print(\"\\nNon-significant results deleted\")\n",
    "    else:\n",
    "        print(\"No panels processed successfully\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    INPUT_DIR = r\"enter folder path here\"\n",
    "    OUTPUT_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    # Run analysis on all panels\n",
    "    analyze_all_panels(INPUT_DIR, OUTPUT_DIR)\n",
    "    \n",
    "    # ========== ADD THIS SECTION ==========\n",
    "    # Generate missing PDFs from existing regression results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Generating missing PDFs...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    panels = [\n",
    "        \"panel_Securities_Enforcement_Equity_Issuance\",\n",
    "        \"panel_Securities_Enforcement_Information_Asymmetry\",\n",
    "        \"panel_Securities_Enforcement_Litigation_Risk\",\n",
    "        \"panel_Securities_Enforcement_Reputation_Risk\",\n",
    "        \"panel_Securities_Enforcement_Unsophisticated_Investors\"\n",
    "    ]\n",
    "    \n",
    "    analyzer = RegressionAnalyzer()\n",
    "    \n",
    "    for panel in panels:\n",
    "        results_file = os.path.join(OUTPUT_DIR, panel, \"regression_results.json\")\n",
    "        output_pdf = os.path.join(OUTPUT_DIR, panel, \"regression_table.pdf\")\n",
    "        \n",
    "        try:\n",
    "            with open(results_file, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Extract title from panel name\n",
    "            title = \"Securities Enforcement\" \n",
    "            \n",
    "            analyzer.save_regression_table_as_pdf(results, title, output_pdf)\n",
    "            print(f\"✓ Created PDF for {panel}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to create PDF for {panel}: {e}\")\n",
    "    \n",
    "    print(\"\\nAll PDFs generated!\")   \n",
    "    \n",
    "# 5. Check and keep significant results \n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def process_significance(base_dir, delete_nonsig=False):\n",
    "    \"\"\"\n",
    "    Process panels based on significance and either delete or move non-significant results.\n",
    "    t-stat >= 1.96 is considered significant.\n",
    "    \"\"\"\n",
    "    # Create directory for non-significant results if not deleting\n",
    "    if not delete_nonsig:\n",
    "        nonsig_dir = os.path.join(os.path.dirname(base_dir), 'nonsignificant_results')\n",
    "        os.makedirs(nonsig_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all panel directories\n",
    "    panel_dirs = glob.glob(os.path.join(base_dir, 'panel_*'))\n",
    "    print(f\"Found {len(panel_dirs)} panel directories\")\n",
    "    \n",
    "    # Track results\n",
    "    significant_count = 0\n",
    "    not_significant_count = 0\n",
    "    \n",
    "    # Process each panel\n",
    "    for panel_dir in panel_dirs:\n",
    "        panel_name = os.path.basename(panel_dir)\n",
    "        json_file = os.path.join(panel_dir, 'regression_results.json')\n",
    "        \n",
    "        try:\n",
    "            # Read regression results\n",
    "            with open(json_file, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Check specification (3)\n",
    "            if '(3)' in results:\n",
    "                t_stat = abs(results['(3)']['t_stats']['treatment_effect'])\n",
    "                is_significant = t_stat >= 1.96\n",
    "                \n",
    "                if is_significant:\n",
    "                    print(f\"{panel_name}: t-stat = {t_stat:.2f} (significant - keeping)\")\n",
    "                    significant_count += 1\n",
    "                else:\n",
    "                    print(f\"{panel_name}: t-stat = {t_stat:.2f} (not significant - {'deleting' if delete_nonsig else 'moving'})\")\n",
    "                    if delete_nonsig:\n",
    "                        shutil.rmtree(panel_dir)\n",
    "                    else:\n",
    "                        shutil.move(panel_dir, os.path.join(nonsig_dir, panel_name))\n",
    "                    not_significant_count += 1\n",
    "            else:\n",
    "                print(f\"{panel_name}: No specification (3) found - {'deleting' if delete_nonsig else 'moving'}\")\n",
    "                if delete_nonsig:\n",
    "                    shutil.rmtree(panel_dir)\n",
    "                else:\n",
    "                    shutil.move(panel_dir, os.path.join(nonsig_dir, panel_name))\n",
    "                not_significant_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {panel_name}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    total_processed = significant_count + not_significant_count\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Total panels processed: {total_processed}\")\n",
    "    print(f\"Significant results: {significant_count} ({(significant_count/total_processed)*100:.1f}%)\")\n",
    "    print(f\"Not significant results: {not_significant_count} ({(not_significant_count/total_processed)*100:.1f}%)\")\n",
    "    if not delete_nonsig:\n",
    "        print(f\"\\nNon-significant results moved to: {nonsig_dir}\")\n",
    "    else:\n",
    "        print(\"\\nNon-significant results deleted\")\n",
    "\n",
    "# Usage\n",
    "base_dir = r\"enter folder path here\"\n",
    "\n",
    "# Choose whether to delete (True) or move (False) non-significant results\n",
    "delete_nonsig = False  # Change to True to delete instead of move\n",
    "process_significance(base_dir, delete_nonsig)\n",
    "\n",
    "# 6. Ask Claude to write a background, theoretical framework, and hypothesis development section\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from anthropic import Anthropic\n",
    "import glob\n",
    "\n",
    "class ComprehensiveAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    def get_laws_with_regression_results(self, csv_file: str, regression_dir: str) -> list:\n",
    "        \"\"\"Read laws from CSV and identify categories with regression results\"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "    \n",
    "        # Get all panel directories\n",
    "        panel_dirs = glob.glob(os.path.join(regression_dir, 'panel_*'))\n",
    "    \n",
    "        # Extract categories and channels that have regression results\n",
    "        categories_with_results = {}\n",
    "    \n",
    "        for panel_dir in panel_dirs:\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            # Format: panel_Investor_Protection_Information_Asymmetry\n",
    "            parts = panel_name.replace('panel_', '').split('_')\n",
    "        \n",
    "            # Reconstruct category (e.g., \"Securities_Enforcement\" -> \"Securities Enforcement\")\n",
    "            # Channels are the known list\n",
    "            channels = ['Litigation_Risk', 'Corporate_Governance', 'Proprietary_Costs', \n",
    "                       'Information_Asymmetry', 'Unsophisticated_Investors', \n",
    "                       'Equity_Issuance', 'Reputation_Risk']\n",
    "        \n",
    "            for channel in channels:\n",
    "                if panel_name.endswith(channel):\n",
    "                    # Everything before the channel is the category\n",
    "                    category_part = panel_name.replace('panel_', '').replace(f'_{channel}', '')\n",
    "                    category = category_part.replace('_', ' ')\n",
    "                    channel_name = channel.replace('_', ' ')\n",
    "                \n",
    "                    if category not in categories_with_results:\n",
    "                        categories_with_results[category] = []\n",
    "                    if channel_name not in categories_with_results[category]:\n",
    "                        categories_with_results[category].append(channel_name)\n",
    "                    break\n",
    "    \n",
    "        print(f\"Found categories with regression results: {categories_with_results}\")\n",
    "    \n",
    "        # For each category, get ALL laws in that category from CSV\n",
    "        category_laws = {}\n",
    "    \n",
    "        for category in categories_with_results.keys():\n",
    "            # Filter CSV for this category\n",
    "            category_df = df[df['Law Category_y'] == category]\n",
    "        \n",
    "            laws_list = []\n",
    "            for _, row in category_df.iterrows():\n",
    "                law_info = {\n",
    "                    'title': row['Regulation Title'],\n",
    "                    'year': row['Year'],\n",
    "                    'state': row['State'],\n",
    "                    'body': row['Regulatory Body'],\n",
    "                    'description': row['Description'],\n",
    "                    'impact': row['Impact']\n",
    "                }\n",
    "                laws_list.append(law_info)\n",
    "        \n",
    "            if laws_list:\n",
    "                category_laws[category] = {\n",
    "                    'laws': laws_list,\n",
    "                    'channels': categories_with_results[category]\n",
    "                }\n",
    "                print(f\"\\n{category}: {len(laws_list)} laws across {len(laws_list)} states\")\n",
    "    \n",
    "        return category_laws\n",
    "    \n",
    "    def _law_names_similar(self, csv_name: str, panel_name: str) -> bool:\n",
    "        \"\"\"Check if two law names are similar enough to be considered a match\"\"\"\n",
    "        # Remove underscores and convert to lowercase for comparison\n",
    "        csv_clean = csv_name.replace('_', '').lower()\n",
    "        panel_clean = panel_name.replace('_', '').lower()\n",
    "        \n",
    "        # Check for exact match or substantial overlap\n",
    "        return (csv_clean == panel_clean or \n",
    "                csv_clean in panel_clean or \n",
    "                panel_clean in csv_clean or\n",
    "                self._calculate_similarity(csv_clean, panel_clean) > 0.8)\n",
    "    \n",
    "    def _calculate_similarity(self, str1: str, str2: str) -> float:\n",
    "        \"\"\"Calculate simple similarity between two strings\"\"\"\n",
    "        if not str1 or not str2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Simple overlap-based similarity\n",
    "        set1 = set(str1)\n",
    "        set2 = set(str2)\n",
    "        intersection = len(set1 & set2)\n",
    "        union = len(set1 | set2)\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        # Remove headers (##, ###, etc.)\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove bold/italic formatting\n",
    "        content = re.sub(r'\\*{2,4}([^*]+?)\\*{2,4}', r'\\1', content)\n",
    "        content = re.sub(r'\\*([^*]+?)\\*', r'\\1', content)\n",
    "        \n",
    "        # Clean up any remaining asterisks\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        \n",
    "        return content\n",
    "\n",
    "    def get_background_hypothesis(self, category: str, laws: list, mechanism: str) -> str:\n",
    "        \"\"\"Get background, theoretical framework, and hypothesis development for a category of laws\"\"\"\n",
    "        \n",
    "        # Format laws list for prompt\n",
    "        laws_text = \"\\n\".join([\n",
    "            f\"- {law['title']} ({law['state']}, {law['year']}): {law['description']}\"\n",
    "            for law in laws\n",
    "        ])\n",
    "    \n",
    "        prompt = f\"\"\"You are an accounting academic writing a research paper examining state-level {category} laws and\n",
    "        their impact on voluntary disclosure through the {mechanism} channel.\n",
    "\n",
    "This study uses a staggered difference-in-differences design where different states adopted {category} laws at\n",
    "different times between 2002-2014.\n",
    "\n",
    "Laws in this category (adopted across multiple states and years):\n",
    "{laws_text}\n",
    "\n",
    "Please write the background, theoretical framework, and hypothesis development section. \n",
    "\n",
    "Please structure your response as follows:\n",
    "\n",
    "1. Background (3 paragraphs, ~400 words total):\n",
    "    - Label this subsection \"Background\"\n",
    "    - Describe the wave of state-level {category} laws adopted between 2002-2014\n",
    "    - Explain why the change was instituted\n",
    "    - Discuss the staggered implementation across different states at different times\n",
    "    - Please also discuss whether there were other contemporaneous law adoptions\n",
    "    - Support each claim with citations to foundational papers \n",
    "\n",
    "2. Theoretical Framework (2-3 paragraphs, ~300 words):\n",
    "    - Label this subsection \"Theoretical Framework\"\n",
    "    - Begin with a brief introduction connecting the laws to the relevant theoretical perspective {mechanism}\n",
    "    - Explain core concepts of {mechanism}\n",
    "    - Connect to voluntary disclosure decisions\n",
    "    - Link to the specific {mechanism} being studied\n",
    "    - Support with 2-3 seminal citations\n",
    "\n",
    "3. Hypothesis Development (3 paragraphs, ~400 words total):\n",
    "    - Label this subsection \"Hypothesis Development\"\n",
    "    - Present economic mechanisms linking state-level {category} laws to voluntary disclosure decisions through the {mechanism} channel\n",
    "    - Draw on established theoretical frameworks specifically related to {mechanism}\n",
    "    - Propose a theoretically supported hypothesis about the relationship between the state-level {category} laws and voluntary disclosure for the specific {mechanism} channel\n",
    "    - Build logical arguments step by step and think through whether prior literature suggests competing theoretical predictions or if the literature suggests only one direction for the relationship\n",
    "    - Present the formal hypothesis statement on its own line, clearly labeled \"H1:\"\n",
    "    - Support each claim with citations to foundational papers \n",
    "\n",
    "Writing Guidelines:\n",
    "- Use active voice (e.g., \"We examine\" instead of \"This paper examines\")\n",
    "- Maintain formal academic tone suitable for a top journal\n",
    "- Include 2-3 citations per paragraph \n",
    "- Use present tense for established findings\n",
    "- Make clear distinctions between correlation and causation\n",
    "- Cite papers from top accounting and finance journals such as:\n",
    "    The Accounting Review, Journal of Accounting Research, \n",
    "    Journal of Accounting and Economics, Contemporary Accounting Research, Accounting, Organizations, and Society,\n",
    "    and Review of Accounting Studies\n",
    "    \n",
    "IMPORTANT: Include in-text citations but do not include a separate References section at the end.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting background and hypothesis: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def create_background_hypothesis_files(self, csv_file: str, regression_dir: str, output_dir: str):\n",
    "        \"\"\"Generate background and hypothesis for categories with regression results\"\"\"\n",
    "        main_dir = os.path.join(output_dir, 'background and hypothesis development')\n",
    "        os.makedirs(main_dir, exist_ok=True)\n",
    "    \n",
    "        # Get categories with results\n",
    "        category_laws = self.get_laws_with_regression_results(csv_file, regression_dir)\n",
    "    \n",
    "        for category, data in category_laws.items():\n",
    "            print(f\"\\nProcessing category: {category}\")\n",
    "        \n",
    "            for channel in data['channels']:\n",
    "                print(f\"  Channel: {channel}\")\n",
    "            \n",
    "                safe_category = category.replace(' ', '_')\n",
    "                safe_channel = channel.replace(' ', '_')\n",
    "                filename = f\"{safe_category}_{safe_channel}_background_hypothesis.txt\"\n",
    "                file_path = os.path.join(main_dir, filename)\n",
    "            \n",
    "                if os.path.exists(file_path):\n",
    "                    print(f\"  Skipping: File exists\")\n",
    "                    continue\n",
    "            \n",
    "                content = self.get_background_hypothesis(category, data['laws'], channel)\n",
    "                content = self.clean_markdown_formatting(content)\n",
    "            \n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "                print(f\"  Saved!\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API key here\"\n",
    "    CSV_FILE = r\"enter file path here\"\n",
    "    REGRESSION_DIR = r\"enter folder path here\"\n",
    "    OUTPUT_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        analyzer = ComprehensiveAnalyzer(API_KEY)\n",
    "        analyzer.create_background_hypothesis_files(CSV_FILE, REGRESSION_DIR, OUTPUT_DIR)\n",
    "        print(\"\\nBackground and hypothesis development sections complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# 7. Send regression results to Claude for interpretation\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from typing import Dict, List\n",
    "from anthropic import Anthropic\n",
    "\n",
    "class RegressionInterpreter:\n",
    "    def __init__(self, input_dir: str, output_dir: str, api_key: str):\n",
    "        \"\"\"Initialize interpreter with input and output directories\"\"\"\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        \n",
    "    def _get_significance_stars(self, pvalue: float) -> str:\n",
    "        \"\"\"Get significance stars based on p-value.\"\"\"\n",
    "        if pvalue < 0.01:\n",
    "            return \"***\"\n",
    "        elif pvalue < 0.05:\n",
    "            return \"**\"\n",
    "        elif pvalue < 0.1:\n",
    "            return \"*\"\n",
    "        return \"\"\n",
    "\n",
    "    def read_regression_results(self, panel_name: str) -> Dict:\n",
    "        \"\"\"Read regression results JSON file for a specific panel\"\"\"\n",
    "        results_path = os.path.join(self.output_dir, panel_name, 'regression_results.json')\n",
    "        \n",
    "        try:\n",
    "            with open(results_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No results file found for {panel_name}\")\n",
    "            return {}\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error reading results file for {panel_name}\")\n",
    "            return {}\n",
    "\n",
    "    def read_hypothesis(self, category: str, channel: str) -> str:\n",
    "        \"\"\"Read hypothesis file for a category-channel combination\"\"\"\n",
    "        hypothesis_dir = os.path.join(os.path.dirname(self.output_dir), \n",
    "                                      'background and hypothesis development')\n",
    "        \n",
    "        # Format filename: Securities_Enforcement_Information_Asymmetry_background_hypothesis.txt\n",
    "        safe_category = category.replace(' ', '_')\n",
    "        safe_channel = channel.replace(' ', '_')\n",
    "        filename = f\"{safe_category}_{safe_channel}_background_hypothesis.txt\"\n",
    "        hypothesis_file = os.path.join(hypothesis_dir, filename)\n",
    "        \n",
    "        print(f\"Looking for hypothesis file: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            with open(hypothesis_file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Extract hypothesis development section\n",
    "            if \"Hypothesis Development\" in content:\n",
    "                hypothesis_section = content.split(\"Hypothesis Development\")[1]\n",
    "                \n",
    "                if \"H1:\" in hypothesis_section:\n",
    "                    hypothesis_development = hypothesis_section.split(\"H1:\")[0].strip()\n",
    "                    h1_statement = \"H1:\" + hypothesis_section.split(\"H1:\")[1].strip()\n",
    "                    return f\"Hypothesis Development:\\n\\n{hypothesis_development}\\n\\n{h1_statement}\"\n",
    "                else:\n",
    "                    return f\"Hypothesis Development:\\n\\n{hypothesis_section.strip()}\"\n",
    "            else:\n",
    "                print(f\"No Hypothesis Development section found\")\n",
    "                return content\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"No hypothesis file found: {filename}\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading hypothesis file: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def format_results_text(self, category: str, channel: str, results: Dict) -> str:\n",
    "        \"\"\"Format regression results into text for the academic prompt\"\"\"\n",
    "        results_text = f\"Regression Analysis: {category} - {channel}\\n\\n\"\n",
    "        results_text += \"This analysis uses a staggered difference-in-differences design examining \"\n",
    "        results_text += f\"the effect of state-level {category} laws on voluntary disclosure through \"\n",
    "        results_text += f\"the {channel} mechanism.\\n\\n\"\n",
    "        \n",
    "        for spec_name, res in results.items():\n",
    "            results_text += f\"\\nSpecification {spec_name}:\\n\"\n",
    "            results_text += f\"Treatment Effect: {res['coefficients']['treatment_effect']:.4f}\\n\"\n",
    "            results_text += f\"T-statistic: {res['t_stats']['treatment_effect']:.2f}\\n\"\n",
    "            results_text += f\"P-value: {res['pvalues']['treatment_effect']:.4f}\\n\"\n",
    "            results_text += f\"R-squared: {res['r_squared']:.4f}\\n\"\n",
    "            results_text += f\"Number of observations: {int(res['n_obs'])}\\n\"\n",
    "            results_text += f\"Number of firms: {res['n_firms']}\\n\"\n",
    "            \n",
    "            if res['controls']:\n",
    "                results_text += \"\\nControl Variables:\\n\"\n",
    "                for control in res['controls']:\n",
    "                    coef = res['coefficients'][control]\n",
    "                    tstat = res['t_stats'][control]\n",
    "                    pvalue = res['pvalues'][control]\n",
    "                    stars = self._get_significance_stars(pvalue)\n",
    "                    results_text += f\"{control}: {coef:.4f}{stars} (t={tstat:.2f}, p={pvalue:.4f})\\n\"\n",
    "            \n",
    "            results_text += \"\\nFixed Effects:\\n\"\n",
    "            for fe, included in res['fixed_effects'].items():\n",
    "                results_text += f\"{fe}: {'Yes' if included else 'No'}\\n\"\n",
    "            \n",
    "            results_text += \"-\" * 50 + \"\\n\"\n",
    "        \n",
    "        return results_text\n",
    "\n",
    "    def generate_claude_interpretation(self, category: str, channel: str, results_text: str, hypothesis_text: str) -> str:\n",
    "        \"\"\"Generate interpretation using Claude API\"\"\"\n",
    "        prompt = f\"\"\"You are an accounting academic with a PhD in accounting. \n",
    "    You should use active voice (e.g. \"We find\" instead of \"It is found\"). \n",
    "    Use present tense for all established findings. \n",
    "    Distinguish between correlation and causation. \n",
    "    Write the results description for this analysis as if you were writing an academic paper for an accounting journal, \n",
    "    you are studying the association between a change in mandatory disclosure and voluntary disclosure. \n",
    "\n",
    "    This study examines the association between state-level {category} laws and voluntary disclosure, \n",
    "    using a staggered difference-in-differences design where different states adopted laws at different times \n",
    "    between 2002-2014.\n",
    "\n",
    "    Here is the hypothesis that was developed:\n",
    "    {hypothesis_text}\n",
    "\n",
    "    Please provide a detailed academic analysis of these regression results:\n",
    "\n",
    "    {results_text}\n",
    "\n",
    "    Please structure your analysis as follows (3 paragraphs, ~600 words total):\n",
    "    1. Label this section Regression Analysis\n",
    "    2. Main finding (treatment effect interpretation)\n",
    "    3. Statistical significance and economic magnitude\n",
    "    4. Model specification comparison\n",
    "    5. Control variable effects\n",
    "       Describe whether the relationship is consistent with prior literature\n",
    "    6. Explain whether the results support the hypothesis stated in the Hypothesis section above\n",
    "\n",
    "    Write in an academic style suitable for a top accounting journal.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting Claude interpretation: {str(e)}\")\n",
    "            return f\"Error in Claude analysis: {str(e)}\"\n",
    "        \n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        content = re.sub(r'\\*{1,4}([^*]*?)\\*{1,4}', r'\\1', content)\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        return content    \n",
    "\n",
    "    def interpret_panel_results(self, panel_name: str) -> str:\n",
    "        \"\"\"Generate interpretation for a single panel's results\"\"\"\n",
    "        \n",
    "        # Extract category and channel from panel name\n",
    "        # Format: panel_Securities_Enforcement_Information_Asymmetry\n",
    "        if not panel_name.startswith('panel_'):\n",
    "            print(f\"Invalid panel name format: {panel_name}\")\n",
    "            return \"\"\n",
    "        \n",
    "        name_parts = panel_name.replace('panel_', '').split('_')\n",
    "        \n",
    "        # Find the channel (last part matching known channels)\n",
    "        channels = ['Litigation_Risk', 'Corporate_Governance', 'Proprietary_Costs', \n",
    "                   'Information_Asymmetry', 'Unsophisticated_Investors', \n",
    "                   'Equity_Issuance', 'Reputation_Risk']\n",
    "        \n",
    "        channel = None\n",
    "        category_parts = []\n",
    "        \n",
    "        for i, part in enumerate(name_parts):\n",
    "            # Check if remaining parts form a known channel\n",
    "            remaining = '_'.join(name_parts[i:])\n",
    "            if remaining in channels:\n",
    "                channel = remaining.replace('_', ' ')\n",
    "                category_parts = name_parts[:i]\n",
    "                break\n",
    "        \n",
    "        if not channel or not category_parts:\n",
    "            print(f\"Could not extract category and channel from {panel_name}\")\n",
    "            return \"\"\n",
    "        \n",
    "        category = ' '.join(category_parts)\n",
    "        \n",
    "        print(f\"Processing: {category} - {channel}\")\n",
    "        \n",
    "        # Check if regression results exist\n",
    "        results = self.read_regression_results(panel_name)\n",
    "        if not results:\n",
    "            print(f\"No regression results found for {panel_name}\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Check if hypothesis exists\n",
    "        hypothesis_text = self.read_hypothesis(category, channel)\n",
    "        if not hypothesis_text:\n",
    "            print(f\"No hypothesis found for {category} - {channel}\")\n",
    "            return \"\"\n",
    "        \n",
    "        print(f\"Both regression results and hypothesis found - proceeding with interpretation\")\n",
    "        \n",
    "        # Format results text\n",
    "        results_text = self.format_results_text(category, channel, results)\n",
    "        \n",
    "        # Generate interpretation using Claude\n",
    "        interpretation = self.generate_claude_interpretation(\n",
    "            category, \n",
    "            channel,\n",
    "            results_text,\n",
    "            hypothesis_text\n",
    "        )\n",
    "        \n",
    "        # Clean markdown formatting before saving\n",
    "        clean_interpretation = self.clean_markdown_formatting(interpretation)\n",
    "        \n",
    "        # Save interpretation to panel folder\n",
    "        panel_dir = os.path.join(self.output_dir, panel_name)\n",
    "        os.makedirs(panel_dir, exist_ok=True)\n",
    "        \n",
    "        claude_path = os.path.join(panel_dir, 'claude_interpretation.txt')\n",
    "        try:\n",
    "            with open(claude_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(clean_interpretation)\n",
    "            print(f\"Saved interpretation to {claude_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving interpretation to file: {str(e)}\")\n",
    "        \n",
    "        return clean_interpretation\n",
    "\n",
    "    def analyze_all_panels(self) -> None:\n",
    "        \"\"\"Analyze results for all panels in the directory\"\"\"\n",
    "        panel_dirs = glob.glob(os.path.join(self.output_dir, \"panel_*\"))\n",
    "        \n",
    "        for panel_dir in panel_dirs:\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            try:\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                self.interpret_panel_results(panel_name)\n",
    "                print(\"=\"*80 + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing {panel_name}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API key here\"\n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    INPUT_DIR = os.path.join(BASE_DIR, \"law_panels\")\n",
    "    OUTPUT_DIR = os.path.join(BASE_DIR, \"regression_analyses\")\n",
    "    \n",
    "    interpreter = RegressionInterpreter(INPUT_DIR, OUTPUT_DIR, API_KEY)\n",
    "    interpreter.analyze_all_panels()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "# 8. Create Correlation tables\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_CENTER, TA_LEFT \n",
    "from scipy import stats\n",
    "from reportlab.lib.pagesizes import letter, landscape\n",
    "\n",
    "def add_underscores_before_capitals(text):\n",
    "    \"\"\"Add underscores before capital letters in a string\"\"\"\n",
    "    return re.sub(r'(?<=[a-z0-9])(?=[A-Z])', '_', text)\n",
    "\n",
    "def create_correlation_table(data_path, output_dir):\n",
    "    \"\"\"\n",
    "    Creates a clean correlation table PDF in the style of academic papers.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the panel data CSV\n",
    "        output_dir (str): Path to save output files\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Select numerical variables for correlation\n",
    "    numeric_vars = ['treatment_effect','freqMF','linstown', 'lsize', 'lbtm', 'lroa', 'lsaret12', 'levol', 'lloss', \n",
    "                    'lcalrisk']\n",
    "                   \n",
    "    # Create shorter variable names for the table\n",
    "    var_mapping = {\n",
    "        'treatment_effect': 'Treatment Effect',\n",
    "        'freqMF': 'FreqMF',\n",
    "        'linstown': 'Institutional ownership',\n",
    "        'lsize': 'Firm size',\n",
    "        'lbtm': 'Book-to-market',\n",
    "        'lroa': 'ROA',\n",
    "        'lsaret12': 'Stock return',\n",
    "        'levol': 'Earnings volatility',\n",
    "        'lloss': 'Loss',\n",
    "        'lcalrisk': 'Class action litigation risk'\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df[numeric_vars].corr()\n",
    "    \n",
    "    # Calculate p-values for significance testing\n",
    "    def calculate_pvalue(x, y):\n",
    "        return stats.pearsonr(x.dropna(), y.dropna())[1]\n",
    "    \n",
    "    p_values = pd.DataFrame(index=numeric_vars, columns=numeric_vars)\n",
    "    for i in numeric_vars:\n",
    "        for j in numeric_vars:\n",
    "            p_values.loc[i,j] = calculate_pvalue(df[i], df[j])\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get panel name from path\n",
    "    panel_name = os.path.basename(os.path.dirname(data_path))\n",
    "    \n",
    "    # Create PDF\n",
    "    clean_name = panel_name.replace('panel_', '')\n",
    "    pdf_path = os.path.join(output_dir, f'{clean_name}_correlation_table.pdf')\n",
    "    doc = SimpleDocTemplate(pdf_path, pagesize=landscape(letter), rightMargin=30, leftMargin=30, topMargin=50, bottomMargin=50)\n",
    "    \n",
    "    # Prepare table data\n",
    "    table_data = [['']]  # First cell empty\n",
    "    \n",
    "    # Add column headers\n",
    "    for var in numeric_vars:\n",
    "        table_data[0].append(var_mapping[var])\n",
    "    \n",
    "    # Add rows\n",
    "    for i, var1 in enumerate(numeric_vars, 1):\n",
    "        row = [var_mapping[var1]]  # Row header\n",
    "        for var2 in numeric_vars:\n",
    "            if var1 == var2:\n",
    "                row.append('1.00')\n",
    "            else:\n",
    "                value = corr_matrix.loc[var1, var2]\n",
    "                # Format to 2 decimal places\n",
    "                formatted_value = f'{value:.2f}'\n",
    "                row.append(formatted_value)\n",
    "        table_data.append(row)\n",
    "    \n",
    "    # Create table style\n",
    "    style = [\n",
    "        ('FONTNAME', (0,0), (-1,-1), 'Times-Roman'),\n",
    "        ('FONTSIZE', (0,0), (-1,-1), 8),\n",
    "        ('ALIGN', (0,0), (-1,-1), 'CENTER'),\n",
    "        ('TOPPADDING', (0,0), (-1,-1), 3),\n",
    "        ('BOTTOMPADDING', (0,0), (-1,-1), 3),\n",
    "        ('GRID', (0,0), (-1,-1), 0.25, colors.black),  # Lighter grid lines\n",
    "        ('BOX', (0,0), (-1,-1), 0.25, colors.black),\n",
    "        # Make column headers and row headers bold\n",
    "        ('FONTNAME', (0,0), (-1,0), 'Times-Bold'),\n",
    "        ('FONTNAME', (0,0), (0,-1), 'Times-Bold'),\n",
    "    ]\n",
    "    \n",
    "    # Add bold style for significant correlations\n",
    "    for i in range(1, len(table_data)):\n",
    "        for j in range(1, len(table_data[0])):\n",
    "            if i != j:  # Skip diagonal\n",
    "                var1 = numeric_vars[i-1]\n",
    "                var2 = numeric_vars[j-1]\n",
    "                if p_values.loc[var1,var2] < 0.05:  # 5% significance level\n",
    "                    style.append(('FONTNAME', (j,i), (j,i), 'Times-Bold'))\n",
    "    \n",
    "    # Create table\n",
    "    table = Table(table_data)\n",
    "    table.setStyle(TableStyle(style))\n",
    "    \n",
    "    # Create title\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=12,\n",
    "        alignment=TA_CENTER,\n",
    "        spaceBefore=12,\n",
    "        spaceAfter=20,\n",
    "        fontName='Times-Bold'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Create Panel title\n",
    "    panel_title = \"\"\n",
    "    if panel_name:\n",
    "        # Extract category and channel from panel name\n",
    "        # Format: panel_Securities_Enforcement_Information_Asymmetry\n",
    "        clean_panel_name = panel_name.replace('panel_', '')\n",
    "    \n",
    "        # Split into parts\n",
    "        name_parts = clean_panel_name.split('_')\n",
    "    \n",
    "        # Find the channel\n",
    "        channels = ['Litigation_Risk', 'Corporate_Governance', 'Proprietary_Costs', \n",
    "                   'Information_Asymmetry', 'Unsophisticated_Investors', \n",
    "                   'Equity_Issuance', 'Reputation_Risk']\n",
    "    \n",
    "        channel = None\n",
    "        category_parts = []\n",
    "    \n",
    "        for i, part in enumerate(name_parts):\n",
    "            remaining = '_'.join(name_parts[i:])\n",
    "            if remaining in channels:\n",
    "                channel = remaining.replace('_', ' ')\n",
    "                category_parts = name_parts[:i]\n",
    "                break\n",
    "    \n",
    "        if channel and category_parts:\n",
    "            category = ' '.join(category_parts)\n",
    "            panel_title = f\"<br/>{category} - {channel}\"\n",
    "        else:\n",
    "            # Fallback to simple formatting\n",
    "            panel_title = f\"<br/>{clean_panel_name.replace('_', ' ')}\"\n",
    "\n",
    "    title = Paragraph(f\"Table 2<br/>Pearson Correlations{panel_title}\", title_style)\n",
    "    \n",
    "    # Add footnote\n",
    "    footnote_style = ParagraphStyle(\n",
    "        'Footnote',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=8,\n",
    "        alignment=TA_LEFT,\n",
    "        fontName='Times-Roman',\n",
    "        spaceBefore=6,\n",
    "        leading=10  # Controls line spacing\n",
    "    )\n",
    "    footnote = Paragraph(\"This table shows the Pearson correlations for the sample. \"\n",
    "                        \"Correlations that are significant at the 0.05 level or better are highlighted in bold. \", footnote_style)\n",
    "    \n",
    "    # Build PDF\n",
    "    doc.build([title, table, Spacer(1, 12), footnote])\n",
    "    \n",
    "    print(f\"Created correlation table PDF for {panel_name}\")\n",
    "    return pdf_path\n",
    "\n",
    "def batch_process_panels(base_dir,output_base_dir):\n",
    "    \"\"\"\n",
    "    Process all panel folders and create correlation tables, skipping existing ones.\n",
    "    \"\"\"\n",
    "    print(f\"Starting to process panels in: {base_dir}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = os.path.join(r\"enter folder path here)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "    \n",
    "    # Count total panels\n",
    "    panel_folders = [f for f in os.listdir(base_dir) if f.startswith('panel_')]\n",
    "    total_panels = len(panel_folders)\n",
    "    processed = 0\n",
    "    skipped = 0\n",
    "    errors = 0\n",
    "    \n",
    "    print(f\"\\nFound {total_panels} panel folders to process\")\n",
    "    \n",
    "    # Process each panel folder\n",
    "    for i, panel_folder in enumerate(panel_folders, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing panel {i} of {total_panels}: {panel_folder}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        panel_path = os.path.join(base_dir, panel_folder)\n",
    "        \n",
    "        # Check if correlation table already exists\n",
    "        clean_name = panel_folder.replace('panel_', '')  # Keep as-is since already formatted\n",
    "        existing_table = os.path.join(output_dir, f'{clean_name}_correlation_table.pdf')\n",
    "        \n",
    "        if os.path.exists(existing_table):\n",
    "            print(f\"Skipping {panel_folder}: Correlation table already exists\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        # Look for the data file\n",
    "        data_file = 'analysis_data.csv'\n",
    "        data_path = os.path.join(panel_path, data_file)\n",
    "            \n",
    "        if os.path.exists(data_path):\n",
    "            try:\n",
    "                table_path = create_correlation_table(data_path, output_dir)\n",
    "                print(f\"Created correlation table for {panel_folder}\")\n",
    "                print(f\"Table saved to: {table_path}\")\n",
    "                processed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {panel_folder}: {str(e)}\")\n",
    "                errors += 1\n",
    "        else:\n",
    "            print(f\"No data file found in {panel_folder}\")\n",
    "            errors += 1\n",
    "        \n",
    "        # Print progress summary\n",
    "        print(f\"\\nProgress Summary:\")\n",
    "        print(f\"Processed: {i}/{total_panels} ({(i/total_panels)*100:.1f}%)\")\n",
    "        print(f\"Successfully created: {processed}\")\n",
    "        print(f\"Skipped (already exist): {skipped}\")\n",
    "        print(f\"Errors: {errors}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # Base directory containing panel folders\n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    OUTPUT_BASE_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    # Process all panels\n",
    "    batch_process_panels(BASE_DIR, OUTPUT_BASE_DIR)\n",
    "                              \n",
    "# 9. Send sample and descriptive statistics results to Claude for interpretation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from typing import Dict, List\n",
    "import json\n",
    "from anthropic import Anthropic\n",
    "import traceback\n",
    "from fpdf import FPDF\n",
    "import re\n",
    "\n",
    "class DescriptiveStatsAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "\n",
    "    def convert_numpy_types(self, obj):\n",
    "        \"\"\"Convert numpy/pandas types to native Python types for JSON serialization\"\"\"\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self.convert_numpy_types(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.convert_numpy_types(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    \n",
    "    def calculate_descriptive_stats(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Calculate descriptive statistics for the dataset\"\"\"\n",
    "        # List of numeric columns to analyze (excluding GVKEY, FYEAR, etc.)\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col not in ['GVKEY', 'FYEAR', 'sic3', 'Year']]\n",
    "        \n",
    "        # Sort columns to match example order if possible\n",
    "        preferred_order = [\n",
    "            'linstown', 'lsize', 'lbtm', 'lroa', 'lsaret12', 'levol', 'lloss', 'lcalrisk'\n",
    "        ]\n",
    "        sorted_cols = sorted(numeric_cols, key=lambda x: \n",
    "                           preferred_order.index(x) if x in preferred_order else float('inf'))\n",
    "        \n",
    "        stats = {}\n",
    "        for col in sorted_cols:  # Use sorted_cols instead of numeric_cols\n",
    "            col_stats = {\n",
    "                'n': len(df[col].dropna()),\n",
    "                'mean': df[col].mean(),\n",
    "                'median': df[col].median(),\n",
    "                'std': df[col].std(),\n",
    "                'p25': df[col].quantile(0.25),\n",
    "                'p75': df[col].quantile(0.75),\n",
    "                'min': df[col].min(),\n",
    "                'max': df[col].max()\n",
    "            }\n",
    "            stats[col] = col_stats\n",
    "        \n",
    "        # Add additional summary statistics\n",
    "        summary_stats = {\n",
    "            'total_observations': len(df),\n",
    "            'unique_firms': len(df['GVKEY'].unique()),\n",
    "            'year_range': f\"{df['FYEAR'].min()} to {df['FYEAR'].max()}\",\n",
    "            'industries': len(df['sic3'].unique())\n",
    "        }\n",
    "\n",
    "        # Convert numpy types to Python types\n",
    "        summary_stats = self.convert_numpy_types(summary_stats)\n",
    "        stats['summary'] = summary_stats\n",
    "\n",
    "        stats = self.convert_numpy_types(stats)\n",
    "        return stats\n",
    "    \n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        # Remove headers (##, ###, etc.)\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove bold/italic formatting - this handles **text**, ***text***, ****text****\n",
    "        content = re.sub(r'\\*{1,4}([^*]*?)\\*{1,4}', r'\\1', content)\n",
    "        \n",
    "        # Clean up any remaining asterisks\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        \n",
    "        # Clean up any extra whitespace that might be left\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def get_claude_interpretation(self, stats: Dict, panel_name: str) -> str:\n",
    "        \"\"\"Get Claude's interpretation of descriptive statistics\"\"\"\n",
    "        # Extract category and mechanism from panel name\n",
    "        # Format: panel_Securities_Enforcement_Information_Asymmetry\n",
    "        category = \"the law category\"\n",
    "        mechanism = \"the mechanism\"\n",
    "    \n",
    "        if panel_name.startswith('panel_'):\n",
    "            name_parts = panel_name.replace('panel_', '').split('_')\n",
    "        \n",
    "            mechanisms = ['Litigation_Risk', 'Corporate_Governance', 'Proprietary_Costs', \n",
    "                         'Information_Asymmetry', 'Unsophisticated_Investors', \n",
    "                         'Equity_Issuance', 'Reputation_Risk']\n",
    "        \n",
    "            for i, part in enumerate(name_parts):\n",
    "                remaining = '_'.join(name_parts[i:])\n",
    "                if remaining in mechanisms:\n",
    "                    mechanism = remaining.replace('_', ' ')\n",
    "                    category = ' '.join(name_parts[:i])\n",
    "                    break\n",
    "    \n",
    "        # Format statistics for Claude\n",
    "        stats_text = f\"Descriptive Statistics for {category} - {mechanism}\\n\\n\"\n",
    "    \n",
    "        # Add summary information\n",
    "        summary = stats['summary']\n",
    "        stats_text += \"Sample Characteristics:\\n\"\n",
    "        stats_text += f\"Total observations: {summary['total_observations']:,}\\n\"\n",
    "        stats_text += f\"Number of unique firms: {summary['unique_firms']:,}\\n\"\n",
    "        stats_text += f\"Sample period: {summary['year_range']}\\n\\n\"\n",
    "    \n",
    "        # Add variable statistics\n",
    "        stats_text += \"Variable Statistics:\\n\"\n",
    "        for var, var_stats in {k: v for k, v in stats.items() if k != 'summary'}.items():\n",
    "            stats_text += f\"\\n{var}:\\n\"\n",
    "            stats_text += f\"N: {var_stats['n']:,}\\n\"\n",
    "            stats_text += f\"Mean: {var_stats['mean']:.3f}\\n\"\n",
    "            stats_text += f\"Median: {var_stats['median']:.3f}\\n\"\n",
    "            stats_text += f\"Std Dev: {var_stats['std']:.3f}\\n\"\n",
    "            stats_text += f\"25th percentile: {var_stats['p25']:.3f}\\n\"\n",
    "            stats_text += f\"75th percentile: {var_stats['p75']:.3f}\\n\"\n",
    "            stats_text += f\"Min: {var_stats['min']:.3f}\\n\"\n",
    "            stats_text += f\"Max: {var_stats['max']:.3f}\\n\"\n",
    "        \n",
    "        # Create prompt for Claude\n",
    "        prompt = f\"\"\"You are an accounting academic with a PhD in accounting. \n",
    "        You should use active voice (e.g. \"We find\" instead of \"It is found\"). \n",
    "        Use present tense for all established findings. Write the descriptive statistics section for this analysis as if \n",
    "        you were writing an academic paper for an accounting journal. Here are the descriptive statistics:\n",
    "\n",
    "{stats_text}\n",
    "\n",
    "Please structure your analysis as follows (400 words):\n",
    "1. Label this section \"Sample Description and Descriptive Statistics\"\n",
    "2. Describe the sample characteristics (number of firms, time period)\n",
    "3. Describe the key variables' distributions\n",
    "4. Highlight any notable patterns or potential outliers\n",
    "5. Compare statistics to relevant benchmarks from prior literature where applicable\n",
    "\n",
    "IMPORTANT: DO NOT include the number of industries in the sample. For example, DO NOT write that the sample represents\n",
    "a specific number of industries. \n",
    "\n",
    "Write in an academic style suitable for a top accounting journal.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting Claude interpretation: {str(e)}\")\n",
    "            return f\"Error in Claude analysis: {str(e)}\"\n",
    "    \n",
    "    def create_descriptive_stats_table(self, stats: Dict, output_path: str, regulation_title: str):\n",
    "        \"\"\"Create a PDF table of descriptive statistics in academic paper format\"\"\"\n",
    "        try:\n",
    "            pdf = FPDF(format='A4', orientation='L')\n",
    "            pdf.add_page()\n",
    "        \n",
    "            # Set Times New Roman font\n",
    "            try:\n",
    "                pdf.add_font('Times', '', 'times.ttf', uni=True)\n",
    "                pdf.add_font('Times', 'B', 'timesbd.ttf', uni=True)\n",
    "                pdf.set_font('Times', size=11)\n",
    "            except:\n",
    "                pdf.set_font('Times', size=11)\n",
    "        \n",
    "            pdf.set_margins(20, 20, 20)\n",
    "        \n",
    "            # Title\n",
    "            pdf.set_font('Times', 'B', 14)\n",
    "            pdf.cell(0, 10, 'Table 1', align='C', ln=True)\n",
    "            pdf.set_font('Times', '', 12)\n",
    "            pdf.cell(0, 10, 'Descriptive Statistics', align='C', ln=True)\n",
    "            pdf.ln(5)\n",
    "        \n",
    "            # Calculate column widths\n",
    "            var_width = 70\n",
    "            num_width = 30\n",
    "        \n",
    "            # Table headers\n",
    "            pdf.set_font('Times', 'B')\n",
    "            headers = ['Variables', 'N', 'Mean', 'Std. Dev.', 'P25', 'Median', 'P75']\n",
    "            pdf.cell(var_width, 8, headers[0], border=1)\n",
    "            for header in headers[1:]:\n",
    "                pdf.cell(num_width, 8, header, border=1, align='C')\n",
    "            pdf.ln()\n",
    "        \n",
    "            # Variable name mappings with ordered display\n",
    "            var_display_names = {\n",
    "                'freqMF': 'FreqMF',\n",
    "                'treatment_effect': 'Treatment Effect',\n",
    "                'linstown': 'Institutional ownership',\n",
    "                'lsize': 'Firm size',\n",
    "                'lbtm': 'Book-to-market',\n",
    "                'lroa': 'ROA',\n",
    "                'lsaret12': 'Stock return',\n",
    "                'levol': 'Earnings volatility',\n",
    "                'lloss': 'Loss',\n",
    "                'lcalrisk': 'Class action litigation risk'\n",
    "            }\n",
    "        \n",
    "            # Excluded variables\n",
    "            excluded_vars = {'sic4', 'permno', 'post-law', 'treated'}\n",
    "        \n",
    "            # Sort variables to ensure FreqMF is first\n",
    "            variables = {k: v for k, v in stats.items() \n",
    "                        if k != 'summary' and k not in excluded_vars}\n",
    "        \n",
    "            # Define display order\n",
    "            display_order = ['freqMF', 'treatment_effect'] + [\n",
    "                k for k in var_display_names.keys() \n",
    "                if k not in ['freqMF', 'treatment_effect']\n",
    "            ]\n",
    "        \n",
    "            pdf.set_font('Times', '')\n",
    "            for var_name in display_order:\n",
    "                if var_name in variables:\n",
    "                    var_stats = variables[var_name]\n",
    "                    display_name = var_display_names.get(var_name, var_name)\n",
    "                    pdf.cell(var_width, 8, display_name, border=1)\n",
    "                \n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['n']:,}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['mean']:.4f}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['std']:.4f}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['p25']:.4f}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['median']:.4f}\", border=1, align='C')\n",
    "                    pdf.cell(num_width, 8, f\"{var_stats['p75']:.4f}\", border=1, align='C')\n",
    "                    pdf.ln()\n",
    "        \n",
    "            # Footnote\n",
    "            pdf.ln(10)\n",
    "            pdf.set_font('Times', '', 10)\n",
    "            footnote = \"This table shows the descriptive statistics. All continuous variables are winsorized at the 1st and 99th percentiles.\"\n",
    "            pdf.multi_cell(0, 5, footnote)\n",
    "        \n",
    "            pdf.output(output_path)\n",
    "            print(f\"Successfully saved descriptive statistics table to {output_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating descriptive statistics table: {str(e)}\")\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        \n",
    "            try:\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Times\", size=12)\n",
    "                pdf.cell(0, 10, \"Error occurred while creating descriptive statistics table\")\n",
    "                pdf.ln()\n",
    "                pdf.cell(0, 10, f\"Error: {str(e)}\")\n",
    "                pdf.output(output_path)\n",
    "            except Exception as e2:\n",
    "                print(f\"Emergency PDF save also failed: {str(e2)}\")\n",
    "    \n",
    "    def analyze_panel(self, panel_dir: str, output_dir: str) -> None:\n",
    "        \"\"\"Analyze descriptive statistics for a single panel dataset\"\"\"\n",
    "        try:\n",
    "            # Get panel name from directory name\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            print(f\"\\nAnalyzing {panel_name}...\")\n",
    "            \n",
    "            # Read filtered data\n",
    "            data_file = os.path.join(panel_dir, 'analysis_data.csv')\n",
    "            if not os.path.exists(data_file):\n",
    "                print(f\"No analysis_data.csv found in {panel_dir}\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Reading data from {data_file}\")\n",
    "            df = pd.read_csv(data_file)\n",
    "            \n",
    "            # Create output directory\n",
    "            panel_output_dir = os.path.join(output_dir, panel_name)\n",
    "            os.makedirs(panel_output_dir, exist_ok=True)\n",
    "            print(f\"Created output directory: {panel_output_dir}\")\n",
    "            \n",
    "            # Calculate descriptive statistics\n",
    "            print(\"Calculating descriptive statistics...\")\n",
    "            stats = self.calculate_descriptive_stats(df)\n",
    "            \n",
    "            # Save descriptive statistics to JSON\n",
    "            stats_path = os.path.join(panel_output_dir, 'descriptive_stats.json')\n",
    "            with open(stats_path, 'w') as f:\n",
    "                json.dump(stats, f, indent=4, default=str)\n",
    "            print(f\"Saved descriptive statistics to {stats_path}\")\n",
    "            \n",
    "            # Create and save descriptive statistics table\n",
    "            table_path = os.path.join(panel_output_dir, 'descriptive_stats_table.pdf')\n",
    "            print(f\"Attempting to create PDF table at {table_path}\")\n",
    "            self.create_descriptive_stats_table(stats, table_path, panel_name)\n",
    "            \n",
    "            # Get Claude's interpretation\n",
    "            print(\"Getting Claude's interpretation...\")\n",
    "            interpretation = self.get_claude_interpretation(stats, panel_name)\n",
    "            interpretation = self.clean_markdown_formatting(interpretation)\n",
    "            \n",
    "            # Save Claude's interpretation\n",
    "            interpretation_path = os.path.join(panel_output_dir, 'descriptive_stats_analysis.txt')\n",
    "            with open(interpretation_path, 'w') as f:\n",
    "                f.write(interpretation)\n",
    "            print(f\"Saved Claude's analysis to {interpretation_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {panel_dir}: {str(e)}\")\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "\n",
    "def analyze_all_panels(base_dir: str, output_dir: str, api_key: str):\n",
    "    \"\"\"Analyze all panel datasets in subfolders\"\"\"\n",
    "    analyzer = DescriptiveStatsAnalyzer(api_key)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Find all subfolders that start with 'panel_'\n",
    "    panel_dirs = [d for d in glob.glob(os.path.join(base_dir, 'panel_*_*')) if os.path.isdir(d)]\n",
    "    print(f\"Found {len(panel_dirs)} panel directories to analyze\")\n",
    "\n",
    "    for i, panel_dir in enumerate(panel_dirs, 1):\n",
    "        print(f\"\\nProcessing panel {i} of {len(panel_dirs)}: {panel_dir}\")\n",
    "        analyzer.analyze_panel(panel_dir, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API key here\"\n",
    "    \n",
    "    # Updated paths for Windows using raw strings to handle backslashes\n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    OUTPUT_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    # Run analysis on all panels\n",
    "    analyze_all_panels(BASE_DIR, OUTPUT_DIR, API_KEY)\n",
    "                              \n",
    "# 10. Ask Claude to write introduction\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "import glob\n",
    "import re\n",
    "\n",
    "class ComprehensiveAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    def get_categories_with_regression_results(self, csv_file: str, regression_dir: str) -> dict:\n",
    "        \"\"\"Read laws from CSV and identify categories with regression results\"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Get all panel directories\n",
    "        panel_dirs = glob.glob(os.path.join(regression_dir, 'panel_*'))\n",
    "        \n",
    "        # Extract categories and mechanisms that have regression results\n",
    "        categories_with_results = {}\n",
    "        \n",
    "        for panel_dir in panel_dirs:\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            # Format: panel_Securities_Enforcement_Information_Asymmetry\n",
    "            \n",
    "            mechanisms = ['Litigation_Risk', 'Corporate_Governance', 'Proprietary_Costs', \n",
    "                         'Information_Asymmetry', 'Unsophisticated_Investors', \n",
    "                         'Equity_Issuance', 'Reputation_Risk']\n",
    "            \n",
    "            for mechanism in mechanisms:\n",
    "                if panel_name.endswith(mechanism):\n",
    "                    category_part = panel_name.replace('panel_', '').replace(f'_{mechanism}', '')\n",
    "                    category = category_part.replace('_', ' ')\n",
    "                    mechanism_name = mechanism.replace('_', ' ')\n",
    "                    \n",
    "                    if category not in categories_with_results:\n",
    "                        categories_with_results[category] = []\n",
    "                    if mechanism_name not in categories_with_results[category]:\n",
    "                        categories_with_results[category].append(mechanism_name)\n",
    "                    break\n",
    "        \n",
    "        print(f\"Found categories with regression results: {categories_with_results}\")\n",
    "        \n",
    "        # For each category, get ALL laws in that category from CSV\n",
    "        category_data = {}\n",
    "        \n",
    "        for category in categories_with_results.keys():\n",
    "            category_df = df[df['Law Category_y'] == category]\n",
    "            \n",
    "            laws_list = []\n",
    "            for _, row in category_df.iterrows():\n",
    "                law_info = {\n",
    "                    'title': row['Regulation Title'],\n",
    "                    'year': row['Year'],\n",
    "                    'state': row['State'],\n",
    "                    'body': row['Regulatory Body'],\n",
    "                    'description': row['Description'],\n",
    "                    'impact': row['Impact']\n",
    "                }\n",
    "                laws_list.append(law_info)\n",
    "            \n",
    "            if laws_list:\n",
    "                category_data[category] = {\n",
    "                    'laws': laws_list,\n",
    "                    'mechanisms': categories_with_results[category]\n",
    "                }\n",
    "                print(f\"\\n{category}: {len(laws_list)} laws\")\n",
    "        \n",
    "        return category_data\n",
    "\n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        content = re.sub(r'\\*{1,4}([^*]*?)\\*{1,4}', r'\\1', content)\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        return content\n",
    "    \n",
    "    def read_regression_results(self, panel_name: str, regression_dir: str) -> dict:\n",
    "        \"\"\"Read regression results for a specific panel\"\"\"\n",
    "        results_file = os.path.join(regression_dir, panel_name, 'regression_results.json')\n",
    "        \n",
    "        if os.path.exists(results_file):\n",
    "            with open(results_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            print(f\"No results file found for {panel_name}\")\n",
    "            return {}\n",
    "\n",
    "    def format_regression_results(self, results: dict) -> str:\n",
    "        \"\"\"Format regression results for a specific panel\"\"\"\n",
    "        if not results:\n",
    "            return \"No regression results available.\"\n",
    "            \n",
    "        formatted_text = \"\\nRegression Results:\\n\\n\"\n",
    "        \n",
    "        for spec_name, spec_results in results.items():\n",
    "            formatted_text += f\"\\nSpecification {spec_name}:\\n\"\n",
    "            try:\n",
    "                formatted_text += f\"Treatment Effect: {spec_results['coefficients']['treatment_effect']:.4f}\\n\"\n",
    "                formatted_text += f\"T-statistic: {abs(spec_results['t_stats']['treatment_effect']):.2f}\\n\"\n",
    "                formatted_text += f\"P-value: {spec_results['pvalues']['treatment_effect']:.4f}\\n\"\n",
    "                formatted_text += f\"R-squared: {spec_results['r_squared']:.4f}\\n\"\n",
    "                \n",
    "                if spec_results['controls']:\n",
    "                    formatted_text += \"\\nControl Variables:\\n\"\n",
    "                    for control in spec_results['controls']:\n",
    "                        coef = spec_results['coefficients'][control]\n",
    "                        tstat = spec_results['t_stats'][control]\n",
    "                        pvalue = spec_results['pvalues'][control]\n",
    "                        formatted_text += f\"{control}: coef={coef:.4f}, t={tstat:.2f}, p={pvalue:.4f}\\n\"\n",
    "                \n",
    "                formatted_text += \"\\n\" + \"-\"*50 + \"\\n\"\n",
    "            except KeyError as e:\n",
    "                print(f\"Missing key in regression results: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return formatted_text\n",
    "\n",
    "    def get_comprehensive_introduction(self, category: str, laws: list, mechanism: str, regression_results: dict) -> str:\n",
    "        \"\"\"Get comprehensive introduction for a category and mechanism\"\"\"\n",
    "        \n",
    "        # Format laws list\n",
    "        laws_text = \"\\n\".join([\n",
    "            f\"- {law['title']} ({law['state']}, {law['year']}): {law['description']}\"\n",
    "            for law in laws\n",
    "        ])\n",
    "        \n",
    "        regression_text = self.format_regression_results(regression_results)\n",
    "        \n",
    "        prompt = f\"\"\"As an accounting academic, please write a comprehensive introduction section examining \n",
    "        state-level {category} laws and their impact on voluntary disclosure through the {mechanism} channel.\n",
    "        \n",
    "This study uses a staggered difference-in-differences design where different states adopted {category} laws at different\n",
    "times between 2002-2014.\n",
    "\n",
    "Laws in this category:\n",
    "{laws_text}\n",
    "\n",
    "Empirical Results:\n",
    "{regression_text}\n",
    "\n",
    "Please structure the introduction as follows:\n",
    "\n",
    "1. Motivation (2 paragraphs, ~200 words):\n",
    "   - Begin with the importance of state-level {category} laws\n",
    "   - Open with a broad statement about state-level {category} laws\n",
    "   - Focus specifically on how the laws relate to {mechanism}\n",
    "   - Explain their relevance to voluntary disclosure through this mechanism\n",
    "   - Identify the specific gap or puzzle in the literature\n",
    "   - Identify specific research questions\n",
    "\n",
    "2. Hypothesis Development (3 paragraphs, ~300 words):\n",
    "   - Present the economic mechanism linking {category} laws to voluntary disclosure\n",
    "   - Explain how {mechanism} affects voluntary disclosure\n",
    "   - Discuss theoretical underpinnings\n",
    "   - Build on established theoretical frameworks\n",
    "   - Develop clear, testable predictions\n",
    "   - Build logical arguments step by step\n",
    "   - Support each claim with citations to foundational papers\n",
    "   - Support arguments with citations\n",
    "\n",
    "3. Results Summary (3 paragraphs, ~300 words):\n",
    "   - Lead with strongest statistical findings\n",
    "   - Present the treatment effect coefficient of {regression_text}\n",
    "   - Summarize the key findings of the analysis, \n",
    "     discussing the significance of the variable in terms of predictive power: {regression_text}\n",
    "   - Discuss significance of variables and their predictive power\n",
    "   - Present results in order of importance\n",
    "   - Include economic significance\n",
    "   - Use precise statistical language\n",
    "   - Connect findings back to the {mechanism} channel\n",
    "\n",
    "4. Contribution (2 paragraphs, ~200 words):\n",
    "   - Position relative to 3-4 most closely related papers\n",
    "   - Highlight novel findings about {mechanism}\n",
    "   - Discuss broader implications for theory and practice\n",
    "   - Emphasize contributions to understanding this specific economic channel\n",
    "\n",
    "Guidelines:\n",
    "- Do not include headers in the write up\n",
    "- Do not include extra text or explanations\n",
    "    -Example of what not to include: \"Here's a comprehensive introduction section following your guidelines\" or \n",
    "    \"Here's a comprehensive introduction section examining Resource Extraction Disclosure Rules and its impact on voluntary disclosure through the Corporate Governance channel\"\n",
    "- Use active voice (e.g., \"We find\" instead of \"It is found\")\n",
    "- Maintain formal academic tone\n",
    "- Include 2-3 citations per paragraph \n",
    "- Use present tense for established findings\n",
    "- Use past tense for your specific results\n",
    "- Make clear distinctions between correlation and causation\n",
    "- Avoid speculation beyond the data\n",
    "- Cite papers from top accounting and finance journals such as:\n",
    "    The Accounting Review, Journal of Accounting Research, \n",
    "    Journal of Accounting and Economics, Contemporary Accounting Research, Accounting, Organizations, and Society,\n",
    "    and Review of Accounting Studies\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting introduction: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def analyze_and_save_introductions(self, regression_dir: str, csv_file: str, output_dir: str):\n",
    "        \"\"\"Generate and save comprehensive introductions for categories\"\"\"\n",
    "        intro_dir = os.path.join(output_dir, 'introduction')\n",
    "        os.makedirs(intro_dir, exist_ok=True)\n",
    "        \n",
    "        # Get categories with results\n",
    "        category_data = self.get_categories_with_regression_results(csv_file, regression_dir)\n",
    "        \n",
    "        total_introductions = 0\n",
    "        \n",
    "        for category, data in category_data.items():\n",
    "            print(f\"\\nProcessing category: {category}\")\n",
    "            \n",
    "            for mechanism in data['mechanisms']:\n",
    "                print(f\"  Writing introduction for mechanism: {mechanism}\")\n",
    "                \n",
    "                # Construct panel name\n",
    "                safe_category = category.replace(' ', '_')\n",
    "                safe_mechanism = mechanism.replace(' ', '_')\n",
    "                panel_name = f\"panel_{safe_category}_{safe_mechanism}\"\n",
    "                \n",
    "                # Get regression results\n",
    "                regression_results = self.read_regression_results(panel_name, regression_dir)\n",
    "                \n",
    "                if not regression_results:\n",
    "                    print(f\"  Warning: No regression results found for {panel_name}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Generate introduction\n",
    "                    intro = self.get_comprehensive_introduction(\n",
    "                        category, data['laws'], mechanism, regression_results\n",
    "                    )\n",
    "                    intro = self.clean_markdown_formatting(intro)\n",
    "                    \n",
    "                    # Save introduction\n",
    "                    filename = f\"{safe_category}_{safe_mechanism}_introduction.txt\"\n",
    "                    with open(os.path.join(intro_dir, filename), 'w', encoding='utf-8') as f:\n",
    "                        f.write(intro)\n",
    "                    \n",
    "                    total_introductions += 1\n",
    "                    print(f\"  ✓ Saved introduction for {category} - {mechanism}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ ERROR: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FINAL SUMMARY:\")\n",
    "        print(f\"Categories processed: {len(category_data)}\")\n",
    "        print(f\"Introductions saved: {total_introductions}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "def main():\n",
    "    API_KEY = \"enter API key here\"\n",
    "    REGRESSION_DIR = r\"enter folder path here\"\n",
    "    CSV_FILE = r\"enter file path here\"\n",
    "    OUTPUT_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        analyzer = ComprehensiveAnalyzer(API_KEY)\n",
    "        analyzer.analyze_and_save_introductions(REGRESSION_DIR, CSV_FILE, OUTPUT_DIR)\n",
    "        print(\"Analysis complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "                              \n",
    "# 11. Ask Claude to write the model specification section of a paper \n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "import glob\n",
    "import re\n",
    "\n",
    "class ComprehensiveAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        \n",
    "    def get_categories_with_regression_results(self, csv_file: str, regression_dir: str) -> dict:\n",
    "        \"\"\"Read laws from CSV and identify categories with regression results\"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Get all panel directories\n",
    "        panel_dirs = glob.glob(os.path.join(regression_dir, 'panel_*'))\n",
    "        \n",
    "        # Extract categories and mechanisms that have regression results\n",
    "        categories_with_results = {}\n",
    "        \n",
    "        for panel_dir in panel_dirs:\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            # Format: panel_Securities_Enforcement_Information_Asymmetry\n",
    "            \n",
    "            mechanisms = ['Litigation_Risk', 'Corporate_Governance', 'Proprietary_Costs', \n",
    "                         'Information_Asymmetry', 'Unsophisticated_Investors', \n",
    "                         'Equity_Issuance', 'Reputation_Risk']\n",
    "            \n",
    "            for mechanism in mechanisms:\n",
    "                if panel_name.endswith(mechanism):\n",
    "                    category_part = panel_name.replace('panel_', '').replace(f'_{mechanism}', '')\n",
    "                    category = category_part.replace('_', ' ')\n",
    "                    mechanism_name = mechanism.replace('_', ' ')\n",
    "                    \n",
    "                    if category not in categories_with_results:\n",
    "                        categories_with_results[category] = []\n",
    "                    if mechanism_name not in categories_with_results[category]:\n",
    "                        categories_with_results[category].append(mechanism_name)\n",
    "                    break\n",
    "        \n",
    "        print(f\"Found categories with regression results: {categories_with_results}\")\n",
    "        \n",
    "        # For each category, get ALL laws in that category from CSV\n",
    "        category_data = {}\n",
    "        \n",
    "        for category in categories_with_results.keys():\n",
    "            category_df = df[df['Law Category_y'] == category]\n",
    "            \n",
    "            laws_list = []\n",
    "            for _, row in category_df.iterrows():\n",
    "                law_info = {\n",
    "                    'title': row['Regulation Title'],\n",
    "                    'year': row['Year'],\n",
    "                    'state': row['State'],\n",
    "                    'body': row['Regulatory Body'],\n",
    "                    'description': row['Description'],\n",
    "                    'impact': row['Impact']\n",
    "                }\n",
    "                laws_list.append(law_info)\n",
    "            \n",
    "            if laws_list:\n",
    "                category_data[category] = {\n",
    "                    'laws': laws_list,\n",
    "                    'mechanisms': categories_with_results[category]\n",
    "                }\n",
    "                print(f\"\\n{category}: {len(laws_list)} laws\")\n",
    "        \n",
    "        return category_data\n",
    "        \n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        # Remove headers (##, ###, etc.)\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove bold/italic formatting - this handles **text**, ***text***, ****text****\n",
    "        content = re.sub(r'\\*{1,4}([^*]*?)\\*{1,4}', r'\\1', content)\n",
    "        \n",
    "        # Clean up any remaining asterisks\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        \n",
    "        # Clean up any extra whitespace that might be left\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def read_regression_results(self, panel_name: str, regression_dir: str) -> dict:\n",
    "        \"\"\"Read regression results for a specific panel\"\"\"\n",
    "        results_file = os.path.join(regression_dir, panel_name, 'regression_results.json')\n",
    "        \n",
    "        if os.path.exists(results_file):\n",
    "            with open(results_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            print(f\"No results file found for {panel_name}\")\n",
    "            return {}\n",
    "\n",
    "    def format_regression_results(self, results: dict) -> str:\n",
    "        \"\"\"Format regression results for a specific panel\"\"\"\n",
    "        if not results:\n",
    "            return \"No regression results available.\"\n",
    "            \n",
    "        formatted_text = \"\\nRegression Results:\\n\\n\"\n",
    "        \n",
    "        for spec_name, spec_results in results.items():\n",
    "            formatted_text += f\"\\nSpecification {spec_name}:\\n\"\n",
    "            try:\n",
    "                # Handle treatment effect with safety checks\n",
    "                treatment_coef = spec_results.get('coefficients', {}).get('treatment_effect', 0)\n",
    "                treatment_tstat = spec_results.get('t_stats', {}).get('treatment_effect', 0)\n",
    "                treatment_pval = spec_results.get('pvalues', {}).get('treatment_effect', 1)\n",
    "                r_squared = spec_results.get('r_squared', 0)\n",
    "                \n",
    "                formatted_text += f\"Treatment Effect: {treatment_coef:.4f}\\n\"\n",
    "                formatted_text += f\"T-statistic: {abs(treatment_tstat):.2f}\\n\"\n",
    "                formatted_text += f\"P-value: {treatment_pval:.4f}\\n\"\n",
    "                formatted_text += f\"R-squared: {r_squared:.4f}\\n\"\n",
    "                \n",
    "                # Handle controls with safety checks\n",
    "                controls = spec_results.get('controls', [])\n",
    "                coefficients = spec_results.get('coefficients', {})\n",
    "                t_stats = spec_results.get('t_stats', {})\n",
    "                pvalues = spec_results.get('pvalues', {})\n",
    "                \n",
    "                if controls:\n",
    "                    formatted_text += \"\\nControl Variables:\\n\"\n",
    "                    for control in controls:\n",
    "                        coef = coefficients.get(control, 0)\n",
    "                        tstat = t_stats.get(control, 0)\n",
    "                        pvalue = pvalues.get(control, 1)\n",
    "                        formatted_text += f\"{control}: coef={coef:.4f}, t={tstat:.2f}, p={pvalue:.4f}\\n\"\n",
    "                \n",
    "                formatted_text += \"\\n\" + \"-\"*50 + \"\\n\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error formatting results for {spec_name}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return formatted_text\n",
    "    \n",
    "    def get_model_specification(self, category: str, laws: list, mechanism: str, regression_results: dict) -> str:\n",
    "        \"\"\"Get model specification section for Securities Enforcement category and specific mechanism\"\"\"\n",
    "        regression_text = self.format_regression_results(regression_results) if regression_results else \"No regression results available.\"\n",
    "        \n",
    "        # Get number of observations from regression results\n",
    "        n_obs = None\n",
    "        if regression_results and '(3)' in regression_results:\n",
    "            n_obs = regression_results['(3)'].get('n_obs', 'Not available')\n",
    "        \n",
    "        # Get list of control variables from regression results\n",
    "        controls = []\n",
    "        if regression_results:\n",
    "            for spec in regression_results.values():\n",
    "                if spec.get('controls'):\n",
    "                    controls.extend(spec['controls'])\n",
    "            controls = list(set(controls))  # Remove duplicates\n",
    "            \n",
    "        # Format laws list\n",
    "        laws_text = \"\\n\".join([\n",
    "            f\"- {law['title']} ({law['state']}, {law['year']}): {law['description']}\"\n",
    "            for law in laws\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"You are an accounting academic writing a research paper examining state-level {category} laws and their impact \n",
    "        on voluntary disclosure through the {mechanism} channel. \n",
    "        Please write the research design section for an academic journal in accounting.\n",
    "\n",
    "Details:\n",
    "Category: {category}\n",
    "Mechanism: {mechanism}\n",
    "\n",
    "Laws in this category:\n",
    "{laws_text}\n",
    "\n",
    "Regression Information:\n",
    "{regression_text}\n",
    "\n",
    "IMPORTANT: This study examines all firms in the Compustat universe using a staggered difference-in-differences design \n",
    "where different states adopted state-level {category} laws at different times between 2002-2014. \n",
    "The sample period is 2000-2016.\n",
    "\n",
    "Please follow these detailed guidelines:\n",
    "\n",
    "1. Sample selection and treatment_effect indicator:\n",
    "    - Explain that the sample includes all firms in the Compustat universe during the sample period \n",
    "    - Describe the regulatory authorities that are responsible for the {category} laws \n",
    "    \n",
    "2. Model Explanation (2-3 paragraphs, ~300 words total):\n",
    "    - Explain the regression model used to examine the relationship between state-level {category} laws\n",
    "      and voluntary disclosure through the {mechanism} channel\n",
    "          -The model is: FreqMF = β₀ + β₁Treatment Effect + γControls + ε\n",
    "    - Only discuss the control variables that appear in the regression results {regression_text}\n",
    "      These variables are based on prior literature and are: Institutional Ownership, Firm Size, Book-to-Market,\n",
    "      ROA, Stock Return, Earnings volatility, Loss, Class action litigation risk\n",
    "    - Support model choices with citations to foundational papers\n",
    "    - Explain potential endogeneity concerns and how the research design addresses them\n",
    "    - Use clear, academic language\n",
    "    - Avoid using underscores in variable names\n",
    "\n",
    "3. Mathematical Model:\n",
    "    - Present the complete regression equation in proper mathematical notation {regression_text}\n",
    "        - Label the equation as follows: FreqMF = β₀ + β₁Treatment Effect + γControls + ε\n",
    "            - Label the dependent variable \"FreqMF\"\n",
    "            - Label the variable of interest as \"Treatment Effect\"\n",
    "            - Label the control variables in the regression equation as \"Controls\"\n",
    "    - Do no include the subscripts i and t in the regression \n",
    "    - Format the equation professionally\n",
    "\n",
    "4. Variable Definitions (2-3 paragraphs, ~300 words total):\n",
    "    - Define the dependent variable (FreqMF - management forecast frequency)\n",
    "    - Define the \"Treatment Effect\" variable as an indicator variable for the post-{category} period after \n",
    "      a state implements {category} regulations..\n",
    "      For example: For example: Indicator equal to 1 when a firm's home state adopts Securities Enforcement\n",
    "      regulation and thereafter, and 0 otherwise\n",
    "    - Define each control variable used in the model as they appear in {regression_text}\n",
    "      These variables are based on prior literature and are: Institutional Ownership, Firm Size, Book-to-Market,\n",
    "      ROA, Stock Return, Earnings volatility, Loss, Class action litigation risk\n",
    "        -Cite the appropriate paper for these variables from the Journal of Accounting Research\n",
    "    - Do no include the subscripts i and t in the variable definition\n",
    "    - For each control variable, provide detailed explanations about their expected relationships with voluntary disclosure\n",
    "    - Explain how variables relate to the {mechanism} channel\n",
    "    \n",
    "\n",
    "5. Sample Construction (2-3 paragraphs, ~300 words total):\n",
    "    - Describe the event window around the staggered adoption periods (2002-2014)\n",
    "        -Always clarify that the treatment effect includes the regulation year by writing \"from adoption year onwards\"\n",
    "    - Describe the source of the data from Compustat, I/B/E/S, Audit Analytics, and CRSP\n",
    "    - Describe the sample construction process based on the number of observations: {n_obs if n_obs else 'Not available'}\n",
    "    - Explain the treatment and control groups in the staggered diff-in-diff design\n",
    "    - Note any sample restrictions\n",
    "\n",
    "Writing Guidelines:\n",
    "- Provide only the write up, no extra text or explanations \n",
    "    -Example of what not to include: \"Here's a comprehensive model specifaction section following your guidelines\"\n",
    "- Use active voice (e.g., \"We find\" instead of \"It is found\")\n",
    "- Maintain formal academic tone\n",
    "- Include 2-3 citations per paragraph\n",
    "- Cite papers from top accounting and finance journals such as:\n",
    "    The Accounting Review, Journal of Accounting Research, \n",
    "    Journal of Accounting and Economics, Contemporary Accounting Research, Accounting, Organizations, and Society,\n",
    "    and Review of Accounting Studies\n",
    "- Use precise statistical language\n",
    "- Make clear connections between variables and theoretical predictions\n",
    "- Do not include Latex format\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting model specification: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def create_model_specifications(self, regression_dir: str, csv_file: str, output_dir: str):\n",
    "        \"\"\"Generate and save model specification sections for all categories\"\"\"\n",
    "        # Create main directory\n",
    "        main_dir = os.path.join(output_dir, 'model_specification')\n",
    "        os.makedirs(main_dir, exist_ok=True)\n",
    "        \n",
    "        # Get categories with results\n",
    "        category_data = self.get_categories_with_regression_results(csv_file, regression_dir)\n",
    "        \n",
    "        total_specifications = 0\n",
    "        \n",
    "        for category, data in category_data.items():\n",
    "            print(f\"\\nProcessing category: {category}\")\n",
    "            \n",
    "            for mechanism in data['mechanisms']:\n",
    "                print(f\"  Writing model specification for mechanism: {mechanism}\")\n",
    "                \n",
    "                # Construct panel name\n",
    "                safe_category = category.replace(' ', '_')\n",
    "                safe_mechanism = mechanism.replace(' ', '_')\n",
    "                panel_name = f\"panel_{safe_category}_{safe_mechanism}\"\n",
    "                \n",
    "                # Get regression results\n",
    "                regression_results = self.read_regression_results(panel_name, regression_dir)\n",
    "                \n",
    "                if not regression_results:\n",
    "                    print(f\"  Warning: No regression results found for {panel_name}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Generate model specification\n",
    "                    model_spec = self.get_model_specification(\n",
    "                        category, data['laws'], mechanism, regression_results\n",
    "                    )\n",
    "                    model_spec = self.clean_markdown_formatting(model_spec)\n",
    "                    \n",
    "                    # Save model specification\n",
    "                    filename = f\"{safe_category}_{safe_mechanism}_model_specification.txt\"\n",
    "                    with open(os.path.join(main_dir, filename), 'w', encoding='utf-8') as f:\n",
    "                        f.write(model_spec)\n",
    "                    \n",
    "                    total_specifications += 1\n",
    "                    print(f\"  ✓ Saved model specification for {category} - {mechanism}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ ERROR: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FINAL SUMMARY:\")\n",
    "        print(f\"Categories processed: {len(category_data)}\")\n",
    "        print(f\"Model specifications saved: {total_specifications}\")\n",
    "        print(f\"{'='*50}\")\n",
    "    \n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API key here\"\n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    CSV_FILE = \"enter file path here\"\n",
    "    OUTPUT_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        analyzer = ComprehensiveAnalyzer(API_KEY)\n",
    "        analyzer.create_model_specifications(BASE_DIR, CSV_FILE, OUTPUT_DIR)\n",
    "        print(\"\\nModel specification sections complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "                              \n",
    "# 12. Ask Claude to write a conclusion \n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "import glob\n",
    "import re\n",
    "\n",
    "class ComprehensiveAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize analyzer with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    def get_categories_with_regression_results(self, csv_file: str, regression_dir: str) -> dict:\n",
    "        \"\"\"Read laws from CSV and identify categories with regression results\"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Get all panel directories\n",
    "        panel_dirs = glob.glob(os.path.join(regression_dir, 'panel_*'))\n",
    "        \n",
    "        # Extract categories and mechanisms that have regression results\n",
    "        categories_with_results = {}\n",
    "        \n",
    "        for panel_dir in panel_dirs:\n",
    "            panel_name = os.path.basename(panel_dir)\n",
    "            # Format: panel_Securities_Enforcement_Information_Asymmetry\n",
    "            \n",
    "            mechanisms = ['Litigation_Risk', 'Corporate_Governance', 'Proprietary_Costs', \n",
    "                         'Information_Asymmetry', 'Unsophisticated_Investors', \n",
    "                         'Equity_Issuance', 'Reputation_Risk']\n",
    "            \n",
    "            for mechanism in mechanisms:\n",
    "                if panel_name.endswith(mechanism):\n",
    "                    category_part = panel_name.replace('panel_', '').replace(f'_{mechanism}', '')\n",
    "                    category = category_part.replace('_', ' ')\n",
    "                    mechanism_name = mechanism.replace('_', ' ')\n",
    "                    \n",
    "                    if category not in categories_with_results:\n",
    "                        categories_with_results[category] = []\n",
    "                    if mechanism_name not in categories_with_results[category]:\n",
    "                        categories_with_results[category].append(mechanism_name)\n",
    "                    break\n",
    "        \n",
    "        print(f\"Found categories with regression results: {categories_with_results}\")\n",
    "        \n",
    "        # For each category, get ALL laws in that category from CSV\n",
    "        category_data = {}\n",
    "        \n",
    "        for category in categories_with_results.keys():\n",
    "            category_df = df[df['Law Category_y'] == category]\n",
    "            \n",
    "            laws_list = []\n",
    "            for _, row in category_df.iterrows():\n",
    "                law_info = {\n",
    "                    'title': row['Regulation Title'],\n",
    "                    'year': row['Year'],\n",
    "                    'state': row['State'],\n",
    "                    'body': row['Regulatory Body'],\n",
    "                    'description': row['Description'],\n",
    "                    'impact': row['Impact']\n",
    "                }\n",
    "                laws_list.append(law_info)\n",
    "            \n",
    "            if laws_list:\n",
    "                category_data[category] = {\n",
    "                    'laws': laws_list,\n",
    "                    'mechanisms': categories_with_results[category]\n",
    "                }\n",
    "                print(f\"\\n{category}: {len(laws_list)} laws\")\n",
    "        \n",
    "        return category_data\n",
    "\n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        # Remove headers (##, ###, etc.)\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove bold/italic formatting - this handles **text**, ***text***, ****text****\n",
    "        content = re.sub(r'\\*{1,4}([^*]*?)\\*{1,4}', r'\\1', content)\n",
    "        \n",
    "        # Clean up any remaining asterisks\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        \n",
    "        # Clean up any extra whitespace that might be left\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def read_regression_results(self, panel_name: str, regression_dir: str) -> dict:\n",
    "        \"\"\"Read regression results for a specific panel\"\"\"\n",
    "        results_file = os.path.join(regression_dir, panel_name, 'regression_results.json')\n",
    "        \n",
    "        if os.path.exists(results_file):\n",
    "            with open(results_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            print(f\"No results file found for {panel_name}\")\n",
    "            return {}\n",
    "\n",
    "    def format_regression_results(self, results: dict) -> str:\n",
    "        \"\"\"Format regression results for a specific panel\"\"\"\n",
    "        if not results:\n",
    "            return \"No regression results available.\"\n",
    "            \n",
    "        formatted_text = \"\\nRegression Results:\\n\\n\"\n",
    "        \n",
    "        for spec_name, spec_results in results.items():\n",
    "            formatted_text += f\"\\nSpecification {spec_name}:\\n\"\n",
    "            try:\n",
    "                # Handle treatment effect with safety checks\n",
    "                treatment_coef = spec_results.get('coefficients', {}).get('treatment_effect', 0)\n",
    "                treatment_tstat = spec_results.get('t_stats', {}).get('treatment_effect', 0)\n",
    "                treatment_pval = spec_results.get('pvalues', {}).get('treatment_effect', 1)\n",
    "                r_squared = spec_results.get('r_squared', 0)\n",
    "                \n",
    "                formatted_text += f\"Treatment Effect: {treatment_coef:.4f}\\n\"\n",
    "                formatted_text += f\"T-statistic: {abs(treatment_tstat):.2f}\\n\"\n",
    "                formatted_text += f\"P-value: {treatment_pval:.4f}\\n\"\n",
    "                formatted_text += f\"R-squared: {r_squared:.4f}\\n\"\n",
    "                \n",
    "                # Handle controls with safety checks\n",
    "                controls = spec_results.get('controls', [])\n",
    "                coefficients = spec_results.get('coefficients', {})\n",
    "                t_stats = spec_results.get('t_stats', {})\n",
    "                pvalues = spec_results.get('pvalues', {})\n",
    "                \n",
    "                if controls:\n",
    "                    formatted_text += \"\\nControl Variables:\\n\"\n",
    "                    for control in controls:\n",
    "                        coef = coefficients.get(control, 0)\n",
    "                        tstat = t_stats.get(control, 0)\n",
    "                        pvalue = pvalues.get(control, 1)\n",
    "                        formatted_text += f\"{control}: coef={coef:.4f}, t={tstat:.2f}, p={pvalue:.4f}\\n\"\n",
    "                \n",
    "                formatted_text += \"\\n\" + \"-\"*50 + \"\\n\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error formatting results for {spec_name}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return formatted_text\n",
    "\n",
    "    def get_conclusion(self, category: str, laws: list, mechanism: str, regression_results: dict) -> str:\n",
    "        \"\"\"Get conclusion section for a category and mechanism with regression results\"\"\"\n",
    "        regression_text = self.format_regression_results(regression_results) if regression_results else \"No regression results available.\"\n",
    "        \n",
    "        # Format laws list\n",
    "        laws_text = \"\\n\".join([\n",
    "            f\"- {law['title']} ({law['state']}, {law['year']}): {law['description']}\"\n",
    "            for law in laws\n",
    "        ]) \n",
    "        \n",
    "        prompt = f\"\"\"You are an accounting academic writing a research paper examining state-level {category} laws and their \n",
    "        impact on voluntary disclosure through the {mechanism} channel. \n",
    "        Please write a conclusion section for an academic journal in accounting.\n",
    "\n",
    "Details:\n",
    "Category: {category}\n",
    "Mechanism: {mechanism}\n",
    "\n",
    "Laws in this category:\n",
    "{laws_text}\n",
    "\n",
    "\n",
    "Empirical Results:\n",
    "{regression_text}\n",
    "\n",
    "Please write a comprehensive conclusion following these guidelines:\n",
    "\n",
    "1. Summary of Main Findings (2-3 paragraphs):\n",
    "    - Restate the research question, focusing on the {mechanism} channel\n",
    "    - Summarize key empirical findings\n",
    "    - Discuss statistical and economic significance\n",
    "    - Interpret the results in the context of {category} laws and {mechanism}\n",
    "\n",
    "2. Implications (1-2 paragraphs):\n",
    "    - Discuss implications for regulators\n",
    "    - Discuss implications for managers\n",
    "    - Discuss implications for investors\n",
    "    - Connect findings to broader literature on {mechanism}\n",
    "\n",
    "3. Limitations and Future Research (1-2 paragraphs):\n",
    "    - Acknowledge key limitations\n",
    "    - Suggest promising avenues for future research\n",
    "    - Discuss potential extensions, particularly related to {mechanism}\n",
    "    \n",
    "Writing Guidelines:\n",
    "- Use active voice (e.g., \"We find\" instead of \"It is found\")\n",
    "- Maintain formal academic tone\n",
    "- Use past tense for your specific results\n",
    "- Use present tense for implications\n",
    "- Make clear distinctions between correlation and causation\n",
    "- Focus on the practical significance of the findings\n",
    "- Cite papers from top accounting and finance journals such as:\n",
    "    The Accounting Review, Journal of Accounting Research, \n",
    "    Journal of Accounting and Economics, Contemporary Accounting Research, Accounting, Organizations, and Society,\n",
    "    and Review of Accounting Studies\n",
    "- Do not include section headers\n",
    "- Do not include journal names after the citation. For example, for these citations: (Christensen et al., 2013,\n",
    "  Journal of Accounting and Economics; Shroff et al., 2013, The Accounting Review). \n",
    "  The Journal of Accounting and Economics and The Accounting Review names\n",
    "  should not be included. \n",
    "- Length: approximately 750 words\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting conclusion: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def create_conclusions(self, regression_dir: str, csv_file: str, output_dir: str):\n",
    "        \"\"\"Generate and save conclusion sections for all categories\"\"\"\n",
    "        # Create main directory\n",
    "        main_dir = os.path.join(output_dir, 'conclusion')\n",
    "        os.makedirs(main_dir, exist_ok=True)\n",
    "        \n",
    "        # Get categories with results\n",
    "        category_data = self.get_categories_with_regression_results(csv_file, regression_dir)\n",
    "        \n",
    "        total_conclusions = 0\n",
    "        \n",
    "        for category, data in category_data.items():\n",
    "            print(f\"\\nProcessing category: {category}\")\n",
    "            \n",
    "            for mechanism in data['mechanisms']:\n",
    "                print(f\"  Writing conclusion for mechanism: {mechanism}\")\n",
    "                \n",
    "                # Construct panel name\n",
    "                safe_category = category.replace(' ', '_')\n",
    "                safe_mechanism = mechanism.replace(' ', '_')\n",
    "                panel_name = f\"panel_{safe_category}_{safe_mechanism}\"\n",
    "                \n",
    "                # Get regression results\n",
    "                regression_results = self.read_regression_results(panel_name, regression_dir)\n",
    "                \n",
    "                if not regression_results:\n",
    "                    print(f\"  Warning: No regression results found for {panel_name}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Generate conclusion\n",
    "                    conclusion = self.get_conclusion(\n",
    "                        category, data['laws'], mechanism, regression_results\n",
    "                    )\n",
    "                    conclusion = self.clean_markdown_formatting(conclusion)\n",
    "                    \n",
    "                    # Save conclusion\n",
    "                    filename = f\"{safe_category}_{safe_mechanism}_conclusion.txt\"\n",
    "                    with open(os.path.join(main_dir, filename), 'w', encoding='utf-8') as f:\n",
    "                        f.write(conclusion)\n",
    "                    \n",
    "                    total_conclusions += 1\n",
    "                    print(f\"  ✓ Saved conclusion for {category} - {mechanism}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ ERROR: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FINAL SUMMARY:\")\n",
    "        print(f\"Categories processed: {len(category_data)}\")\n",
    "        print(f\"Conclusions saved: {total_conclusions}\")\n",
    "        print(f\"{'='*50}\")\n",
    "    \n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API key here\"\n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    CSV_FILE = \"enter file path here\"\n",
    "    OUTPUT_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        analyzer = ComprehensiveAnalyzer(API_KEY)\n",
    "        analyzer.create_conclusions(BASE_DIR, CSV_FILE, OUTPUT_DIR)\n",
    "        print(\"\\nConclusion sections complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "                              \n",
    "# 13. Ask Claude to write an abstract\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from anthropic import Anthropic\n",
    "import re\n",
    "\n",
    "class AbstractGenerator:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize abstract generator with Claude API key\"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        \n",
    "    def clean_markdown_formatting(self, content: str) -> str:\n",
    "        \"\"\"Remove all markdown formatting from content\"\"\"\n",
    "        # Remove headers (##, ###, etc.)\n",
    "        content = re.sub(r'^#+\\s*', '', content, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove bold/italic formatting - this handles **text**, ***text***, ****text****\n",
    "        content = re.sub(r'\\*{1,4}([^*]*?)\\*{1,4}', r'\\1', content)\n",
    "        \n",
    "        # Clean up any remaining asterisks\n",
    "        content = re.sub(r'\\*+', '', content)\n",
    "        \n",
    "        # Clean up any extra whitespace that might be left\n",
    "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def generate_abstract(self, introduction_content: str) -> str:\n",
    "        \"\"\"Generate an abstract based on an existing introduction\"\"\"\n",
    "        prompt = f\"\"\"As an accounting academic, please convert the following introduction into a concise academic abstract.\n",
    "\n",
    "Guidelines:\n",
    "- Maintain the key points from the introduction\n",
    "- Condense the content to 150-250 words\n",
    "- Include background, research objective, methodology, key findings, and contribution\n",
    "- Use a formal academic tone\n",
    "- Avoid adding new information not present in the original text\n",
    "- Use present tense for established findings\n",
    "- Use past tense for specific results\n",
    "- Do not include citations in the abstract\n",
    "- Do not use the label \"Abstract\"\n",
    "- Write in one paragraph\n",
    "\n",
    "Introduction to Convert:\n",
    "{introduction_content}\n",
    "\n",
    "Please provide a structured abstract that captures the essence of the original introduction.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=4000,\n",
    "                temperature=0.5,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.content[0].text if hasattr(response, 'content') else \"Error: No content in response\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating abstract: {str(e)}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "    \n",
    "    def process_introductions(self, input_dir: str, output_dir: str):\n",
    "        \"\"\"Process introduction files and generate corresponding abstracts\"\"\"\n",
    "        # Create abstracts directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "        # Find all introduction files\n",
    "        introduction_files = [f for f in os.listdir(input_dir) if f.endswith('_introduction.txt')]\n",
    "        \n",
    "        total_files = len(introduction_files)\n",
    "        processed = 0\n",
    "        generated = 0\n",
    "        skipped = 0\n",
    "        \n",
    "        print(f\"Found {total_files} introduction files\")\n",
    "    \n",
    "        # Process each introduction file\n",
    "        for intro_file in introduction_files:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Processing file {processed + 1} of {total_files}: {intro_file}\")\n",
    "            \n",
    "            try:\n",
    "                # Create abstract filename (replace 'introduction' with 'abstract')\n",
    "                abstract_filename = intro_file.replace('_introduction.txt', '_abstract.txt')\n",
    "                abstract_path = os.path.join(output_dir, abstract_filename)\n",
    "            \n",
    "                # CHECK FOR EXISTING FILES\n",
    "                if os.path.exists(abstract_path):\n",
    "                    print(f\"  Skipping: Abstract already exists\")\n",
    "                    skipped += 1\n",
    "                    processed += 1\n",
    "                    continue\n",
    "            \n",
    "                # Read introduction content\n",
    "                with open(os.path.join(input_dir, intro_file), 'r', encoding='utf-8') as f:\n",
    "                    introduction_content = f.read()\n",
    "            \n",
    "                # Generate abstract\n",
    "                print(f\"  Generating abstract...\")\n",
    "                abstract = self.generate_abstract(introduction_content)\n",
    "            \n",
    "                # Clean markdown formatting\n",
    "                clean_abstract = self.clean_markdown_formatting(abstract)\n",
    "            \n",
    "                # Save abstract\n",
    "                with open(abstract_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(clean_abstract)\n",
    "            \n",
    "                generated += 1\n",
    "                print(f\"  ✓ Successfully generated abstract\")\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ ERROR: {str(e)}\")\n",
    "            \n",
    "            processed += 1\n",
    "            print(f\"\\nProgress: {processed}/{total_files} ({(processed/total_files)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FINAL SUMMARY:\")\n",
    "        print(f\"Total introduction files: {total_files}\")\n",
    "        print(f\"Abstracts generated: {generated}\")\n",
    "        print(f\"Abstracts skipped (already exist): {skipped}\")\n",
    "        print(f\"{'='*50}\")\n",
    "            \n",
    "def main():\n",
    "    # Configuration\n",
    "    API_KEY = \"enter API key here\"  \n",
    "    \n",
    "    # Directories\n",
    "    INPUT_DIR = r\"enter folder path here\"\n",
    "    OUTPUT_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize and run abstract generator\n",
    "        generator = AbstractGenerator(API_KEY)\n",
    "        generator.process_introductions(INPUT_DIR, OUTPUT_DIR)\n",
    "        print(\"Abstract generation complete!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "                              \n",
    "# 14. Combine AI-generated content from Txt. files\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def get_available_categories_from_files(base_dir: str) -> dict:\n",
    "    \"\"\"Get categories and mechanisms that actually have files available\"\"\"\n",
    "    abs_dir = os.path.join(base_dir, 'abstracts')\n",
    "    if not os.path.exists(abs_dir):\n",
    "        return {}\n",
    "    \n",
    "    abstract_files = [f for f in os.listdir(abs_dir) if f.endswith('_abstract.txt')]\n",
    "    category_mechanisms = {}\n",
    "    mechanisms = ['Information_Asymmetry', 'Unsophisticated_Investors', 'Corporate_Governance', \n",
    "                 'Proprietary_Costs', 'Litigation_Risk', 'Equity_Issuance', 'Reputation_Risk']\n",
    "    \n",
    "    for file in abstract_files:\n",
    "        filename_base = file.replace('_abstract.txt', '')\n",
    "        category_name = None\n",
    "        mechanism = None\n",
    "        \n",
    "        # Check if filename ends with any mechanism\n",
    "        for mech in mechanisms:\n",
    "            if filename_base.endswith('_' + mech):\n",
    "                category_name = filename_base[:-len('_' + mech)]  # Remove mechanism from end\n",
    "                mechanism = mech.replace('_', ' ')\n",
    "                break\n",
    "        \n",
    "        if category_name and mechanism:\n",
    "            if category_name not in category_mechanisms:\n",
    "                category_mechanisms[category_name] = []\n",
    "            category_mechanisms[category_name].append(mechanism)\n",
    "    \n",
    "    return category_mechanisms\n",
    "\n",
    "def combine_single_category_mechanism(base_dir: str, category_name: str, mechanism: str) -> str:\n",
    "    \"\"\"Combine text sections for a specific category and mechanism\"\"\"\n",
    "    \n",
    "    clean_mechanism = mechanism.replace(' ', '_')\n",
    "    \n",
    "    output_filename = f\"{category_name}_{clean_mechanism}_combined.txt\"\n",
    "    \n",
    "    # Define folder paths\n",
    "    folders = {\n",
    "        'abstracts': os.path.join(base_dir, 'abstracts'),\n",
    "        'introduction': os.path.join(base_dir, 'introduction'),\n",
    "        'background': os.path.join(base_dir, 'background and hypothesis development'),\n",
    "        'model_specification': os.path.join(base_dir, 'model_specification'),\n",
    "        'descriptive_stats': os.path.join(base_dir, 'descriptive_stats'),\n",
    "        'regression_analyses': os.path.join(base_dir, 'regression_analyses'),\n",
    "        'conclusion': os.path.join(base_dir, 'conclusion'),\n",
    "        'output': os.path.join(base_dir, 'combined_sections')\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== PROCESSING {category_name} - {mechanism} ===\")\n",
    "    \n",
    "    # Define file paths\n",
    "    files = {\n",
    "        'abstract': os.path.join(folders['abstracts'], f\"{category_name}_{clean_mechanism}_abstract.txt\"),\n",
    "        'introduction': os.path.join(folders['introduction'], f\"{category_name}_{clean_mechanism}_introduction.txt\"),\n",
    "        'background': os.path.join(folders['background'], f\"{category_name}_{clean_mechanism}_background_hypothesis.txt\"),\n",
    "        'model': os.path.join(folders['model_specification'], f\"{category_name}_{clean_mechanism}_model_specification.txt\"),\n",
    "        'descriptive_stats': os.path.join(folders['descriptive_stats'], f\"panel_{category_name}_{clean_mechanism}\", 'descriptive_stats_analysis.txt'),\n",
    "        'regression': os.path.join(folders['regression_analyses'], f\"panel_{category_name}_{clean_mechanism}\", 'claude_interpretation.txt'),\n",
    "        'conclusion': os.path.join(folders['conclusion'], f\"{category_name}_{clean_mechanism}_conclusion.txt\")\n",
    "    }\n",
    "    \n",
    "    # Check all files exist\n",
    "    missing_files = []\n",
    "    for section, filepath in files.items():\n",
    "        if not os.path.exists(filepath):\n",
    "            missing_files.append(section)\n",
    "            print(f\"MISSING: {section} - {filepath}\")\n",
    "        else:\n",
    "            print(f\"FOUND: {section} - {filepath}\")\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"Skipping {category_name} - {mechanism}: Missing {', '.join(missing_files)}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"All sections found. Proceeding with combination...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(folders['output'], exist_ok=True)\n",
    "    \n",
    "    # Combine content\n",
    "    combined_text = f\"Analysis of {category_name} through {mechanism} channel\\n\\n\"\n",
    "    \n",
    "    # Add each section\n",
    "    sections = [\n",
    "        ('abstract', 'Abstract: '),\n",
    "        ('introduction', 'INTRODUCTION\\n' + '='*50 + '\\n\\n'),\n",
    "        ('background', 'BACKGROUND AND HYPOTHESIS DEVELOPMENT\\n' + '='*50 + '\\n\\n'),\n",
    "        ('model', 'RESEARCH DESIGN\\n' + '='*50 + '\\n\\n'),\n",
    "        ('descriptive_stats', 'DESCRIPTIVE STATISTICS\\n' + '='*50 + '\\n\\n'),\n",
    "        ('regression', 'RESULTS\\n' + '='*50 + '\\n\\n'),\n",
    "        ('conclusion', 'CONCLUSION\\n' + '='*50 + '\\n\\n')\n",
    "    ]\n",
    "    \n",
    "    for section_name, header in sections:\n",
    "        filepath = files[section_name]\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"Adding {section_name} from {filepath}\")\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                if section_name == 'abstract':\n",
    "                    combined_text += header + content.strip() + \"\\n\\n\"\n",
    "                    combined_text += \"\\f\"  # Page break\n",
    "                else:\n",
    "                    combined_text += header + content + \"\\n\\n\"\n",
    "    \n",
    "    # Save combined file\n",
    "    output_file = os.path.join(folders['output'], output_filename)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(combined_text)\n",
    "    \n",
    "    print(f\"✓ Successfully saved: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to combine all categories\"\"\"\n",
    "    \n",
    "    print(\"Script starting...\")\n",
    "    \n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    \n",
    "    # Get available categories\n",
    "    category_mechanisms = get_available_categories_from_files(BASE_DIR)\n",
    "    print(f\"Found {len(category_mechanisms)} categories with available files:\")\n",
    "    for category, mechanisms in category_mechanisms.items():\n",
    "        print(f\"  - {category}: {mechanisms}\")\n",
    "    print()\n",
    "    \n",
    "    total_combinations = sum(len(mechanisms) for mechanisms in category_mechanisms.values())\n",
    "    print(f\"\\nProcessing {total_combinations} category-mechanism combinations\")\n",
    "    \n",
    "    # Process each combination\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for category_name, mechanisms in category_mechanisms.items():\n",
    "        for mechanism in mechanisms:\n",
    "            try:\n",
    "                result = combine_single_category_mechanism(BASE_DIR, category_name, mechanism)\n",
    "                if result:\n",
    "                    successful += 1\n",
    "                else:\n",
    "                    failed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error processing {category_name} - {mechanism}: {str(e)}\")\n",
    "                failed += 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL SUMMARY:\")\n",
    "    print(f\"Total combinations: {total_combinations}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "                              \n",
    "# 15. Ask Claude to create a reference list\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import anthropic\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_LEFT\n",
    "\n",
    "def create_reference_pdf(references, output_path):\n",
    "    \"\"\"\n",
    "    Creates a PDF with properly formatted references using ReportLab.\n",
    "    \n",
    "    Args:\n",
    "        references (list or str): List of references or string containing references\n",
    "        output_path (str): Path where the PDF will be saved\n",
    "    \"\"\"\n",
    "    doc = SimpleDocTemplate(\n",
    "        output_path,\n",
    "        pagesize=letter,\n",
    "        rightMargin=72,\n",
    "        leftMargin=72,\n",
    "        topMargin=72,\n",
    "        bottomMargin=72\n",
    "    )\n",
    "    \n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    # Create style for references with proper hanging indentation\n",
    "    ref_style = ParagraphStyle(\n",
    "        'Reference',\n",
    "        parent=styles['Normal'],\n",
    "        fontName='Times-Roman',\n",
    "        fontSize=12,\n",
    "        leading=14,\n",
    "        leftIndent=36,  # Overall left indent\n",
    "        firstLineIndent=-36,  # Creates hanging indent\n",
    "        alignment=TA_LEFT,\n",
    "        spaceAfter=12  # Space between references\n",
    "    )\n",
    "    \n",
    "    # Create header style\n",
    "    header_style = ParagraphStyle(\n",
    "        'Header',\n",
    "        parent=styles['Normal'],\n",
    "        fontName='Times-Bold',\n",
    "        fontSize=12,\n",
    "        spaceBefore=0,\n",
    "        spaceAfter=20,\n",
    "        alignment=TA_LEFT\n",
    "    )\n",
    "    \n",
    "    # Initialize story for the PDF\n",
    "    story = []\n",
    "    \n",
    "    # Add References header\n",
    "    story.append(Paragraph(\"References\", header_style))\n",
    "    \n",
    "    # Process references\n",
    "    if isinstance(references, str):\n",
    "        refs = clean_references(references)\n",
    "    else:\n",
    "        refs = references\n",
    "    \n",
    "    # Add each reference\n",
    "    for ref in refs:\n",
    "        if ref.strip():\n",
    "            # Clean and format the reference\n",
    "            ref = clean_reference(ref)\n",
    "            story.append(Paragraph(ref, ref_style))\n",
    "    \n",
    "    # Build PDF\n",
    "    doc.build(story)\n",
    "\n",
    "def clean_reference(ref):\n",
    "    \"\"\"\n",
    "    Cleans and formats a single reference.\n",
    "    \n",
    "    Args:\n",
    "        ref (str): Reference string to clean\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned reference\n",
    "    \"\"\"\n",
    "    # Remove TextBlock formatting more aggressively\n",
    "    ref = re.sub(r'TextBlock\\s*\\([^)]*\\)', '', ref)\n",
    "    ref = re.sub(r'citations=None,?\\s*text=', '', ref)\n",
    "    ref = re.sub(r'type=\\'text\\',?\\s*', '', ref)\n",
    "    \n",
    "    # Remove line breaks and excess whitespace\n",
    "    ref = ' '.join(ref.split())\n",
    "    \n",
    "    # Remove various formatting markers\n",
    "    ref = re.sub(r'\\'|\\\\\\n|\\\\n', '', ref)\n",
    "    \n",
    "    # Fix spacing around periods in author names\n",
    "    ref = re.sub(r'\\.\\s*([A-Z])', r'. \\1', ref)\n",
    "    \n",
    "    # Fix spacing around ampersands\n",
    "    ref = re.sub(r'\\s*&\\s*', ' & ', ref)\n",
    "    \n",
    "    # Fix multiple spaces\n",
    "    ref = re.sub(r'\\s+', ' ', ref)\n",
    "    \n",
    "    # Remove asterisks around journal names while preserving italics in PDF\n",
    "    ref = re.sub(r'\\s*\\*([^*]+)\\*', r' \\1', ref)\n",
    "    \n",
    "    # Ensure proper spacing after commas\n",
    "    ref = re.sub(r',\\s*', ', ', ref)\n",
    "    \n",
    "    # Fix spacing around parentheses\n",
    "    ref = re.sub(r'\\s*\\(\\s*', ' (', ref)\n",
    "    ref = re.sub(r'\\s*\\)', ')', ref)\n",
    "    \n",
    "    # Remove any remaining parenthetical formatting artifacts\n",
    "    ref = re.sub(r'\\([^)]*citations[^)]*\\)', '', ref)\n",
    "    \n",
    "    # Ensure the reference ends with a period\n",
    "    ref = ref.rstrip('.')\n",
    "    ref += '.'\n",
    "    \n",
    "    return ref.strip()\n",
    "\n",
    "def clean_references(text):\n",
    "    \"\"\"\n",
    "    Cleans and splits reference text into individual references.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Full text containing references\n",
    "    \n",
    "    Returns:\n",
    "        list: List of cleaned references\n",
    "    \"\"\"\n",
    "    # First, remove all TextBlock formatting\n",
    "    text = re.sub(r'TextBlock\\s*\\([^)]*\\)\\s*', '', text)\n",
    "    text = re.sub(r'citations=None,?\\s*text=', '', text)\n",
    "    text = re.sub(r'type=\\'text\\',?\\s*', '', text)\n",
    "    \n",
    "    # Remove extra quotes and formatting\n",
    "    text = re.sub(r'[\\'\\\"]', '', text)\n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    \n",
    "    # Split into potential references\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Initialize variables\n",
    "    refs = []\n",
    "    current_ref = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Skip empty lines, headers, and formatting remnants\n",
    "        if not line or line.lower() == 'references' or 'textblock' in line.lower():\n",
    "            continue\n",
    "            \n",
    "        # If line starts with a capital letter and previous reference exists,\n",
    "        # it's probably a new reference\n",
    "        if re.match(r'^[A-Z]', line) and current_ref:\n",
    "            refs.append(' '.join(current_ref))\n",
    "            current_ref = [line]\n",
    "        else:\n",
    "            current_ref.append(line)\n",
    "    \n",
    "    # Add the last reference\n",
    "    if current_ref:\n",
    "        refs.append(' '.join(current_ref))\n",
    "    \n",
    "    # Clean each reference\n",
    "    cleaned_refs = []\n",
    "    for ref in refs:\n",
    "        cleaned = clean_reference(ref)\n",
    "        if cleaned and not cleaned.isspace() and len(cleaned) > 10:  # Filter out very short \"references\"\n",
    "            cleaned_refs.append(cleaned)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_refs = []\n",
    "    for ref in cleaned_refs:\n",
    "        if ref not in seen:\n",
    "            seen.add(ref)\n",
    "            unique_refs.append(ref)\n",
    "    \n",
    "    return unique_refs\n",
    "\n",
    "def get_formatted_references(prompt, max_retries=3):\n",
    "    \"\"\"\n",
    "    Gets formatted references using the Anthropic Claude API with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send to Claude\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted references from Claude, or None if failed\n",
    "    \"\"\"\n",
    "    # Get API key from environment variable\n",
    "    api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "    if not api_key:\n",
    "        # Fallback to hardcoded key if environment variable not set\n",
    "        api_key = \"enter API key here\"\n",
    "    \n",
    "    try:\n",
    "        client = anthropic.Anthropic(api_key=api_key)\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Make the API call\n",
    "                message = client.messages.create(\n",
    "                    model=\"claude-sonnet-4-20250514\",\n",
    "                    max_tokens=4000,\n",
    "                    temperature=0,\n",
    "                    system=\"You are a helpful research assistant with expertise in academic citations. Format references in proper APA style with full journal names, volumes, and page numbers.\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                # Extract and clean the content - FIXED\n",
    "                if message and hasattr(message, 'content'):\n",
    "                    content = message.content\n",
    "                    if isinstance(content, list):\n",
    "                        # Extract just the text from TextBlock objects\n",
    "                        text_parts = []\n",
    "                        for item in content:\n",
    "                            if hasattr(item, 'text'):\n",
    "                                text_parts.append(item.text)\n",
    "                            else:\n",
    "                                text_parts.append(str(item))\n",
    "                        content = '\\n'.join(text_parts)\n",
    "                    elif hasattr(content, 'text'):\n",
    "                        content = content.text\n",
    "                    return content\n",
    "                \n",
    "                return None\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"API Error on attempt {attempt + 1}: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    return None\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Client initialization error: {e}\")\n",
    "        return None\n",
    "\n",
    "def batch_process_files(input_dir, output_dir, start_from=0, delay_seconds=2):\n",
    "    \"\"\"\n",
    "    Process all text files in a directory and create corresponding reference PDFs.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Path to directory containing input text files\n",
    "        output_dir (str): Path to directory where PDFs will be saved\n",
    "        start_from (int): File index to start from (for resuming)\n",
    "        delay_seconds (int): Delay between API calls to avoid rate limits\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get list of files to process\n",
    "    files_to_process = [f for f in os.listdir(input_dir) if f.endswith('_combined.txt')]\n",
    "    files_to_process.sort()  # Process in consistent order\n",
    "    \n",
    "    # Counter for processed files\n",
    "    processed = 0\n",
    "    errors = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    print(f\"Found {len(files_to_process)} files to process\")\n",
    "    print(f\"Starting from file index {start_from}\")\n",
    "    \n",
    "    # Process each file in the input directory\n",
    "    for i, filename in enumerate(files_to_process):\n",
    "        if i < start_from:\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Construct full input path\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            \n",
    "            # Create output filename\n",
    "            output_filename = filename.replace('_combined.txt', '_references.pdf')\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            # Skip if output already exists\n",
    "            if os.path.exists(output_path):\n",
    "                print(f\"Skipping {filename} - output already exists\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing {i+1}/{len(files_to_process)}: {filename}\")\n",
    "            \n",
    "            # Read input file\n",
    "            with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Create prompt for Claude\n",
    "            prompt = f\"\"\"Based on the following text, generate a reference list in APA format. \n",
    "            Format each reference exactly like these examples:\n",
    "\n",
    "            Leuz, C., & Verrecchia, R. E. (2000). The economic consequences of increased disclosure. Journal of Accounting Research, 91-124.\n",
    "\n",
    "            Bourveau, T., She, G., & Zaldokas, A. (2020). Corporate disclosure as a tacit coordination mechanism: Evidence from cartel enforcement regulations. Journal of Accounting Research, 58(2), 295-332.\n",
    "\n",
    "            Text for analysis:\n",
    "            {text}\n",
    "\n",
    "            Please format each reference following the exact style above, including:\n",
    "            1. Remove any asterisks, TextBlock tags, or other formatting markers \n",
    "            2. Author names with initials\n",
    "            3. Full title in sentence case\n",
    "            4. Journal name in italics (use *journal name* for italics)\n",
    "            5. Volume, issue, and page numbers where applicable\n",
    "            6. Year in parentheses\n",
    "            7. One reference per line \n",
    "            8. Subsequent references should be followed by a space after the previous reference\n",
    "            9. Sort alphabetically by author's last name\n",
    "            10. Provide only the references, no extra text or explanations\"\"\"\n",
    "\n",
    "            \n",
    "            # Get formatted references from Claude\n",
    "            formatted_refs = get_formatted_references(prompt)\n",
    "            \n",
    "            if formatted_refs:\n",
    "                # Create the PDF with the formatted references\n",
    "                create_reference_pdf(formatted_refs, output_path)\n",
    "                processed += 1\n",
    "                print(f\"✓ Successfully processed: {filename}\")\n",
    "            else:\n",
    "                errors += 1\n",
    "                print(f\"✗ Error getting references for: {filename}\")\n",
    "            \n",
    "            # Add delay between API calls to avoid rate limits\n",
    "            if delay_seconds > 0 and i < len(files_to_process) - 1:\n",
    "                time.sleep(delay_seconds)\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            print(f\"✗ Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"PROCESSING COMPLETE!\")\n",
    "    print(f\"Total files found: {len(files_to_process)}\")\n",
    "    print(f\"Skipped: {skipped} files\")\n",
    "    print(f\"Successfully processed: {processed} files\")\n",
    "    print(f\"Errors: {errors} files\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set input and output directories\n",
    "    input_directory = r\"enter folder path here\"\n",
    "    output_directory = r\"enter folder path here\"\n",
    "    \n",
    "    # Process all files\n",
    "    batch_process_files(input_directory, output_directory, start_from=0, delay_seconds=2)\n",
    "                              \n",
    "# 16. Combine manuscript files with pdf table files for descriptive statistics and regression analysis\n",
    "\n",
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfMerger\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_JUSTIFY, TA_LEFT, TA_CENTER\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "\n",
    "def format_title_name(name):\n",
    "    \"\"\"Format name for display in titles with proper spacing\"\"\"\n",
    "    # Replace underscores with spaces\n",
    "    return name.replace('_', ' ')\n",
    "\n",
    "def get_available_category_mechanism_pairs(base_dir):\n",
    "    \"\"\"Get actual category-mechanism pairs from existing combined files\"\"\"\n",
    "    combined_dir = os.path.join(base_dir, 'combined_sections')\n",
    "    if not os.path.exists(combined_dir):\n",
    "        print(f\"Combined sections directory not found: {combined_dir}\")\n",
    "        return []\n",
    "    \n",
    "    pairs = []\n",
    "    for filename in os.listdir(combined_dir):\n",
    "        if filename.endswith('_combined.txt'):\n",
    "            \n",
    "            # Remove _combined.txt suffix\n",
    "            base_name = filename.replace('_combined.txt', '')\n",
    "            \n",
    "            # Known mechanisms\n",
    "            known_mechanisms = [\n",
    "                'Information_Asymmetry', 'Corporate_Governance', 'Unsophisticated_Investors',\n",
    "                'Litigation_Risk', 'Reputation_Risk', 'Proprietary_Costs', 'Equity_Issuance'\n",
    "            ]\n",
    "            \n",
    "            # Find the mechanism\n",
    "            mechanism_found = None\n",
    "            category_name = None\n",
    "            \n",
    "            for mechanism in known_mechanisms:\n",
    "                if base_name.endswith('_' + mechanism):\n",
    "                    mechanism_found = mechanism\n",
    "                    category_name = base_name[:-len('_' + mechanism)]\n",
    "                    break\n",
    "            \n",
    "            if mechanism_found and category_name:\n",
    "                pairs.append((category_name, mechanism_found))\n",
    "            else:\n",
    "                print(f\"Warning: Could not parse mechanism from: {filename}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def merge_pdf_files(base_dir: str, category_name: str, mechanism: str):\n",
    "    \"\"\"Merge manuscript PDF with tables and references\"\"\"\n",
    "    # Register Times New Roman font\n",
    "    try:\n",
    "        pdfmetrics.registerFont(TTFont('Times New Roman', 'times.ttf'))\n",
    "        pdfmetrics.registerFont(TTFont('Times New Roman Bold', 'timesbd.ttf'))\n",
    "    except:\n",
    "        print(\"Warning: Times New Roman font not found, using default font\")\n",
    "    \n",
    "    # Define file paths\n",
    "    combined_sections_dir = os.path.join(base_dir, 'combined_sections')\n",
    "    reg_dir = os.path.join(base_dir, 'regression_analyses')\n",
    "    desc_dir = os.path.join(base_dir, 'descriptive_stats')\n",
    "    ref_dir = os.path.join(base_dir, 'references')\n",
    "    output_dir = os.path.join(base_dir, 'final_manuscripts')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Clean mechanism for file naming\n",
    "    clean_mechanism = mechanism.replace(' ', '_')\n",
    "    full_base_name = f\"{category_name}_{clean_mechanism}\"\n",
    "    \n",
    "    # Check if manuscript file exists\n",
    "    manuscript_file = os.path.join(combined_sections_dir, f\"{full_base_name}_combined.txt\")\n",
    "    if not os.path.exists(manuscript_file):\n",
    "        print(f\"Manuscript file not found: {manuscript_file}\")\n",
    "        return False\n",
    "    \n",
    "    # Format category name for display\n",
    "    formatted_category_name = format_title_name(category_name)\n",
    "    formatted_mechanism = format_title_name(mechanism)\n",
    "\n",
    "    # Create intermediate PDF with formatting\n",
    "    temp_pdf_path = os.path.join(output_dir, f'temp_{full_base_name}.pdf')\n",
    "    temp_pdf = SimpleDocTemplate(\n",
    "        temp_pdf_path,\n",
    "        pagesize=letter,\n",
    "        rightMargin=72,\n",
    "        leftMargin=72,\n",
    "        topMargin=72,\n",
    "        bottomMargin=72\n",
    "    )\n",
    "\n",
    "    # Create styles\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    custom_title = ParagraphStyle(\n",
    "        name='CustomTitle',\n",
    "        fontName='Times New Roman Bold',\n",
    "        fontSize=16,\n",
    "        spaceAfter=16,\n",
    "        spaceBefore=24,\n",
    "        firstLineIndent=0,\n",
    "        alignment=TA_CENTER,\n",
    "        leading=24\n",
    "    )\n",
    "    \n",
    "    subtitle_style = ParagraphStyle(\n",
    "        name='CustomSubtitle',\n",
    "        fontName='Times New Roman',\n",
    "        fontSize=12,\n",
    "        spaceAfter=24,\n",
    "        spaceBefore=12,\n",
    "        firstLineIndent=0,\n",
    "        alignment=TA_CENTER,\n",
    "        leading=24\n",
    "    )\n",
    "    \n",
    "    regular_style = ParagraphStyle(\n",
    "        name='CustomRegular',\n",
    "        fontName='Times New Roman',\n",
    "        fontSize=12,\n",
    "        spaceAfter=12,\n",
    "        firstLineIndent=36,\n",
    "        leading=24,\n",
    "        alignment=TA_JUSTIFY\n",
    "    )\n",
    "    \n",
    "    heading_style = ParagraphStyle(\n",
    "        name='CustomHeading',\n",
    "        fontName='Times New Roman',\n",
    "        fontSize=12,\n",
    "        spaceAfter=6,\n",
    "        spaceBefore=18,\n",
    "        firstLineIndent=0, \n",
    "        alignment=TA_LEFT, \n",
    "        leading=24\n",
    "    )\n",
    "    \n",
    "    subheading_style = ParagraphStyle(\n",
    "        name='CustomSubheading',\n",
    "        fontName='Times New Roman',\n",
    "        fontSize=12,\n",
    "        spaceAfter=6,\n",
    "        spaceBefore=12,\n",
    "        firstLineIndent=0,  \n",
    "        alignment=TA_LEFT,  \n",
    "        leading=24\n",
    "    )\n",
    "\n",
    "    # Read manuscript\n",
    "    with open(manuscript_file, 'r', encoding='utf-8') as f:\n",
    "        manuscript_text = f.read()\n",
    "\n",
    "    # Create story (content)\n",
    "    story = []\n",
    "\n",
    "    # Add title and subtitle\n",
    "    title = f\"{formatted_category_name} and Voluntary Disclosure\"\n",
    "    story.append(Paragraph(title, custom_title))\n",
    "    story.append(Paragraph(\"Artemis Intelligencia\", subtitle_style))\n",
    "    story.append(Paragraph(\"September 10, 2025\", subtitle_style))\n",
    "    story.append(Spacer(1, 24))\n",
    "\n",
    "    # Common section headers\n",
    "    main_headers = ['INTRODUCTION', 'BACKGROUND AND HYPOTHESIS DEVELOPMENT', \n",
    "                    'RESEARCH DESIGN', 'DESCRIPTIVE STATISTICS', 'RESULTS', 'CONCLUSION']\n",
    "    \n",
    "    subheaders_exact = ['Background', 'Theoretical Framework','Hypothesis Development', \n",
    "                        'Model Explanation', 'Mathematical Model', 'Regression Analysis',\n",
    "                        'Variable Definitions', 'Empirical Model', 'Sample Selection', \n",
    "                        'Sample Construction', 'Sample Description and Descriptive Statistics', \n",
    "                        'Model Specification', 'Model Development', 'Regression Specification',\n",
    "                        'Mathematical Specification', 'Variable Definitions and Measurement']\n",
    "\n",
    "    # Process manuscript text\n",
    "    lines = manuscript_text.split('\\n')\n",
    "    current_paragraph = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line_stripped = line.strip()\n",
    "    \n",
    "        # Skip lines with just equals signs\n",
    "        if line_stripped and all(c == '=' for c in line_stripped):\n",
    "            continue\n",
    "    \n",
    "        # Skip duplicate \"Research Design\" after \"RESEARCH DESIGN\"\n",
    "        if line_stripped == \"Research Design\":\n",
    "            skip_this_line = False\n",
    "            for j in range(i-1, -1, -1):\n",
    "                prev_line = lines[j].strip()\n",
    "                if prev_line and not all(c == '=' for c in prev_line):\n",
    "                    if prev_line == \"RESEARCH DESIGN\":\n",
    "                        skip_this_line = True\n",
    "                    break\n",
    "            if skip_this_line:\n",
    "                continue\n",
    "                \n",
    "        # Skip empty lines\n",
    "        if not line_stripped:\n",
    "            if current_paragraph:\n",
    "                paragraph_text = ' '.join(current_paragraph)\n",
    "                story.append(Paragraph(paragraph_text, regular_style))\n",
    "                current_paragraph = []\n",
    "            continue\n",
    "            \n",
    "        # Skip the original title if it appears\n",
    "        if i < 5 and ('Voluntary Disclosure' in line_stripped or 'Analysis of' in line_stripped) and len(line_stripped) < 150:\n",
    "            continue\n",
    "            \n",
    "        # Check if it's a main header\n",
    "        is_main_header = False\n",
    "        for header in main_headers:\n",
    "            if header == line_stripped.upper().replace('=', '').strip():\n",
    "                is_main_header = True\n",
    "                if current_paragraph:\n",
    "                    paragraph_text = ' '.join(current_paragraph)\n",
    "                    story.append(Paragraph(paragraph_text, regular_style))\n",
    "                    current_paragraph = []\n",
    "                story.append(Paragraph(header, heading_style))\n",
    "                break\n",
    "        \n",
    "        if is_main_header:\n",
    "            continue\n",
    "            \n",
    "        # Check if it's a subheader\n",
    "        is_subheader = False\n",
    "        if len(line_stripped) < 50:\n",
    "            for subheader in subheaders_exact:\n",
    "                if subheader.lower() == line_stripped.lower():\n",
    "                    is_subheader = True\n",
    "                    if current_paragraph:\n",
    "                        paragraph_text = ' '.join(current_paragraph)\n",
    "                        story.append(Paragraph(paragraph_text, regular_style))\n",
    "                        current_paragraph = []\n",
    "                    story.append(Paragraph(line_stripped, subheading_style))\n",
    "                    break\n",
    "        \n",
    "        if is_subheader:\n",
    "            continue\n",
    "            \n",
    "        # Regular paragraph text\n",
    "        current_paragraph.append(line_stripped)\n",
    "    \n",
    "    # Add last paragraph\n",
    "    if current_paragraph:\n",
    "        paragraph_text = ' '.join(current_paragraph)\n",
    "        story.append(Paragraph(paragraph_text, regular_style))\n",
    "\n",
    "    # Create the intermediate PDF\n",
    "    temp_pdf.build(story)\n",
    "\n",
    "    # Merge PDFs\n",
    "    merger = PdfMerger()\n",
    "    \n",
    "    try:\n",
    "        # Add formatted manuscript\n",
    "        merger.append(temp_pdf_path)\n",
    "        \n",
    "        # Add references\n",
    "        ref_file = os.path.join(ref_dir, f\"{full_base_name}_references.pdf\")\n",
    "        if os.path.exists(ref_file):\n",
    "            merger.append(ref_file)\n",
    "            print(f\"  Added references\")\n",
    "        else:\n",
    "            print(f\"  Warning: No references file found\")\n",
    "    \n",
    "        # Add descriptive statistics table\n",
    "        panel_name = f\"panel_{full_base_name}\"\n",
    "        desc_file = os.path.join(desc_dir, panel_name, 'descriptive_stats_table.pdf')\n",
    "        if os.path.exists(desc_file):\n",
    "            merger.append(desc_file)\n",
    "            print(f\"  Added descriptive statistics\")\n",
    "        else:\n",
    "            print(f\"  Warning: No descriptive statistics found\")\n",
    "    \n",
    "        # Add correlation table\n",
    "        corr_dir = os.path.join(base_dir, 'correlations')\n",
    "        corr_file = os.path.join(corr_dir, f\"{full_base_name}_correlation_table.pdf\")\n",
    "        if os.path.exists(corr_file):\n",
    "            merger.append(corr_file)\n",
    "            print(f\"  Added correlation table\")\n",
    "        else:\n",
    "            print(f\"  Warning: No correlation table found\")\n",
    "    \n",
    "        # Add regression table\n",
    "        reg_file = os.path.join(reg_dir, panel_name, 'regression_table.pdf')\n",
    "        if os.path.exists(reg_file):\n",
    "            merger.append(reg_file)\n",
    "            print(f\"  Added regression table\")\n",
    "        else:\n",
    "            print(f\"  Warning: No regression table found\")\n",
    "            \n",
    "        # Output filename\n",
    "        output_file = os.path.join(output_dir, \n",
    "                                   f\"{formatted_category_name} and Voluntary Disclosure_{formatted_mechanism}_final.pdf\")\n",
    "        \n",
    "        merger.write(output_file)\n",
    "        merger.close()\n",
    "        \n",
    "        # Clean up temporary file\n",
    "        os.remove(temp_pdf_path)\n",
    "        \n",
    "        print(f\"  ✓ Successfully created: {output_file}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error creating PDF: {str(e)}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if 'merger' in locals():\n",
    "            merger.close()\n",
    "\n",
    "def batch_merge_pdfs(base_dir):\n",
    "    \"\"\"Process all available category-mechanism combinations\"\"\"\n",
    "    \n",
    "    print(f\"Base directory: {base_dir}\\n\")\n",
    "    \n",
    "    # Get actual pairs from existing files\n",
    "    category_mechanisms = get_available_category_mechanism_pairs(base_dir)\n",
    "    \n",
    "    if not category_mechanisms:\n",
    "        print(\"No category-mechanism pairs found in combined_sections directory\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(category_mechanisms)} category-mechanism combinations to process:\")\n",
    "    for category, mechanism in category_mechanisms:\n",
    "        print(f\"  - {category} - {mechanism}\")\n",
    "    print()\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for category, mechanism in category_mechanisms:\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Processing: {category} - {mechanism}\")\n",
    "        try:\n",
    "            result = merge_pdf_files(base_dir, category, mechanism)\n",
    "            if result:\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {str(e)}\")\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL SUMMARY:\")\n",
    "    print(f\"Total combinations: {len(category_mechanisms)}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    BASE_DIR = r\"enter folder path here\"\n",
    "    batch_merge_pdfs(BASE_DIR)\n",
    "                              \n",
    "# 17. ADD PAGE NUMBERS TO EXISTING PDFs \n",
    "import os\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter\n",
    "import io\n",
    "\n",
    "def add_page_numbers(input_path, output_path):\n",
    "    reader = PdfReader(input_path)\n",
    "    writer = PdfWriter()\n",
    "    \n",
    "    for i in range(len(reader.pages)):\n",
    "        page = reader.pages[i]\n",
    "        packet = io.BytesIO()\n",
    "        can = canvas.Canvas(packet, pagesize=(page.mediabox.width, page.mediabox.height))\n",
    "        \n",
    "        if i > 0:\n",
    "            can.setFont('Times-Roman', 12)\n",
    "            can.drawString(page.mediabox.width/2 - 6, 40, str(i))\n",
    "        \n",
    "        can.save()\n",
    "        packet.seek(0)\n",
    "        number_pdf = PdfReader(packet)\n",
    "        \n",
    "        if len(number_pdf.pages) > 0:\n",
    "            page.merge_page(number_pdf.pages[0])\n",
    "        writer.add_page(page)\n",
    "    \n",
    "    with open(output_path, \"wb\") as output_file:\n",
    "        writer.write(output_file)\n",
    "\n",
    "def batch_process_pdfs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    pdfs = [f for f in os.listdir(input_dir) if f.endswith('.pdf')]\n",
    "    \n",
    "    for i, filename in enumerate(pdfs, 1):\n",
    "        print(f\"\\nProcessing {i}/{len(pdfs)}: {filename}\")\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, f\"numbered_{filename}\")\n",
    "        try:\n",
    "            add_page_numbers(input_path, output_path)\n",
    "            print(f\"Successfully processed: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "# Usage\n",
    "input_dir = r\"enter folder path here\"\n",
    "output_dir = r\"enter folder path here\"\n",
    "batch_process_pdfs(input_dir, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
